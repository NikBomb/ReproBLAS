\subsection{Euclidean Norm}
  \label{sec:compositeops_nrm}
    
    The Euclidean norm operation computes
    \begin{equation}
        \texttt{nrm2}(x) = \sqrt{\sum_{j=0}^{n-1} x_j^2}, \quad x_j \in \F.
        \label{eq:nrm2}
    \end{equation}

    Since the summands in the summation of \eqref{eq:nrm2} are all non-negative,
    there is no cancellation, the euclidean norm operation
    is usually of high accuracy.
    It can be implemented simply using a dot product operation on two copies of the same input vector,
    or by using the sum operation on the squares of the input vector,
    both of which can be made reproducible using a corresponding reproducible
    dot product and summation operations. However, both of these methods are prone to unnecessary overflow and underflow.
    On one hand, if some input values were too big ($\geq \sqrt{2^{e_{max}+1}}$),
    the partial sum would overflow. The computed result would
    be $\texttt{Inf}$ when the real Euclidean norm would be much smaller than the
    overflow threshhold.
    On the other hand, if all input values were too small ($ < \sqrt{2^{e_{min}}}$),
    their squared value would be smaller than the smallest representable floating point number and 
    underflow would occur, causing the returned result to be 0.

    Scaling techniques can be used to handle the underflow/overflow issues.
    For example, the function \texttt{xnrm2} from the BLAS library \cite{BLAS}
    scales input data by the intermediate maximum absolute value
    to avoid both overflow and underflow.
    However, in our indexed format scheme, such a scaling would alter
    the mantissae of the input data and the splitting of the mantissae, and therefore
    cannot guarantee reproducibility.
    In order to maintain reproducibility, 
    first, scaling factors must be powers of two so that the scaling and rescaling
    won't change any mantissae of input values.
    Second, the scaling factors' exponents must differ only by multiples of $W$,
    so that the slicing processes using different scaling factors are identical.
    %Therefore the reproducibility is maintained after rescaling.
    Algorithm~\ref{alg:nrm2} summarizes the algorithm for sum of squares,
    which will be used to compute the euclidean norm.
    Note that the handling of special input values \texttt{Inf}, \texttt{-Inf}, and \texttt{NaN}
    is similar to the sum operation, which is not included here for better readibility.

    \begin{samepage}
    \begin{alg}
      If $Y$ is the $K$-fold indexed sum of some $(x_0/s)^2, ..., (x_{n - 1}/s)^2$ where $x_0, ..., x_{n} \in \F$, $s \in 2^{W\Z}$, produce the $K$-fold indexed sum of $(x_0/t)^2, ..., (x_{n}/t)^2$ where $t \in 2^{W\Z}$. 
      \begin{algorithmic}[1]
        \Require
        \Statex $s=0$ if $\max_{j \in \{0, ..., n - 1\}}|x_j| = 0$.
        Otherwise $2^{-p-W-1} s < \max_{j \in \{0, ..., n - 1\}}|x_j| < 2^{W+2} s$
        and $s \in 2^{\Z*W}$.
        \Statex $Y$ is the indexed sum of $(x_0/s)^2, ..., (x_{n-1}/s)^2$.
        \Function{Add Float To Indexed Norm}{$K$, $x_n$, $Y$, $s$}
          \State $e = W * \lfloor\max(\exp(x_n) - 1,e_{min}+W) / W\rfloor$
              \label{alg:nrm2.line.e}
          \State $t = 2^e$
          \If{$s < t$}
              \For {$k=0$ \To $K-1$}
                  \State ${Y_k}_P = (s/t)^2 * {Y_k}_P$
              \EndFor
              \State $s = t$
          \EndIf
        \State \Call{Deposit}{$K$, $(x_n / s)^2$, $Y$}
      \EndFunction
      \Ensure
      \Statex $s=0$ if $\max_{j \in \{0, ..., n\}}|x_j| = 0$.
      Otherwise $2^{-p-W-1} s < \max_{j \in \{0, ..., n\}}|x_j| < 2^{W+2} s$
      and $s \in 2^{\Z*W}$.
      \Statex $Y$ is the indexed sum of $(x_0/s)^2, ..., (x_{n}/s)^2$.
      \end{algorithmic}
      \label{alg:nrm2}
    \end{alg}
    \end{samepage}

    A method to add the scaled indexed sum of squares of a vector to a scaled indexed type (using an Algorithm similar to \ref{alg:nrm2}) is available in ReproBLAS as \texttt{idxdBLAS\_xixssq} in \texttt{idxdBLAS.h}. A method to return the reproducible Euclidean norm of a vector is available as \texttt{reproBLAS\_rxnrm2} in \texttt{reproBLAS.h}. The function \texttt{idxd\_xixiaddsq} in \texttt{idxd.h} can be used to add two scaled indexed sums of squares. An MPI data type that holds a scaled indexed type can be created (creation is only performed once, subsequent calls return the same copy) and returned using the \texttt{idxdMPI\_DOUBLE\_INDEXED\_SCALED}, etc. function of \texttt{idxdMPI.h}. \texttt{idxdMPI.h} also contains the \texttt{XIXIADDSQ} method, an MPI reduction operator that can reduce the scaled sums of squares in parallel. (see Section \ref{sec:reproBLAS} for details).


    \begin{lem}
    \label{lem:nrm2_scaling}
    Let $s$ and $Y$ be the output of Algorithm~\ref{alg:nrm2}, then the updated scaling factor $s$ is either $0$ or it satisfies
    \begin{equation}
        2^{-p-W-1}s \leq \max_{j \in \{0, ..., n\}}|x_j| < 2^{W+2} s
    \end{equation}
    \end{lem}

    \begin{proof}
         From line~\ref{alg:nrm2.line.e} of Algorithm~\ref{alg:nrm2}, it is to see that
         $e$ is a multiple of $W$ and
         \[
            \begin{aligned}
                e & \geq W * \lfloor(e_{\min} + W) / W\rfloor > e_{\min} \\
                e & \leq \max(\exp(x_n) - 1, e_{\min} + W) < e_{\max}.
            \end{aligned}
         \]
         Therefore both $t=2^e$ and $1/t$ is representable.
         It also means that if there is no input exceptional values, the scaling factor $s$ is always representable.

        The proof is trivial for the case $x_n = 0$. We suppose that $x_n \neq 0$.
        Hence $x_n \geq 2^{e_{\min} - p}$, or $e_{\min} \leq \exp(x_n) + p$,
        where $p$ is the precision of input floating-point numbers.
        Therefore $e \leq \max(\exp(x_n), \exp(x_n) + p + W)$,
        which means
        \begin{equation}
            t = 2^e \leq 2^{\exp(x_n)} 2^{p+W} < |x_n| 2^{p+W+1}.
            \label{eq:nrm2_scaling.upper}
        \end{equation}
        Moreover, we have
        \(
            e \geq W * ((\exp(x_n) -  1) / W - 1) = \exp(x_n) - W -1
        \)
        So 
        \begin{equation}
            s = 2^e \geq 2^{\exp(x_n)} 2^{- W - 1} > |x_n| 2^{-W-2}
            \label{eq:nrm2_scaling.lower}
        \end{equation}
        The proof can be deduced by combining the Algorithm's requirement
        with \eqref{eq:nrm2_scaling.upper} and \eqref{eq:nrm2_scaling.lower}.
    \end{proof}

    Lemma~\ref{lem:nrm2_scaling} means that 
    for non-trival input values, the maximum absolute value of the scaled inputs
    will always be kept in range $(2^{-p-W-1}, 2^{W+2})$, which guarantees that
    there will be no overflow or underflow in computing the sum of squares.

    Since the scaling factors in Algorithm~\ref{alg:nrm2} are always reprentable and $s \in 2^{W\Z}$,
    the binning process is not affected by the scaling as well as the rescaling.
    Therefore the reproducibility of Algorithm~\ref{alg:nrm2} is guaranteed
    by the reproducibility of the indexed sum (see Section~\ref{sec:indexed_sum}).
    Finally, the euclidean norm can be simply computed as (using \eqref{eq:indexedvalue})
    \begin{equation}
      s \sqrt{\mathcal{Y}}
    \end{equation}

    It is worth noting that the same scaling technique can be used to avoid
    overflow and underflow for the summation, regardless of the index of the partial sum.
    It is possible to scale input data by a factor in $2^{W\Z}$ so that the maximum
    absolute value is always in range $[1,2^W)$.
    This will help to avoid computing the index of partial sum
    at the cost of having to store and communicate the scaling factor along computation.
    It is therefore left as a tuning parameter for future work.

