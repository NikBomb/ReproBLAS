\subsection{Testing}
  In understanding the testing methodology behind ReproBLAS, is important to distinguish between the metrics of reproducibility and accuracy. Although high accuracy can sometimes result in reproducibility, it is not a guarantee. For this reason, we test the accuracy and the reproducibility of the ReproBLAS methods separately.

  Testing in ReproBLAS starts with the BLAS1 methods (\texttt{xsum}, \texttt{xasum}, \texttt{xnrm2}, and \texttt{xdot}). These methods are first checked to see that their results are accurate, then checked to see if the results are reproducible. All tests are repeated several times with parameters changed. The data is scaled by $1$ and $-1$. The data is permuted by reversing, sorting (in ascending or descending order of value or absolute value), or random shuffling. To check that the result is independent of blocking, the data is grouped into several blocks and each block is operated on separately, then the results for each block are combined using the appropriate function. Several different block sizes are tested for each permutation of the data.

  The accuracy of the reproducible BLAS1 methods is validated by checking to see if the results of each method are within the error margin specified by \eqref{eq:error2} from the true results. Because the true summation result must be known, these tests are performed on distributions with known sums. The input vectors used are shown in Table \ref{tbl:validateblas1data}. Note that $\text{random}()$ is a floating point number uniformly distributed between 0 and 1.

    \begin{table}[!htbp]
        \centering
        \begin{tabular}{ | l |  l | l |} \hline
            Name & Value & Sum\\ \hline
            $sine$ & $X_j = \begin{cases}\sin(2 \pi j/n) \text{ if } j < \lfloor n/2 \rfloor\\ 0 \text{ if } j = (n - 1)/2 \\ -X_{n - j - 1} \text{ otherwise } \end{cases}$ & 0\\ \hline
            $mountain$ & $X_j = \begin{cases}\text{random}()\text{ if } n = 0 \\ X_{j - 1} * 2^{-\lfloor p/2\rfloor - 1} \text{ if } j < \lfloor n/2 \rfloor\\ 0 \text{ if } j = (n - 1)/2 \\ -X_{n - j - 1} \text{ otherwise } \end{cases}$ & 0 \\\hline
            $small + big$ & $X_j = \begin{cases} 2^{\lfloor p/2\rfloor + 1} \text{ if } j = 0 \\ 2^{-\lfloor p/2\rfloor - 1} \text{ otherwise} \end{cases}$ & $2^{\lfloor p/2\rfloor + 1} + (n - 1) 2^{-\lfloor p/2\rfloor - 1}$ \\\hline
            $small + big + big$ & $X_j = \begin{cases} 2^{\lfloor p/2\rfloor + 1} \text{ if } j = 0 \text{ or } j = n - 1 \\ 2^{-\lfloor p/2\rfloor - 1} \text{ otherwise} \end{cases}$ & $2^{\lfloor p/2\rfloor + 2} + (n - 2) 2^{-\lfloor p/2\rfloor - 1}$\\ \hline
            $small + big - big$ & $X_j = \begin{cases} 2^{\lfloor p/2\rfloor + 1} \text{ if } j = 0\\-2^{\lfloor p/2\rfloor + 1} \text{ if } j = n - 1\\ 2^{-\lfloor p/2\rfloor - 1} \text{ otherwise} \end{cases}$ & $(n - 2) 2^{-\lfloor p/2\rfloor - 1}$\\ \hline
        \end{tabular}
        \caption{ReproBLAS Accuracy Validation Vectors $X \in \F^n$}
        \label{tbl:validateblas1data}
    \end{table}

  Because the reproducible summation methods operate on bins, we repeat the accuracy tests on scaled versions of the input data set $W$ times, each time increasing the scale by a factor of two. This allows us to test all splits of the sum across bin boundaries. This is performed on data very close to overflow to test cases where the data is split between bin 0 and bin 1. This is also performed on data very close to underflow to test cases where some data is lost due to underflow.

  To validate the accuracy of the methods in the presence of \texttt{Inf}, \texttt{-Inf}, and \texttt{NaN}, the reproducible BLAS1 methods are tested on the vectors (scaled by $1$ and $-1$) shown in Table \ref{tbl:validateinfblas1data}.
    \begin{table}[!htbp]
        \centering
        \begin{tabular}{ | l |  l | l |} \hline
            Name & Value & Sum\\ \hline
            $\texttt{Inf}$ & $X_j = \begin{cases} \texttt{Inf} \text{ if } j = 0 \\ 0 \text{ otherwise} \end{cases}$ & \texttt{Inf}\\ \hline
            $\texttt{Inf} + \texttt{Inf}$ & $X_j = \begin{cases} \texttt{Inf} \text{ if } j = 0 \text{ or } j = n - 1 \\ 0 \text{ otherwise} \end{cases}$ & \texttt{Inf}\\\hline
            $\texttt{Inf} - \texttt{Inf}$ & $X_j = \begin{cases} \texttt{Inf} \text{ if } j = 0 \\ -\texttt{Inf} \text{ if } j = n - 1 \\ 0 \text{ otherwise} \end{cases}$& \texttt{NaN}\\ \hline
            $\texttt{NaN}$ & $X_j = \begin{cases} \texttt{NaN} \text{ if } j = 0 \\ 0 \text{ otherwise} \end{cases}$& \texttt{NaN}\\ \hline
            $\texttt{Inf} + \texttt{NaN}$ & $X_j = \begin{cases} \texttt{Inf} \text{ if } j = 0 \\ \texttt{NaN} \text{ if } j = n - 1 \\ 0 \text{ otherwise} \end{cases}$& \texttt{NaN}\\ \hline
            $\texttt{Inf} + \texttt{NaN} + \texttt{Inf}$ & $X_j = \begin{cases} \texttt{Inf} \text{ if } j = 0 \\ \texttt{NaN} \text{ if } j = \lfloor n/2 \rfloor \\ \texttt{Inf} \text{ if } j = n - 1 \\ 0 \text{ otherwise} \end{cases}$& \texttt{NaN}\\ \hline
            $\texttt{Inf} + \texttt{NaN} - \texttt{Inf}$ & $X_j = \begin{cases} \texttt{Inf} \text{ if } j = 0 \\ \texttt{NaN} \text{ if } j = \lfloor n/2 \rfloor \\ -\texttt{Inf} \text{ if } j = n - 1 \\ 0 \text{ otherwise} \end{cases}$& \texttt{NaN}\\ \hline
        \end{tabular}
        \caption{ReproBLAS Exception Validation Vectors $X \in \F^n$}
        \label{tbl:validateinfblas1data}
    \end{table}

  After validating the accuracy of the reproducible BLAS1 methods, their reproducibility is verified. Each method is checked to see if it produces the same result under the list of permutations given above (reversing, sorting, or random shuffling). Because we do not need to know the true sum of the data, we test using a uniform random and normal random distribution (in addition to those mentioned in Tables \ref{tbl:validateblas1data} and \ref{tbl:validateinfblas1data}).

  Once the BLAS1 methods are tested, the results of BLAS2 and BLAS3 (\texttt{xgemv} and \texttt{xgemm}) methods are tested against ``reference'' reproducible versions. These reference versions are simply naive implementations of \texttt{xgemv} and \texttt{xgemm} in terms of the reproducible dot product. They use no blocking and are simple to understand and code. Because \texttt{xgemv} and \texttt{xgemm} compute vectors and matrices of dot products, we can use the same data that was used for the dot product. For \texttt{xgemv}, we permute the columns of the input matrix together with the entries of the input vector as discussed in \eqref{eq:gemvpermute}. We break the computation into column-blocks of various sizes as discussed in \eqref{eq:gemvblock} and combine them to ensure that the operation is reproducible with respect to blocking of computation. For \texttt{xgemm}, we permute the columns of one input matrix together with the rows the other input matrix as discussed in \eqref{eq:gemmpermute}. We break the computation into column-blocks and row-blocks of various sizes as discussed in \eqref{eq:gemmblock} and combine them to ensure that the operation is reproducible with respect to blocking of computation.

  Each test described above must be performed with different values for several parameters, including $K$ (where the indexed types used are $K$-fold), the increment between elements of a vector, row or column major ordering of matrices, whether or not to transpose matrices, and scaling factors on vectors and matrices. To handle this software complexity, a Python test harness was created to exhaustively test each combination of values for these parameters.
