\documentclass[12pt]{article}
\usepackage{amsfonts}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{graphicx}
\usepackage{tikz}
\pagestyle{myheadings}

\author{Peter Ahrens, Hong Diep Nguyen, James Demmel}
\title{ReproBLAS: Much Summation! So Reproducible!}
\providecommand{\ceil}[1]{\left \lceil #1 \right \rceil }
\providecommand{\floor}[1]{\left \lfloor #1 \right \rfloor }
\providecommand{\R}{\ensuremath{\mathbb{R}}}
\providecommand{\F}{\ensuremath{\mathbb{F}}}
\providecommand{\Z}{\ensuremath{\mathbb{Z}}}
\providecommand{\exp}{\ensuremath{\text{exp}}}
\providecommand{\min}{\ensuremath{\text{min}}}
\providecommand{\max}{\ensuremath{\text{max}}}
\providecommand{\ulp}{\ensuremath{\text{ulp}}}
\providecommand{\ufp}{\ensuremath{\text{ufp}}}
\providecommand{\fl}{\ensuremath{\text{fl}}}
\providecommand{\To}{\ensuremath{\text{ to }}}
\providecommand{\roundtonearestinfty}{\ensuremath{\mathcal{R}_\text{$\infty$}}}
\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{alg}{Algorithm}[section]
\numberwithin{equation}{section}

\graphicspath{{plots}}
\begin{document}
\noindent
\maketitle
\tableofcontents
\newpage
\section{Introduction}
\section{Notation and Background}
  Let $\R, \F, $ and $\Z$ denote the sets of real numbers, floating-point numbers, and integers respectively.

  Assume that floating point arithmetic complies with the IEEE 754-2008 standard \cite{ieee754} in ``to nearest even'' rounding mode and that underflow occurs gradually, although methods to handle abrupt underflow will be considered in Section \ref{sec:indexed_implementation_underflow_abrupt}.

  Let $f = s \times 2^e \times m \in \F$ be a floating-point number represented in IEEE 754-2008 format \cite{ieee754} where $s = \pm 1$ is the \textbf{sign}, $e_{\max} \geq e \geq e_{\min}$ is the \textbf{exponent} ($\exp(f)$ is defined to be $e$), $p$ is the \textbf{precision}, and $m = m_0.m_1m_2...m_{p-1}$ where $m_0, ..., m_{p - 1} \in \{0, 1\}$ is the \textbf{significand} of $f$. $f$ is said to be \textbf{normalized} if $m_0 = 1$ and $e > e_{\min}$, \textbf{unnormalized} if $m_0 = 0$, and \textbf{denormalized} if $m_0 = 0$ and $e = e_{\min}$. $f = 0$ if all $m_k = 0$ and $e = e_{\min}$.

  Machine epsilon, $\epsilon$, the difference between 1 and the greatest floating point number smaller than 1, is defined as $\epsilon = 2^{-p}$.

  The unit in the last place of $f \in \F$, $\ulp(f)$, is the spacing between two consecutive floating point numbers of the same exponent as $f$. If $f$ is normalized, $\ulp(f) = 2^{\exp(f) - p + 1} = 2 \times \epsilon \times 2^{\exp(f)}$.

  The unit in the first place of $f \in F$, $\ufp(f)$, is the value of the first significant bit of $f$. If $f$ is normalized, $\ufp(f) = 2^{\exp(f)}$.

  $\fl(\cdot)$ denotes the evaluated result of an expression in floating point arithmetic.

  We define the function $\roundtonearestinfty(x, k), x \in R, k \in \Z$ as

  \begin{equation}
    \roundtonearestinfty(x, k) = \begin{cases}\floor{x/2^k + 1/2} \times 2^k \text{ if } x \geq 0\\ \\ \ceil{x/2^k - 1/2}\times 2^k \text{ otherwise}\end{cases}
  \end{equation}

  $\roundtonearestinfty(x, k)$ rounds $x$ to the nearest multiple of $2^k$, breaking ties away from 0. A property of such rounding is shown in Equation \ref{eq:round}

  \begin{equation}
    |x - \roundtonearestinfty(x, k)| \leq 2^{k - 1}
    \label{eq:round}
  \end{equation}

  As ReproBLAS is written in C, \verb|float| and \verb|double| refer to the floating point types specified in the 1989 C standard \cite{c89} and we assume that they correspond to the \verb|binary-32| and \verb|binary-64| types in the IEEE 754-2008 floating point standard \cite{ieee754}.

  All indices start at $0$ in correspondence with the actual ReproBLAS implementation.
\section{The Indexed Type}
  \subsection{Section Summary}
    An indexed type is is composed of several accumulators, each accumulating input in certain ranges (more formally defined later as bins). We begin in Section \ref{sec:indexed_binning} with an original mathematical theory to support the binning scheme. Section \ref{sec:indexed_representation} serves as a summary of the indexed type defined in \cite{repsum}.
    Section \ref{sec:indexed_implementation_datatypes} explains implementation details of \cite{repsum} of the indexed type. Section \ref{sec:indexed_implementation_primary_carry} elaborates on the indexed type by explaining the ``1-Reduction Technique'' of \cite{repsum} using the new binning theory.
    Sections \ref{sec:indexed_implementation_overflow}, \ref{sec:indexed_implementation_underflow_gradual}, \label{sec:indexed_implementation_underflow_abrupt}, and \ref{sec:indexed_implementation_exceptions} contain original extensions of the indexed type and associated algorithms to a larger (more practical) range of inputs.
  \subsection{Binning}
    \label{sec:indexed_binning}
    We start by dividing the exponent range $(e_{\min} - p, ..., e_{\max} + 1]$ into \textbf{bins} $(a_i, b_i]$ of \textbf{width} $W$ according to Equations \ref{eq:a} and \ref{eq:b}.

    \begin{equation}
      a_i = e_{\max} - W + 1 - i\times W, 0 \leq i < (e_{\max} - e_{\min} + p - 1)/W
      \label{eq:a}
    \end{equation}

    \begin{equation}
      b_i = a_i + W
      \label{eq:b}
    \end{equation}

    We say the bin $(a_{i_0}, b_{i_0}]$ is \textbf{greater} than the bin $(a_{i_1}, b_{i_1}]$ if $a_{i_0} < a_{i_1}$ (which is equivalent to both $b_{i_0} > b_{i_1}$ and $i_0 < i_1$).

    We say the bin $(a_{i_0}, b_{i_0}]$ is \textbf{lesser} than the bin $(a_{i_1}, b_{i_1}]$ if $a_{i_0} > a_{i_1}$ (which is equivalent to both $b_{i_0} < b_{i_1}$ and $i_0 > i_1$).

    We use $i < (e_{\max} - e_{\min} + p - 1)/W$ to ensure that $a_i > e_{\min} - p + 1$ as discussed in Section \ref{sec:indexed_implementation_underflow_gradual}. This means that the greatest bin is

    \begin{equation}
      (e_{\max} + 1 - W, e_{\max} + 1]
      \label{eq:binmax}
    \end{equation}

    and the least bin is

    \begin{equation}
      (e_{\min} - p + 2 + ((e_{\max} - e_{\min} + p - 1)\mod W),
      e_{\min} - p + 2 + W + ((e_{\max} - e_{\min} + p - 1)\mod W)]
      \label{eq:binmin}
    \end{equation}

    Section \ref{sec:indexed_implementation_underflow_gradual} explains why the range

    \begin{equation*}
    (e_{\min} - p, e_{\min} - p + 2 + ((e_{\max} - e_{\min} + p - 1) \mod W)]
    \end{equation*}

    is ignored.

    Throughout the text we will refer to the \textbf{slice} of some $x \in \F$ in the bin $(a_i, b_i]$. $x$ can be split into several slices, each slice corresponding to a bin $(a_i, b_i]$ and expressible as the (possibly negated) sum of a subset of $\{2^e, e \in (a_i, b_i]\}$, such that the sum of the slices provides a good approximation of $x$. Specifically, the slice of $x \in \F$ in the bin $(a_i, b_i]$ is defined as $d(x, i)$ in Equation \ref{eq:d}.
    \begin{equation}
      d(x, i) = \roundtonearestinfty(x - \sum\limits_{k=0}^{i - 1}d(x,k), a_i + 1)
      \label{eq:d}
    \end{equation}

    As it may be unclear that $d(x, i)$ is well defined, Theorem \ref{thm:ddefined} follows.
    \begin{thm}
      $d(x, i)$ is well defined for all bins $(a_i, b_i]$ and $x \in \F$.
      \label{thm:ddefined}
    \end{thm}
    \begin{proof}
      We show the claim by induction on $i$.

      In the base case, $d(x, 0) = \roundtonearestinfty(x, a_0 + 1)$ and $d(x, 0)$ is well defined for all $x \in \F$.

      In the inductive step, assume that $d(x, 0), ..., d(x, i)$ are well defined for all $x \in \F$. $d(x, i + 1)$ is an expression in well defined terms.
    \end{proof}

    \ref{thm:dzero} and \ref{thm:dmiddle} follow from the definition of $d(x, i)$

    \begin{thm}
      For all bins $(a_i, b_i]$ and $x \in \F$ such that $|x| < 2^{a_i}$, $d(x, i) = 0$
      \label{thm:dzero}
    \end{thm}
    \begin{proof}
      We show the claim by induction on $i$.

      In the base case, we have $|x| < 2^{a_0}$ and $d(x, 0) = \roundtonearestinfty(x, a_0 + 1)$. By the triangle inequality and Equation \ref{eq:round},
      \begin{equation*}
        |\roundtonearestinfty(x, a_0 + 1)| \leq |x| + |x - \roundtonearestinfty(x, a_0 + 1)| < 2^{a_0 + 1}
      \end{equation*}
      Since $\roundtonearestinfty(x, a_0 + 1) \in 2^{a_0 + 1}\Z$, we then have
      \begin{equation*}
        d(x, 0) = \roundtonearestinfty(x, a_0 + 1) = 0
      \end{equation*}

      In the inductive step, we have $|x| < 2^{a_{i + 1}} \leq ... \leq 2^{a_0}$ by Equation \ref{eq:a} and therefore $d(x, i)= ... = d(x, 0) = 0$. Thus,
      \begin{equation*}
        d(x, i + 1) = \roundtonearestinfty(x - \sum\limits_{k = 0}^{i}d(x, k), a_{i + 1} + 1) = \roundtonearestinfty(x, a_{i+1} + 1)
      \end{equation*}
      Again by the triangle inequality and Equation \ref{eq:round},
      \begin{equation*}
        |\roundtonearestinfty(x, a_{i + 1} + 1)| \leq |x| + |x - \roundtonearestinfty(x, a_{i + 1} + 1)| < 2^{a_{i + 1} + 1}
      \end{equation*}
      Since $\roundtonearestinfty(x, a_{i + 1} + 1) \in 2^{a_{i + 1} + 1}\Z$, we then have
      \begin{equation*}
        d(x, i + 1) = \roundtonearestinfty(x, a_{i + 1} + 1) = 0
      \end{equation*}
    \end{proof}

    \begin{thm}
      For all bins $(a_i, b_i]$ and $x \in \F$ such that $|x| < 2^{b_i}$, $d(x, i) = \roundtonearestinfty(x, a_i + 1)$
      \label{thm:dmiddle}
    \end{thm}
    \begin{proof}
      The claim is a simple consequence of Theorem \ref{thm:dzero}.

      By Equations \ref{eq:a} and \ref{eq:b}, $|x| < 2^{b_i} \leq 2^{a_{i - 1}} \leq  ... \leq 2^{a_0}$. Therefore Theorem \ref{thm:dzero} implies $d(x, 0) = ... = d(x, i - 1) = 0$ and we have
      \begin{equation*}
        d(x, i) = \roundtonearestinfty(x - \sum\limits_{k = 0}^{i - 1}d(x, k), a_{i} + 1) = \roundtonearestinfty(x, a_{i} + 1)
      \end{equation*}
    \end{proof}

    Theorem \ref{thm:dzero}, Theorem \ref{thm:dmiddle}, and Equation \ref{eq:d} can be combined to yield an equivalent definition of $d(x, i)$ for all bins $(a_i, b_i]$ and $x \in \F$.

    \begin{equation}
      d(x, i) = \begin{cases}0 \text{ if } |x| < 2^{a_i}\\ \roundtonearestinfty(x, a_i + 1) \text{ if } 2^{a_i} \leq |x| < 2^{b_i}\\\roundtonearestinfty(x - \sum\limits_{k=0}^{i - 1}d(x,k), a_i + 1) \text{ if } 2^{b_i} \leq |x| \end{cases}
      \label{eq:d2}
    \end{equation}

    Theorem \ref{thm:dround} shows that sum of the slices of $x \in \F$ provides a good approximation of $x$.

    \begin{thm}
      For all bins $(a_i, b_i]$ and $x \in \F$, $|x - \sum \limits_{k = 0}^id(x, k)| \leq 2^{a_i}$
      \label{thm:dround}
    \end{thm}

    \begin{proof}
      We apply Equation \ref{eq:round} and \ref{eq:d2}
      \begin{equation*}
        |x - \sum \limits_{k = 0}^{i}d(x, k)| = |(x - \sum \limits_{k = 0}^{i - 1}d(x, k)) - d(x, i)|
      \end{equation*}
      \begin{equation*}
         = |(x - \sum \limits_{k = 0}^{i - 1}d(x, k)) - \roundtonearestinfty(x - \sum \limits_{k = 0}^{i - 1}d(x, k), a_{i} + 1)| \leq 2^{a_{i}}
      \end{equation*}
    \end{proof}

  \subsection{Representation}
    \label{sec:indexed_representation}
    The \textbf{indexed type} is used to represent the intermediate result of accumulation using Algorithms 6 and $7$ in \cite{repsum}.
    An indexed type $Y$ is a data structure composed of several accumulator data structures $Y_0, ..., Y_{fold - 1}$. The number of accumulators in an indexed type is referred to as its \textbf{fold}. Due to their low accuracy, indexed types of fold 1 are not considered.

    Let $Y$ be the indexed type corresponding to the reproducibly computed sum of $x_0, ..., x_{n - 1} \in \F$.
    Each accumulator $Y_j$ is a data structure used to accumulate the slices of input in the bin $(a_{I + j}, b_{I + j}]$ where $I$ is the \textbf{index} of $Y$. The \textbf{width} of an indexed type is equal to the width of its bins, $W$. As discussed in \cite{repsum}, $W < p - 2$. As discussed in Section \ref{sec:indexed_implementation_overflow}, $2\times W > p + 1$.

    The accumulators in an indexed type correspond to contiguous bins in decreasing order. The index of $Y$ is defined as the least $I \in \Z$ such that $2^{b_I + 1} > \max(|x_0|, ..., |x_n|)$ (equivalently, the least $I \in \Z$ such that $b_I > \max(e_0, ..., e_n)$). If $Y$ has index $I$, then $Y_j, j \in \{0, ..., fold - 1\}$ accumulates slices of input in the bin $(a_{I + j}, b_{I + j}]$.

  \subsection{Implementation}
    \subsubsection{Data Types}
      \label{sec:indexed_implementation_datatypes}
      As discussed in \cite{repsum}, indexed types are represented using floating point numbers to minimize traffic between floating point and integer arithmetic units. If an indexed type is used to sum \verb|doubles|, then it is composed entirely of \verb|doubles| and likewise for \verb|floats|. ReproBLAS supports complex types as pairs of real and imaginary components (stored contiguously in memory). If an indexed type is used to sum complex \verb|doubles| or \verb|floats|, then it is composed of pairs (real part, imaginary part) of \verb|doubles| or \verb|floats| respectively. The decision to keep the real and imaginary components together (as opposed to keeping separate indexed types for real and imaginary parts of the sum) was motivated by a desire to process accumulators simultaneously with vectorized (SIMD) instructions.

    \subsubsection{Primary and Carry}
      \label{sec:indexed_implementation_primary_carry}
      The accumulators $Y_j$ of an indexed type $Y$ are each implemented using two underlying floating point fields. The \textbf{primary} field ${Y_j}_P$ is used during accumulation, while the \textbf{carry} field ${Y_j}_C$ holds overflow from the primary field. Because primary fields are frequently accessed sequentially, the primary fields and carry fields are each stored contiguously in separate arrays. The notation for the primary field ${Y_j}_P$ and carry field ${Y_j}_C$ corresponds to the ``$S_k$'' and ``$C_k$'' of Algorithm $6$ in \cite{repsum}.

      The value ${\mathcal{Y}_j}_P$ represented by data stored in the primary field ${Y_j}_P$ is an offset from $1.5\times \epsilon^{-1} \times 2^{a_{I + j}}$ (corresponding to ``$M_{[i]}$'' at the beginning of Section IV.A. in \cite{repsum}), where $I$ is the index of $Y$, as shown in Equation \ref{eq:pri} below.
      \begin{equation}
        {\mathcal{Y}_j}_P = {Y_j}_P - 1.5\times\epsilon^{-1}\times2^{a_{I + j}}
        \label{eq:pri}
      \end{equation}
      This greatly simplifies the process of extracting the slices of input in bins $(a_{I + j}, b_{I + j}]$. It will be shown in Theorem \ref{thm:ddeposit} that if we represent each primary value ${\mathcal{Y}_j}_P$ as in (\ref{eq:pri}) and keep ${Y_j}_P$ within the range $(\epsilon^{-1} \times 2^{a_{I + j}}, 2 \times \epsilon^{-1} \times 2^{a_{I + j}})$, Algorithm \ref{alg:deposit} extracts the slices of $x$ in bins $(a_I, b_I], ..., (a_{I + fold - 1}, b_{I + fold - 1}]$ and adds them to ${Y_0}_P, ..., {Y_{fold - 1}}_P$ without error (and hence reproducibly) for all $x \in \F, |x| < 2^{b_I}$.
      \begin{alg}
        Extract slices of floating point $x \in \F, |x| < 2^{b_I}$ in bins $(a_I, b_I], ..., (a_{I + fold - 1}, b_{I + fold - 1}]$ and add to indexed $Y$. Here, $(r | 1)$ represents the result of setting the last bit of the mantissa ($m_{p - 1}$) of floating point number $r$ to 1. This is a restatement of lines 15-17 of Algorithm $6$ in \cite{repsum}
        \begin{algorithmic}[1]
          \Require
          \Statex No overflow occurs.
          \Statex Operations are performed in ``to nearest'' rounding mode.
          \Statex $|x| < 2^{b_I}$.
          \Statex ${Y_j}_P \in (\epsilon^{-1} \times 2^{a_{I + j}}, 2 \times \epsilon^{-1} \times 2^{a_{I + j}})$ at all times.
          \Function{DepositRestricted}{fold, x, Y}
            \State $r = x$ \label{alg:deposit:rgetsx}
            \For{$j = 0 \To (fold - 2)$} \label{alg:deposit:loop}
              \State $S = {Y_j}_P + (r | 1)$ \label{alg:deposit:split}
              \State $q = S - {Y_j}_P$
              \State ${Y_j}_P = S$
              \State $r = r - q$\label{alg:deposit:endsplit}
            \EndFor
            \State ${Y_{fold - 1}}_P = {Y_{fold - 1}}_P + (r | 1)$
          \EndFunction
          \Ensure
          \Statex The amount added to ${Y_j}_P$ by this algorithm is exactly $d(x, I + j)$.
        \end{algorithmic}
        \label{alg:deposit}
      \end{alg}
      The last bit of $r$ is set to break ties when rounding ``to nearest even'' such that the amount added to ${Y_j}_P$ does not depend on the size of ${Y_j}_P$ so far. The amount added to each ${Y_j}_P$ is exactly $d(x, I + j)$, a well-defined quantity that depends only on $x$ and $I + j$.

      \begin{thm}
        Let $Y$ be an indexed type of index $I$ and fold $fold$. Assume that we run Algorithm \ref{alg:deposit} on $Y$ and some $x \in \F$, $|x| < 2^{b_I}$. For all $j \in \{0, ..., fold - 1\}$, if ${Y_j}_P \in (\epsilon^{-1} \times 2^{a_{I + j}}, 2 \times \epsilon^{-1} \times 2^{a_{I + j}})$ at all times, then the amount added to ${Y_j}_P$ is exactly $d(x, I + j)$
        \label{thm:ddeposit}
      \end{thm}
      \begin{proof}
        Throughout the proof, assume that the phrase ``for all $j$'' means ``for all $j \in \{0, ..., fold - 1\}$.'' Assume also that $r_j$ and $S_j$ refer to the value of $r$ and $S$ after executing line \ref{alg:deposit:split} in the $j^{th}$ iteration of the loop. Finally, assume ${Y_j}_P$ to refers to the initial value of ${Y_j}_P$ and $S_j$ refers to the final value of ${Y_j}_P$. Therefore, $S_j - {Y_j}_P$ is the amount added to ${Y_j}_P$.

        Note that lines \ref{alg:deposit:split}-\ref{alg:deposit:endsplit} correspond to Algorithm 4 of \cite{repsum}.
        Therefore, if $\ulp({Y_j}_P) = \ulp(S_j)$ and $\ulp(r_j) < \frac{1}{2}\ulp({Y_j}_P)$, Corollary 3 of \cite{repsum} applies and we have that $S_j - {Y_j}_P \in \ulp({Y_j})\Z \in 2^{a_{I + j} + 1}\Z$ and that $|r_{j + 1}| \leq \frac{1}{2}\ulp({Y_j}_P) = 2^{a_{I + j}}$.

        As ${Y_j}_P, S_j \in (\epsilon^{-1} \times 2^{a_{I + j}}, 2 \times \epsilon^{-1} \times 2^{a_{I + j}})$, we have $\ulp(S_j) = \ulp({Y_j}_P)$ for all $j$.

        We show $|r_j| \leq 2^{b_{I + j}} = 2^{a_{I + j + 1}}$ for all $j$ inductively. As a base case, since $r_0 = x$ (from line \ref{alg:deposit:rgetsx}), $|r_0| = |x| < 2^{b_{I}}$.
        As an inductive step, if $|r_j| \leq 2^{b_{I + j}}$ then Theorem \ref{thm:underflowulp} (which will be shown in Section \ref{sec:indexed_implementation_underflow_gradual}) yields $\ulp(r_j) < \frac{1}{2}\ulp({Y_j}_P)$. Thus, Corollary 3 of \cite{repsum} applies and we have that $|r_{j + 1}| \leq \frac{1}{2}\ulp({Y_j}_P) = 2^{a_{I + j}}$.

        Next, we show $S_j - {Y_j}_P = \roundtonearestinfty(r_j, a_{I + j} + 1)$. As Corollary 3 of \cite{repsum} applies for all $j$, then $S_j - {Y_j}_P \in 2^{a_{I + j} + 1}\Z$. By Theorem 3 of \cite{repsum}, $r_{j + 1} = r_j - (S_j - {Y_j}_P)$. Since $|r_{j + 1}| \leq 2^{a_{I + j}}$, we consider two cases.

        If $|r_j - (S_j - {Y_j}_P)| < 2^{a_{I + j}}$, then ${S_j}_P - {Y_j}_P = \roundtonearestinfty(r_j, a_{I + j} + 1)$.

        If $|r_j - (S_j - {Y_j}_P)| = 2^{a_{I + j}}$, then $r_j \in 2^{a_{I + j}}\Z$ and because $\ulp(r_j) < 2^{a_{I + j}}$, $(r_j | 1) - r_j > 0$ and the rounding ``to nearest'' in line \ref{alg:deposit:split} must have gone away from 0. $S_j - {Y_j}_P = \roundtonearestinfty(r_j, a_{I + j} + 1)$.

        We can now show $r_{j + 1} = x - \sum\limits_{i = 0}^{I + j}d(x, i)$ and $S_j - {Y_j}_P = d(x, I + j)$ for all $j$ by induction on $j$.

        In the base case, $S_0 - {Y_0}_P = \roundtonearestinfty(r_0, a_I + 1) = \roundtonearestinfty(x, a_I + 1)$. As $|x| < b_I$, Theorem \ref{thm:dmiddle} implies $S_0 - {Y_0}_P = d(x, I)$. By Theorem 3 of \cite{repsum}, $r_1 = r_0 - (S_0 - {Y_0}_P) = x - d(x,I)$. By assumption and Equations \ref{eq:a} and \ref{eq:b}, $|x| < b_I \leq a_{i}$ for all $i \in \{0, ..., I - 1\}$, and therefore by Theorem \ref{thm:dzero}, $r_1 = x - \sum\limits_{i = 0}^Id(x, i)$.

        In the inductive step, assume $r_{j + 1} = x - \sum\limits_{i = 0}^{I + j}d(x, i)$. Then by definition,

        \begin{equation*}
          S_{j + 1} - {Y_{j + 1}}_P = \roundtonearestinfty(r_{j + 1}, a_{I + j + 1} + 1) = \roundtonearestinfty(x - \sum\limits_{i = 0}^{I + j}d(x, i), a_{I + j + 1} + 1) = d(x, I + j + 1)
        \end{equation*}

        And by Theorem 3 of \cite{repsum},
        \begin{equation*}
          r_{j + 2} = r_{j + 1} - (S_{j + 1} - {Y_{j + 1}}_P) = (x - \sum\limits_{i = 0}^{I + j}d(x, i)) - d(x, I + j + 1) = x - \sum\limits_{i = 0}^{I + j + 1}d(x, i)
        \end{equation*}
      \end{proof}

      Because $d(x, I + j) = 0$ for bins with $x < 2^{a_{I + j}}$, the top $fold$ nonzero accumulator values can be computed reproducibly by computing the values in the top $fold$ accumulators needed for the largest $x$ seen so far and shifting them upwards (towards index $0$) as necessary. \cite{repsum} contains further discussions of the reproducibility of the algorithm.

      In order to keep the primary fields in the necessary range during a deposit operation and to keep the representation of $Y_j$ unique, ${Y_j}_P$ is routinely renormalized to the range $[1.5 \times\epsilon^{-1} \times 2^{a_{I + j}}, 1.75 \times\epsilon^{-1} \times 2^{a_{I + j}})$.
      To renormalize, ${Y_j}_P$ is incremented or decremented by $0.25 \times\epsilon^{-1} \times 2^{a_{I + j}}$ when necessary, leaving the carry field ${Y_j}_C$ to record the number of such adjustments.
      The value ${\mathcal{Y}_j}_C$ stored in the carry field ${Y_j}_C$ of an indexed type $Y$ of index $I$ is expressed in Equation \ref{eq:car}
      \begin{equation}
        {\mathcal{Y}_j}_C = {Y_j}_C \times 0.25\times\epsilon^{-1}\times2^{a_{I + j}}
        \label{eq:car}
      \end{equation}
      Combining Equations \ref{eq:pri} and \ref{eq:car}, we get that the value $\mathcal{Y}_j$ stored in the accumulator $Y_j$ of an indexed type $Y$ of index $I$ is
      \begin{equation}
        \mathcal{Y}_j = {\mathcal{Y}_j}_P + {\mathcal{Y}_j}_C = ({Y_j}_P - 1.5 \times\epsilon^{-1}\times 2^{a_{I + j}}) + {Y_j}_C \times 0.25\times\epsilon^{-1}\times2^{a_{I + j}}
        \label{eq:acc}
      \end{equation}
      Therefore, using Equation \ref{eq:acc}, the value $\mathcal{Y}$ represented by an indexed type $Y$ of index $I$ and fold $fold$ (the sum of $Y$'s accumulators) is
      \begin{equation}
        \mathcal{Y} = \sum\limits_{j = 0}^{fold - 1} \mathcal{Y}_j = \sum\limits_{j = 0}^{fold - 1} (({Y_j}_P - 1.5 \times\epsilon^{-1}\times 2^{a_{I + j}}) + {Y_j}_C \times 0.25\times\epsilon^{-1}\times2^{a_{I + j}})
        \label{eq:indexedvalue}
      \end{equation}
      It is worth noting here that because the primary field ${Y_0}_P$ is stored with an exponent of $a_I + p$, it is unnecessary to store the index of an indexed type explicitly. The index can be determined by simply examining the exponent of ${Y_0}_P$, as all $a_I$ are distinct and the mapping between the exponent of ${Y_0}_P$ and the index of $Y$ is bijective.

    \subsubsection{Overflow}
      \label{sec:indexed_implementation_overflow}
      If an indexed type $Y$ has index 0 and the width is $W$, then the value in the primary field ${Y_0}_P$ is stored as an offset from $1.5\times\epsilon^{-1}\times2^{e_{\max} + 1 - W}$. However, $1.5\times\epsilon^{-1}\times2^{e_{\max} + 1 - W} = 1.5 \times 2^{e_{\max} + 1 + (p - W)} > 2 \times 2^{e_{\max}}$ since $W < p - 2$ \cite{repsum}, so it is out of the range of the floating-point system and not representable. Before discussing the solution to this overflow problem, take note of Theorem \ref{thm:overflow}.
      \begin{thm}
        If $2\times W > p + 1$, then for any indexed type $Y$ of index $I$ and any ${Y_j}_P$ such that $I + j \geq 1$, $|{Y_j}_P| < 2^{e_{\max}}$.
        \label{thm:overflow}
      \end{thm}
      \begin{proof}
        $a_1 = e_{\max} + 1 - 2\times W$ by definition, therefore $a_1 < e_{\max} - p$ and since all quantities are integers, $a_1 \leq e_{\max} - p - 1$. As $a_0, a_1, ...$ is a positive decreasing sequence, $a_{I + j} \leq e_{\max} - p - 1$ since $I + j \geq 1$.

        ${Y_j}_P$ is kept within the range $(\epsilon^{-1} \times 2^{a_{I + j}}, 2 \times \epsilon^{-1} \times 2^{a_{I + j}})$, therefore
        \begin{equation*}
          |{Y_j}_P| < 2 \times \epsilon^{-1} \times 2^{a_{I + j}} \leq 2^{1 + p} \times 2^{e_{\max} - 1 - p} = 2^{e_{\max}}
        \end{equation*}
      \end{proof}
      By Theorem \ref{thm:overflow}, if $2\times W > p + 1$ then the only primary field that could possibly be in overflow is a primary field corresponding to bin 0, and all other primary fields have exponent less than $e_{\max}$. Therefore, we impose $2\times W > p + 1$ and express the value of the primary field corresponding to bin 0 as a scaled offset from $1.5\times2^{e_{\max}}$. Note that this preserves uniqueness of the exponent of the primary field corresponding to bin 0 because no other primary field has an exponent of $e_{\max}$. The value ${\mathcal{Y}_0}_P$ stored in the primary field ${Y_0}_P$ of an indexed type $Y$ of index 0 is expressed in Equation \ref{eq:pri0}.
      \begin{equation}
        {\mathcal{Y}_0}_P = 2^{p - W + 1}\times({Y_0}_P - 1.5\times2^{e_{\max}})
        \label{eq:pri0}
      \end{equation}
      Of course, what remains to be seen is how we can extract and add (as in Algorithm \ref{alg:deposit}) the components of a floating point number to an indexed type $Y$ of index 0. Algorithm \ref{alg:deposit0} shows the adaptation of Algorithm $\ref{alg:deposit}$ for types of index 0.
      \begin{alg}
        Extract components of floating point $x$ in bins $(a_0, b_0], ..., (a_{fold - 1}, b_{fold - 1}]$ and add to indexed $Y$ of index 0. Here, $(r | 1)$ represents the result of setting the last bit of the mantissa ($m_{p - 1}$) of floating-point $r$ to 1.
        \begin{algorithmic}[1]
          \Require
            \Statex All requirements (except for the abscence of overflow, which we will ensure) from Algorithm \ref{alg:deposit} except that ${Y_0}_P$ must now be kept within the range $(2^{e_{\max}}, 2 \times 2^{e_{\max}})$ if $Y$ has index 0.
          \Function{Deposit}{fold, x, Y}
            \If{Y has index 0}
              \State $r = x / 2^{p - W + 1}$
              \State $S = {Y_0}_P + (r | 1)$
              \State $q = S - {Y_0}_P$
              \State ${Y_0}_P = S$
              \State $q = q \times 2^{p - W}$
              \State $r = x \times 0.5$ \label{alg:deposit0:halfx}
              \State $r = r - q$ \label{alg:deposit0:formr}
              \State $r = r \times 2.0$ \label{alg:deposit0:formrtwice}
              \For{$j = 1 \To (fold - 2)$}
                \State $S = {Y_j}_P + (r | 1)$
                \State $q = S - {Y_j}_P$
                \State ${Y_j}_P = S$
                \State $r = r - q$
              \EndFor
              \State ${Y_{fold - 1}}_P = {Y_{fold - 1}}_P + (r | 1)$
            \Else
              \State\Call{DepositRestricted}{fold, x, Y}
            \EndIf
          \EndFunction
          \Ensure
            \Statex No overflow occurs during the algorithm.
            \Statex The amount added to ${Y_j}_P$ is exactly $d(x, I + j)$ if $I + j \neq 0$.
            \Statex The amount added to ${Y_0}_P$ is exactly $d(x, 0)/2^{p - W + 1}$ if $I = 0$.
        \end{algorithmic}
        \label{alg:deposit0}
      \end{alg}

      Algorithm \ref{alg:deposit0} has a few key features worth pointing out in the case that $Y$ has index 0. First, since $|x| < 2^{e_{\max} + 1}$, then the value of $r$ after its first assignment satisfies $|r| < 2^{e_{\max} - p + W + 1}$. This shows both that the extraction of the top denormalized bin will work correctly and that the top bin is the greatest bin necessary to deposit any finite $x \in \F$.
      Furthermore, since $x$ is scaled by a power of two, the significand of $r$ is equal to that of $x$.
      Therefore, the amount added to ${Y_0}_P$ is $\roundtonearestinfty(x/2^{p - W + 1}, a_0 - (p - W + 1) + 1) = d(x, 0)/2^{p - W + 1}$. We can therefore scale this slice by $2^{p - W}$ to obtain half the value of the slice of $x$ belonging to bin $0$. As $x < 2 \times 2^{e_{\max}}$, $d(x, 0) \leq 2 \times 2^{e_{\max}}$. It is for this reason that $x$ is halved (to ensure that no overflow occurs) in line \ref{alg:deposit0:halfx} before half of $r$ is formed in line \ref{alg:deposit0:formr}. Because we know that the unscaled value of $r$ is representable, we can double the half of $r$ in line \ref{alg:deposit0:formrtwice}. We can then continue as in Algorithm \ref{alg:deposit}.

      Notice that after the first extraction, we cannot simply subtract $q$ from $r$ and then scale $r$ up and continue, as there may have been underflow when scaling $x$ down. We must scale $q$ up to half its true value and subtract it from half the original $x$. This will result (at worst) in underflow of the smallest representable bit, but this bit does not matter to the algorithm as shown in Section \ref{sec:indexed_implementation_underflow_gradual}. Although it likely does not matter in practice, this detail ensures that proofs in Section \ref{sec:indexed_implementation_primary_carry} may extend to this overflow case.
    \subsubsection{Gradual Underflow}
      \label{sec:indexed_implementation_underflow_gradual}
      Here we consider the effects of gradual underflow on algorithms described in \cite{repsum} and how they can be avoided. In Algorithm \ref{alg:deposit}, when the last bit of $r$ is set to 1, it is assumed that this last bit will only serve to fix the direction of the rounding mode. However, if ${Y_j}_P$ is denormalized, that last bit may very well be accumulated!

      For Algorithm \ref{alg:deposit} to work correctly when adding $r | 1$ to ${Y_j}_P$, $\ulp(r)$ must be less than rounding error when adding a number to ${Y_j}_P$. Mathematically, $\ulp(r) < \frac{1}{2}\ulp({Y_j}_P)$.

      \begin{thm}
        For any primary field ${Y_j}_P$ of an indexed type $Y$ of index $I$ and any $r \in \F$, $\ulp(r) < \frac{1}{2}\ulp({Y_j}_P)$ and $a_{I + j} > e_{\min} - p + 1$.
        \label{thm:underflowulp}
      \end{thm}
      \begin{proof}
      The least bin is

      \begin{equation*}
      (e_{\min} - p + 2 + ((e_{\max} - e_{\min} + p - 1 ) \mod W), e_{\min} - p + 2 + W + ((e_{\max} - e_{\min} + p - 1)\mod W)]
      \end{equation*}

      Because ${Y_j}_P \in (\epsilon \times a_{I + j}, 2 \times \epsilon \times a_{I + j})$ and

      \begin{equation*}
      e_{\min} - p + 2 + ((e_{\max} - e_{\min} + p- 1) \mod W) > e_{\min} - p + 2
      \end{equation*}

      ${Y_j}_P$ is normalized and the unit in the last place of ${Y_j}_P$ is $a_{I + j} + 1 > e_{\min} - p + 2$.

      If $r$ is normalized, then because $\ulp(r) \leq 2^{1 - p}|r| \leq 2^{b_{I + j} - (p - 1)} \leq 2^{a_{I + j} + W - (p - 1)}$, and $W < p - 2$, we have $\ulp(r) < \frac{1}{2}\ulp({Y_j}_P)$. (This case is considered in \cite{repsum}).

      If $r$ is denormalized, $\ulp(r) = 2^{e_{\min} - p + 1}$ since the unit in the last place of a denormalized number is always equal to $2^{e_{\min} - p + 1}$. Therefore $\ulp(r) = 2^{e_{\min} - p + 1} < 2^{a_{I + j}} = \frac{1}{2}\ulp({Y_j}_P)$.
      \end{proof}

      Since $W < p - 2$,

      \begin{equation*}
        a_{\floor{(e_{\max} - e_{\min} + p - 1)/W} - 1} = e_{\min} - p + 2 + ((e_{\max} - e_{\min} + p - 1 ) \mod W) \leq {e_{\min} - 1}
      \end{equation*}

      As a consequence, we can use Theorems \ref{thm:underflowulp} and \ref{thm:dround} to say that for any $x \in \F$,
      \begin{equation*}
        |x - \sum\limits_{i = 0}^{\floor{(e_{\max} - e_{\min} + p - 1)/W}} d(x, i)| \leq 2^{e_{\min} - 1}
      \end{equation*}
      This means that we can approximate $x$ using the sum of its slices to within $2^{e_{\min} - 1}$.

      It is possible to deposit the input in the denormalized range. One simple way the algorithm could be extended to denormalized inputs would be to scale the least bins up, as was done to handle overflow. Due to the relatively low priority for accumulating denormalized values, this method was not implemented in ReproBLAS.

    \subsubsection{Abrupt Underflow}
      \label{sec:indexed_implementation_underflow_abrupt}
      If underflow is abrupt, several approaches may be taken to adapt the algorithms in this paper.

      The simplest approach would be to accumulate input in the denormalized range by scaling the smaller inputs up. This has the added advantage of increasing the accuracy of the algorithm. A major disadvantage to this approach is the additional branching cost incurred due to the conditional scaling.

      A more efficient way to solve the problem would be to set the least bin to have $a_i = 2^{e_{\min} - 1}$. This could be accomplished either by keeping the current binning scheme and having the least bin be of a width not necessarily equal to $W$, or by shifting all other bins to be greater. The disadvantage of shifting the other bins is that it may cause multiple greatest bins to overflow, adding multiple scaling cases. Setting such a least bin would enforce the condition that no underflow occurs since all intermediate sums are either $0$ or greater than the underflow threshold. The denormal range would be discarded.

    \subsubsection{Exceptions}
      \label{sec:indexed_implementation_exceptions}
      Indexed types are capable of representing exceptional cases such as \verb|NaN| (Not a Number) and \verb|Inf| (Infinity). An indexed type $Y$ stores its exception status in its first primary field ${Y_0}_P$.

      A value of $0$ in ${Y_0}_P$ indicates that nothing has been added to ${Y_0}_P$ yet (${Y_0}_P$ is initialized to $0$).

      The lowest bin is

      \begin{equation*}
      (e_{\min} - p + 2 + ((e_{\max} - e_{\min} + p - 1 ) \mod W), e_{\min} - p + 2 + W + ((e_{\max} - e_{\min} + p - 1)\mod W)]
      \end{equation*}

      and since the ${Y_j}_P$ are kept within the range $(\epsilon^{-1} \times 2^{a_{I + j}}, 2 \times \epsilon^{-1} \times 2^{a_{I + j}})$ (where $I$ is the index), the least possible ${Y_j}_P$ is

      \begin{equation*}
      \epsilon^{-1} \times 2^{e_{\min} - p + 2 + ((e_{\max} - e_{\min} + p - 1)\mod W)} > 2^{e_{\min}}
      \end{equation*}

      Therefore the value of $0$ in a primary field is unused in any previously specified context and may be used as a sentinel value. (As the exponent of $0$ is distinct from the exponent of normalized values, the bijection between the index of an indexed type $Y$ and the exponent of ${Y_0}_P$ is preserved)

      A value of \verb|Inf| or \verb|-Inf| in ${Y_0}_P$ indicates that one or more \verb|Inf| or \verb|-Inf| (and no other exceptional values) have been added to $Y$ respectively.

      A value of \verb|NaN| in ${Y_0}_P$ indicates that one or more \verb|NaN| have been added to $Y$ or one or more of both \verb|Inf| and \verb|-Inf| have been added to $Y$.

      As the ${Y_j}_P$ are kept finite to store finite values, \verb|Inf|, \verb|-Inf|, and \verb|NaN| are unused in any previously specified context and are valid sentinel values. (As the exponent of \verb|Inf|, \verb|-Inf|, and \verb|NaN| is distinct from the exponent of finite values, the bijection between the index of an indexed type $Y$ and the exponent of ${Y_0}_P$ is preserved)

      This behavior follows the behavior for exceptional values in IEEE 754-2008 floating point arithmetic and is therefore easy to implement. At the beginning of \ref{alg:deposit}, we may simply check $x$ and ${Y_0}_P$ for the exceptional values \verb|Inf|, \verb|-Inf|, and \verb|NaN|. If any one of $x$ or ${Y_0}_P$ is indeed exceptional, we add $x$ to the (possibly finite) ${Y_0}_P$. Otherwise, we deposit the finite value normally.

      As \verb|Inf|, \verb|-Inf|, and \verb|NaN| add associatively, this behavior is reproducible.

      It should be noted here that it is possible to achieve a final result of \verb|Inf| or \verb|-Inf| when ${Y_0}_P$ is finite. This is due to the fact that the indexed representation can express values outside of the range of the floating point numbers that it is composed with. More specifically, it is possible for the value $\mathcal{Y}$ represented by the indexed type $Y$ to satisfy $|\mathcal{Y}| \geq 2 \times 2^{e_{\max}}$. The condition that $\mathcal{Y}$ is not representable is discovered when calculating $\mathcal{Y}$ (converting $Y$ to a floating point number). The methods used to avoid overflow and correctly return the \verb|Inf| or \verb|-Inf| are discussed in Section \ref{sec:convert}.

\section{Basic Operations}
  \subsection{Section Summary}
  \subsection{Update}
    As noted in Algorithm \ref{alg:deposit}, when adding $x \in \F$ to an indexed type $Y$ of index $I$ and fold $fold$, we make the assumption that $|x| < 2^{b_I}$. However, this is not always the case and sometimes it is necessary to adjust the index of $Y$. This adjustment is called an \textbf{update}. The process of updating $Y$ to have index $J > I$ is summarized succinctly in Algorithm \ref{alg:update}.
    \begin{alg}
      Update indexed $Y$ of index $I$ and fold $fold$ to have an index suitable to deposit $x$ where the bin $(a_{J}, b_{J}]$ satisfies $2^{b_{J}} > |x| \geq 2^{a_{J}}$.
      \begin{algorithmic}[1]
        \Function{Update}{fold, x, Y}
          \If{$J < I$}
            \State $[{Y_{\min(I - J, fold)}}_P, ..., {Y_{fold - 1}}_P] = [{Y_0}_P, ..., {Y_{fold - 1 - \min(I - J, fold)}}_P]$
            \State $[{Y_0}_P, ..., {Y_{\min(I - J, fold) - 1}}_P] = [1.5 \times \epsilon^{-1} \times a_{J}, ..., 1.5 \times \epsilon^{-1} \times a_{\min(I, fold + J) - 1}]$
            \State $[{Y_{\min(I - J, fold)}}_C, ..., {Y_{fold - 1}}_C] = [{Y_0}_C, ..., {Y_{fold - 1 - \min(I - J, fold)}}_C]$
            \State $[{Y_0}_C, ..., {Y_{\min(I - J, fold) - 1}}_C] = [0, ..., 0]$
          \EndIf
        \EndFunction
      \end{algorithmic}
      \label{alg:update}
    \end{alg}
    The update operation is described in the update section of Algorithm $6$ in \cite{repsum}.

    It should be noted that if ${Y_0}_P$ is 0, then the update is performed as if $I + fold < J$. If ${Y_0}_P$ is \verb|Inf|, \verb|-Inf|, or \verb|NaN|, then $Y$ is not modified by an update.

    To speed up this operation, the factors $1.5 \times \epsilon^{-1} \times a_k$ for all valid $k \in Z$ are stored in a precomputed array.

  \subsection{Deposit}
    The deposit operation (here referred to as Algorithm \ref{alg:deposit}, a special case where $fold = 3$ is described in the extract $K$ first bins section of Algorithm $6$ in \cite{repsum}) lies firmly at the heart of ReproBLAS.

    The indexed type $Y$ that results from depositing the floating point values $x_0, ..., x_{n - 1}$ into an empty indexed type is referred to as the \textbf{indexed sum} of $x_0, ..., x_{n - 1}$

  \subsection{Renormalize}
    \label{sec:renormalize}
    When depositing values into an indexed type $Y$ of index $I$, the assumption is made that ${Y_j}_P \in (\epsilon^{-1}\times 2^{a_{I + j}}, 2 \times \epsilon^{-1}\times 2^{a_{I + j}})$ throughout the routine. To enforce this condition, the indexed type must be renormalized every $2^{p - w - 2}$ deposit operations. The renormalization procedure is shown in Algorithm \ref{alg:renorm}.
    \begin{alg}
      For an indexed $Y$ with index $I$, assuming ${Y_j}_P \in [1.25 \times \epsilon^{-1}\times 2^{a_{I + j}}, 2 \times \epsilon^{-1}\times 2^{a_{I + j}})$, renormalize $Y$ such that ${Y_j}_P \in [1.5 \times \epsilon^{-1}\times 2^{a_{I + j}}, 1.75 \times \epsilon^{-1}\times 2^{a_{I + j}})$.
      \begin{algorithmic}[1]
        \Function{Renorm}{fold, Y}
          \For{$j = 0 \To fold - 1$}
            \If{${Y_j}_P < 1.5 \times \ufp({Y_j}_P)$}
              \State ${Y_j}_P = {Y_j}_P + 0.25 \times \ufp({Y_j}_P)$
              \State ${Y_j}_C = {Y_j}_C - 1$
            \EndIf
            \If{${Y_j}_P \geq 1.75 \times \ufp({Y_j}_P)$}
              \State ${Y_j}_P = {Y_j}_P - 0.25 \times \ufp({Y_j}_P)$
              \State ${Y_j}_C = {Y_j}_C + 1$
            \EndIf
          \EndFor
        \EndFunction
      \end{algorithmic}
      \label{alg:renorm}
    \end{alg}
    The renormalization operation is described in the carry-bit propagation section of Algorithm $6$ in \cite{repsum}, although it has been slightly modified so as not to include an extraneous case.

    To show the reasoning behind the assumptions in Algorithm \ref{alg:renorm}, consider the case in which floating point $x_0, x_1, ...$ are successively deposited in an indexed type $Y$ which satisfies ${Y_j}_P \in [1.5 \times \epsilon^{-1}\times 2^{a_{I + j}}, 1.75 \times \epsilon^{-1}\times 2^{a_{I + j}})$. (Such a condition is satisfied upon initialization of an empty $Y$)

    First, note that $|d(x_k, a_{I + j}, b_{I + j})| \leq 2^{b_{I + j}}$, where $d(x_k, a_{I + j}, b_{I + j})$ is the amount added to ${Y_j}_P$ on iteration $j$. This is due to the definition of $d(x_k, a_{I+j}, b_{I + j})$ or to Corollary 3 of \cite{repsum}.

    As the deposit operation extracts and adds the appropriate component of $x_k$ exactly (assuming ${Y_j}_P$ lies within the appropriate boundaries at each step, which will be shown),

    Therefore,

    \begin{equation*}
    |\sum \limits_{n = 0}^{k - 1} d(x_k, a_{I + j}, b_{I + j})| \leq n \times 2^{b_{I + j}} = n \times 2^{W} \times 2^{a_{I + j}}
    \end{equation*}

    and if $n <= 2^{p - W - 2}$, then after the $n^{th}$ deposit

    \begin{equation*}
    {Y_j}_P \in [(1.5 \times \epsilon^{-1} - n \times 2^W)\times 2^{a_{I + j}}, (1.75 \times \epsilon^{-1} + n \times 2^W)\times 2^{a_{I + j}})
    \end{equation*}

    \begin{equation*}
    \in [1.25 \times \epsilon^{-1}\times 2^{a_{I + j}}, 2 \times \epsilon^{-1}\times 2^{a_{I + j}})
    \end{equation*}

    Therefore, after another renormalization, the primary fields would once again satisfy

    \begin{equation*}
    {Y_j}_P \in [1.5 \times \epsilon^{-1}\times 2^{a_{I + j}}, 1.75 \times \epsilon^{-1}\times 2^{a_{I + j}})
    \end{equation*}

  \subsection{Reduce}
    An operation to produce the sum of two indexed types is necessary to perform a reduction. For completeness we include the algorithm here, although apart from the renormalization step, it is equivalent to Algorithm $7$ in \cite{repsum}.
    \begin{alg}
      For an indexed type $Y$ (the indexed sum of some $x_0, ..., x_{n - 1} \in \F$) with index $I$ and indexed type $Z$ with index $J$ (the indexed sum of some $x_n, ..., x_{n + m - 1} \in \F$), add $Z$ to $Y$. That is, set $Y$ to the indexed sum of $x_0, ..., x_{n + m - 1}$. At the start and end of the algorithm, we may assume

    \begin{equation*}
      {Y_j}_P \in [1.5 \times \epsilon^{-1}\times 2^{a_{I + j}}, 1.75 \times \epsilon^{-1}\times 2^{a_{I + j}})
    \end{equation*}
    and
    \begin{equation*}
      {Z_j}_P \in [1.5 \times \epsilon^{-1}\times 2^{a_{J + j}}, 1.75 \times \epsilon^{-1}\times 2^{a_{J + j}})
    \end{equation*}
      \begin{algorithmic}[1]
        \Function{Reduce}{fold, Y, Z}
          \If{$J > I$}
            \State $R = Z$
            \State \Call{Reduce}{fold, $R$, $Y$}
            \State $Y = R$
          \EndIf
          \For{$j = 0 \To J$}
            \State ${Y_{j + I - J}}_P = {Y_{j + I - J}}_P + {Z_j}_P - 1.5 \times \epsilon^{-1} \times 2^{a_{I + j}}$
            \State ${Y_{j + I - J}}_C = {Y_{j + I - J}}_C + {Z_j}_C$
          \EndFor
          \State \Call{Renorm}{fold, $Y$}
        \EndFunction
      \end{algorithmic}
      \label{alg:reduce}
    \end{alg}

  \subsection{Convert}
    \label{sec:convert}
    It is necessary to provide the ability to convert between indexed and floating point representations of a number.

    Converting a floating point number to an indexed type should produce, for transparency and reproducibility, an indexed type equivalent to the indexed sum of the floating point number.
    The procedure is very simply summarized by Algorithm \ref{alg:conv2indexed}
    \begin{alg}
      Convert floating point $x$ to indeed $Y$ with fold $fold$.
      \begin{algorithmic}[1]
        \Function{ConvertFloatToIndexed}{fold, x, Y}
          \State $Y = 0$
          \State \Call{Update}{fold, x, Y}
          \State \Call{Deposit}{fold, x, Y}
          \State \Call{Renorm}{fold, Y}
        \EndFunction
      \end{algorithmic}
      \label{alg:conv2indexed}
    \end{alg}

    Converting an indexed type to a floating point number is more difficult. However, because \cite{repsum} guarantees that all fields in the indexed type are reproducible, as long as the fields are operated upon deterministically, any method to evaluate Equation \ref{eq:indexedvalue} accurately and without unnecessary overflow is suitable. Following the methods in \cite{sortsum}, we add the floating point numbers in order of decreasing unnormalized exponent using a higher intermediate precision. As discussed in $\cite{sortsum}$, the intermediate precision must be greater than $p + \ceil{\log_2(2 \times fold)}$ as there are $2 \times fold$ elements to be summed.

    When adding the fields it is not necessary to examine the values in the fields or sort them explicitly. Their unnormalized exponent does not depend on their values, and their unnormalized exponents have a predetermined order.

    Consider indexed $Y$ of index $I$ and fold $fold$.
    As each value ${\mathcal{Y}_j}_P$ in a primary field ${Y_j}_P$ is an offset from $1.5 \times \epsilon^{-1} \times 2^{a_{I + j}}$ and ${Y_j}_P \in (\epsilon^{-1} \times 2^{a_{I + j}}, 2 \times \epsilon^{-1} \times 2^{a_{I + j}})$, ${\mathcal{Y}_j}_P$ can be expressed exactly using an unnormalized floating point number ${\mathcal{Y}'_P}_j$ with an exponent of $a_{I + j} + p - 1$.
    As each carry field ${Y_j}_C$ is a count of renormalization adjustments later scaled by $0.25 \times \epsilon^{-1} \times 2^{a_{I + j}}$, ${\mathcal{Y}_j}_C$ can be expressed exactly using an unnormalized floating point number ${\mathcal{Y}'_j}_C$ with an exponent of $a_{I + j} + p + p - 3$.

    For all $k < j$, $\exp({\mathcal{Y}'_k}_P) > \exp({\mathcal{Y}'_j}_P)$ and $\exp({\mathcal{Y}'_k}_C) > \exp({\mathcal{Y}'_j}_C)$ because $a_{I + k} > a_{I + j}$.

    Note that
    \begin{equation*}
      \exp({\mathcal{Y}'_j}_C) = a_{I + j} + p + p - 3
    \end{equation*}

    and

    \begin{equation*}
      \exp({\mathcal{Y}'_{j - 1}}_P) = a_{I + j - 1} + p - 1 = a_{I + j} + W + p - 1
    \end{equation*}

    Therefore $\exp({\mathcal{Y}'_j}_C) > \exp({\mathcal{Y}'_{j - 1}}_P)$ using $W < p - 2$.

    Note that

    \begin{equation*}
      \exp({\mathcal{Y}'_{j - 2}}_P) = a_{I + j - 1} + p - 1 = a_{I + j} + 2\times W + p - 1
    \end{equation*}

    Therefore $\exp({\mathcal{Y}'_j}_C) < \exp({\mathcal{Y}'_{j - 2}}_P)$ using $2 \times W > p + 1$.

    Combining the above inequalities,

    \begin{equation*}
    \exp({\mathcal{Y}'_0}_C)
    \end{equation*}

    \begin{equation*}
    > \exp({\mathcal{Y}'_1}_C)
    \end{equation*}

    \begin{equation*}
    > \exp({\mathcal{Y}'_0}_P)
    \end{equation*}

    \begin{equation*}
    > \exp({\mathcal{Y}'_2}_C)
    \end{equation*}

    \begin{equation*}
    > \exp({\mathcal{Y}'_1}_P)
    \end{equation*}

    \begin{equation*}
    \vdots
    \end{equation*}

    \begin{equation*}
    > \exp({\mathcal{Y}'_{fold - 2}}_C)
    \end{equation*}

    \begin{equation*}
    > \exp({\mathcal{Y}'_{fold - 3}}_P)
    \end{equation*}

    \begin{equation*}
    > \exp({\mathcal{Y}'_{fold - 1}}_C)
    \end{equation*}

    \begin{equation*}
    > \exp({\mathcal{Y}'_{fold - 2}}_P)
    \end{equation*}

    \begin{equation*}
    > \exp({\mathcal{Y}'_{fold - 1}}_P)
    \end{equation*}

    These unnormalized floating point numbers may, for convenience of notation, be referred to in decreasing order of unnormalized exponent as $\gamma'_0, ..., \gamma'_{2 \times fold - 1}$.
    $\gamma_k$ denotes the normalized representations of the $\gamma'_k$, and it should be noted that $\gamma_k = \gamma'_k$ as real numbers and that $\exp(\gamma_k) \leq \exp(\gamma'_k)$.

    It should be noted that the ${\mathcal{Y}'_j}_P$ and the ${\mathcal{Y}'_j}_C$ can be expressed exactly using floating point types of the same precision as ${Y_j}_P$ and ${Y_j}_C$ (except in the case of overflow, in which a scaled version may be obtained), and such exact floating point representations can be obtained using Equation \ref{eq:pri} and Equation \ref{eq:car}.

    Notice that $|\gamma'_0| = |{\mathcal{Y}'_0}_C| < 2 \times 2^{\exp({\mathcal{Y}'_0}_C)} = 2 \times 2^{e_{\max} + 1 - W + p + p - 3}$ and $\exp(\gamma'_0) > ... > \exp(\gamma'_{2 \times fold - 1})$.  Therefore $|\gamma'_k| \leq 2^{e_{\max} - W + p + p - 1 - k}$. The absolute value represented by an indexed type can therefore be bounded by

    \begin{equation}
      \label{eq:maxindexedvalue}
      \sum\limits_{k = 0}^{2 \times fold - 1} |\gamma_k| < \sum\limits_{k = 0}^{2 \times fold - 1} 2^{e_{\max} - W + p + p - 1 - k} < \sum\limits_{k = 0}^{\infty} 2^{e_{\max} - W + p + p - 1 - k} = 2^{e_{\max} - W + p + p}
    \end{equation}

    If the intermediate precision has a maximum exponent greater than or equal to $e_{\max} - W + p + p - 1$, then no special cases to guard against overflow are needed.

    Algorithm \ref{alg:conv2float} represents a conversion routine in such a case.

    \begin{alg}
      Convert indexed $Y$ with index $I$ and fold $fold$ to floating point $x$. Here, $z$ is a floating point type with precision $P$ greater than $p + \ceil{\log_2(2 \times fold)}$ and maximum exponent $E_{\max}$ greater than $e_{\max} - W + p + p$
      \begin{algorithmic}[1]
        \Function{ConvertIndexedToFloat}{fold, x, Y}
          \State $z = {\mathcal{Y}_0}_C$
          \For{$j = 1 \To fold - 1$}
            \State $z = z + {\mathcal{Y}_j}_C$
            \State $z = z + {\mathcal{Y}_{j - 1}}_P$
          \EndFor
          \State $z = {\mathcal{Y}_{fold - 1}}_P$
          \State $x = z$
        \EndFunction
      \end{algorithmic}
      \label{alg:conv2float}
    \end{alg}

    If an intermediate type with exponent greater than ore equal to $e_{\max} - W + p + p - 1$ is not available, the $\gamma_k$ must be scaled down by some factor during addition and the sum scaled back up when subsequent additions can no longer effect an overflow situation.

    If the sum is to overflow, it will overflow regardless of the values of any ${\mathcal{Y}_j}_P$ or ${\mathcal{Y}_j}_C$ with $|{\mathcal{Y}_j}_P| < 0.5 \times 2^{-P} \times 2^{e_{\max}}$ or $|{\mathcal{Y}_j}_C| < 0.5 \times 2^{-P} \times 2^{e_{\max}}$. If the floating point sum of precision $P$ has exponent greater than or equal to $e_{\max}$ these numbers are not large enough to have any effect when added to the sum. If the sum has exponent less than $e_{\max}$, then additions of these numbers cannot cause the exponent of the sum to exceed $e_{\max}$ for similar reasons.

    As the maximum absolute value of the true sum is strictly smaller than $2^{e_{\max} - W + p + p}$, a sufficient scaling factor is $2^{p + p - W - 2}$, meaning that the maximum absolute value of the true scaled sum is strictly smaller $2 \times 2^{e_{\max} - 1}$ (and since it will be shown later that the computed sum is accurate to within a small factor of the true sum, the computed sum will stay strictly smaller than $2 \times 2^{e_{\max}}$ and will not overflow.)

  When $\exp(\gamma'_k) < e_{\max} - P - 1$, the sum may be scaled back up and the remaining numbers added without scaling. Notice that no overflow can occur during addition in this algorithm. If an overflow is to occur, it will happen only when scaling back up.

    If the sum is not going to overflow, then the smaller $y'_k$ must be added as unscaled numbers to avoid underflow.

    Algorithm \ref{alg:conv2floatoverflow} represents a conversion routine in such a case.

    \begin{alg}
      Convert indexed $Y$ with index $I$ and fold $fold$ to floating point $x$. Here, $z$ is a floating point type with precision $P$ greater than $p + \ceil{\log_2(2 \times fold)}$.
      \begin{algorithmic}[1]
        \Function{ConvertIndexedToFloat}{fold, x, Y}
          \State $j = 1$
          \While{$j \leq 2 \times fold$ and $\exp(\gamma_j) \geq e_{\max} - P - 1$}
            \State $z = z + (\gamma_j / 2^{p + p - W - 2})$
            \State $j = j + 1$
          \EndWhile
          \State $z = z \times 2^{p + p - W - 2}$
          \While{$j \leq 2 \times fold$}
            \State $z = z + \gamma_j$
            \State $j = j + 1$
          \EndWhile
          \State $x = z$
        \EndFunction
      \end{algorithmic}
      \label{alg:conv2floatoverflow}
    \end{alg}

    If an indexed type is composed of \verb|float|, then \verb|double| provides sufficient precision and exponent to use as an intermediate type and Algorithm \ref{alg:conv2float} may be used to convert to a floating point number.
    However, if an indexed type is composed of \verb|double|, many machines may not have any higher precision available. To provide a conversion function that is identical across hardware, we use \verb|double-double| precision implemented in software to achieve sufficient intermediate precision \cite{doubledouble}. As this does not extend the exponent range we must use Algorithm \ref{alg:conv2floatoverflow} to convert to a floating point number.

  \subsection{Error Bound}
    \begin{thm}
      Given $n$ floating point numbers $f_0, ..., f_{n - 1}$ for which there exists (possibly unnormalized) floating point numbers $f'_0, ..., f'_{n - 1}$ such that
      \begin{enumerate}
        \item $f_k = f'_k$ for all $k \in \{0, ..., n - 1\}$
        \item $\exp(f'_0) > ... > \exp(f'_{n - 1})$
        \item $\exp(f'_k) \geq \exp(f'_{k + 2}) + \ceil{\frac{p + 1}{2}}$ for all $k \in \{0, ..., n - 3\}$
      \end{enumerate}
      \label{thm:mysortsum}
      Then we have
      \begin{equation*}
        |\sum \limits_{k = 0}^{n - 1} f_k - \fl(\sum \limits_{k = 0}^{n - 1} f_k)| < 16\epsilon|\sum\limits_{k = 0}^{n - 1}f_k|
      \end{equation*}
    \end{thm}

    \begin{proof}
      Let $S_0 = \overline{S_0} = f_0$, $S_k = S_{k - 1} + f_k$, and $\overline{S_k} = \fl(\overline{S_{k - 1}} + f_k)$ so that $S_{n - 1} = \sum \limits_{k = 0}^{n - 1} f_k$ and $\overline{S_{n - 1}} = \fl(\sum \limits_{k = 0}^{n - 1} f_k)$.

      Let $m$ be the location of the first error such that $S_{m - 1} = \overline{S_{m - 1}}$ and $S_{m} \neq \overline{S_{m}}$.

      If no such $m$ exists then the computed sum is exact ($\fl(\sum \limits_{k = 0}^{n - 1} f_k) = \sum \limits_{k = 0}^{n - 1} f_k$) and we are done.

      If such an $m$ exists, then because $\exp(f_0') > ... > \exp(f_m')$, $f_0, ..., f_m \in \ulp(f_m')\Z$. Thus, $S_m \in \ulp(f_m')\Z$.

      Assume for contradiction that $|S_m| < 2 \times 2^{\exp(f_m')}$. Because $S_m \in \ulp(f_m')\Z$, this would imply that $S_m$ is representable as a floating point number, a contradiction as $\overline{S_m} \neq S_m$. Therefore, we have
      \begin{equation}
        |S_m| \geq 2 \times 2^{\exp(f_m')}
        \label{eq:smbound}
      \end{equation}

      If $n - 1 \geq m + 1$, then because $\exp(f_m') > \exp(f_{m + 1}')$,
      \begin{equation}
        |f_{m + 1}| < 2\times2^{\exp(f_m' - 1)} = 2^{\exp(f_m')}
        \label{eq:smpbound}
      \end{equation}

      If $n - 1 \geq m + 2$, then because $\exp(f_m') \geq \exp(f_{m + 2}') + \ceil{\frac{p + 1}{2}}$ and $\exp(f_0') > ... > \exp(f_{n - 1}')$,
      \begin{equation*}
        |\sum \limits_{k = m + 2}^{n - 1} f_k| < \sum \limits_{k = m + 2}^{n - 1} |f_k| < \sum \limits_{k = m + 2}^{n - 1} 2 \times 2^{\exp(f_k')} \leq \sum \limits_{k = m + 2}^{n - 1} 2 \times 2^{\exp(f_m') - \ceil{\frac{p + 1}{2}} - (m + 2 - k)}
      \end{equation*}
      \begin{equation}
        < \sum \limits_{k = 0}^{\infty} 2 \times \sqrt{\epsilon} \times 2^{\exp(f_m') - k} = 4 \times \sqrt\epsilon \times 2^{\exp(f_m')}
        \label{eq:smppbound}
      \end{equation}

      We can combine Equations \ref{eq:smpbound} and \ref{eq:smppbound} to obtain

      \begin{equation}
        |\sum\limits_{k = m + 1}^{n - 1} f_k| \leq \sum\limits_{k = m + 1}^{n - 1} |f_k| < 2^{\exp{f_m'}} + 4 \times \sqrt\epsilon \times 2^{\exp(f_m')} = (1 + 4 \times \sqrt\epsilon )\times 2^{\exp(f_m')}
        \label{eq:smsbound}
      \end{equation}

      By Equations \ref{eq:smbound} and \ref{eq:smsbound},

      \begin{equation*}
        |\sum\limits_{k = 0}^{n - 1} f_k| \geq |\sum\limits_{k = 0}^{m} f_k| - |\sum\limits_{k = m + 1}^{n - 1} f_k| = |S_m| - |\sum\limits_{k = m + 1}^{n - 1} f_k|
      \end{equation*}

      \begin{equation}
        \geq 2 \times 2^{\exp(f_{m}')} - (1 + 4 \times \sqrt\epsilon)\times 2^{\exp(f_m')} = (1 - 4 \times \sqrt\epsilon)\times 2^{\exp(f_m')}
        \label{eq:sbound}
      \end{equation}

      By Equations \ref{eq:sbound} and \ref{eq:smsbound},
      \begin{equation}
        \sum\limits_{k = m + 1}^{n - 1}|f_k| \leq (1 + 4 \times \sqrt\epsilon)\times2^{\exp(f_m')}\leq (\frac{1 + 4 \times \sqrt\epsilon}{1 - 4 \times \sqrt\epsilon})|\sum\limits_{k = 0}^{n - 1}f_k|
        \label{eq:smsrelsbound}
      \end{equation}

      And by Equations \ref{eq:sbound} and \ref{eq:smsrelsbound},

      \begin{equation}
        |S_m| \leq |\sum\limits_{k = 0}^{n - 1}f_k| + |\sum\limits_{k = m + 1}^{n - 1} f_k| \leq (1 + (\frac{1 + 4 \times \sqrt\epsilon}{1 - 4 \times \sqrt\epsilon}))|\sum\limits_{k = 0}^{n - 1}f_k| \leq (\frac{2}{1 - 4 \times \sqrt\epsilon})|\sum\limits_{k = 0}^{n - 1}f_k|
        \label{eq:smrelsbound}
      \end{equation}

      Notice that since $S_{m - 1} = \overline{S_{m - 1}}$,
      \begin{equation*}
        |\sum \limits_{k = 0}^{n - 1} f_k - \fl(\sum \limits_{k = 0}^{n - 1} f_k)| = |(S_{m - 1} + \sum \limits_{k = m}^{n - 1} f_k) - \fl(\overline{S_{m - 1}} + \sum \limits_{k = m}^{n - 1} f_k)|
      \end{equation*}

      If $n - 1 \leq m + 4$, we may combine Equations \ref{eq:smrelsbound} and \ref{eq:smsrelsbound} with Equation 2.4 from \cite{higham} to obtain

      \begin{equation*}
        |\sum \limits_{k = 0}^{n - 1} f_k - \fl(\sum \limits_{k = 0}^{n - 1} f_k)| = |(S_{m - 1} + \sum \limits_{k = m}^{n - 1} f_k) - \fl(\overline{S_{m - 1}} + \sum \limits_{k = m}^{n - 1} f_k)|
      \end{equation*}
      \begin{equation*}
        \leq (\frac{5\epsilon}{1 - 5 \epsilon})|S_{m - 1} + f_m| + (\frac{4\epsilon}{1 - 4 \epsilon})\sum\limits_{k = m + 1}^{n - 1}|f_k|
      \end{equation*}
      \begin{equation*}
        \leq (\frac{5\epsilon}{1 - 5 \epsilon})|S_m| + (\frac{4\epsilon}{1 - 4 \epsilon})\sum\limits_{k = m + 1}^{n - 1}|f_k|
      \end{equation*}
      \begin{equation*}
 \leq ((\frac{5\epsilon}{1 - 5 \epsilon})(\frac{2}{1 - 4 \times \sqrt\epsilon}) + (\frac{4\epsilon}{1 - 4 \epsilon})(\frac{1 + 4 \times \sqrt\epsilon}{1 - 4 \times \sqrt\epsilon}))|\sum\limits_{k = 0}^{n - 1}f_k|
      \end{equation*}
      And assuming $\epsilon << 1$,
      \begin{equation*}
        |\sum \limits_{k = 0}^{n - 1} f_k - \fl(\sum \limits_{k = 0}^{n - 1} f_k)| \leq 16\epsilon|\sum\limits_{k = 0}^{n - 1}f_k|
      \end{equation*}
      And we are done.

      If $n - 1 > m + 4$, we may combine Equations \ref{eq:smrelsbound} and \ref{eq:smsrelsbound} with Equation 2.4 from \cite{higham} to obtain

      \begin{equation*}
        |\sum \limits_{k = 0}^{m + 4} f_k - \fl(\sum \limits_{k = 0}^{m + 4} f_k)| = |(S_{m - 1} + \sum \limits_{k = m}^{m + 4} f_k) - \fl(\overline{S_{m - 1}} + \sum \limits_{k = m}^{m + 4} f_k)|
      \end{equation*}
      \begin{equation*}
        \leq (\frac{5\epsilon}{1 - 5 \epsilon})|S_{m - 1} + f_m| + (\frac{4\epsilon}{1 - 4 \epsilon})\sum\limits_{k = m + 1}^{m + 4}|f_k|
      \end{equation*}
      \begin{equation*}
        \leq (\frac{5\epsilon}{1 - 5 \epsilon})|S_m| + (\frac{4\epsilon}{1 - 4 \epsilon})\sum\limits_{k = m + 1}^{m + 4}|f_k|
      \end{equation*}
      \begin{equation}
 \leq ((\frac{5\epsilon}{1 - 5 \epsilon})(\frac{2}{1 - 4 \times \sqrt\epsilon}) + (\frac{4\epsilon}{1 - 4 \epsilon})(\frac{1 + 4 \times \sqrt\epsilon}{1 - 4 \times \sqrt\epsilon}))|\sum\limits_{k = 0}^{n - 1}f_k|
        \label{eq:smfiveerror}
      \end{equation}

      Notice that
        \begin{equation*}
          \exp(f_m') \geq \exp(f_{m + 2}') + \ceil{\frac{p+ 1}{2}} \geq \exp(f_{m + 4}') + 2 \times \ceil{\frac{p+ 1}{2}} > \exp(f_{m + 5}')+ 2 \times \ceil{\frac{p+ 1}{2}}
        \end{equation*}
      Therefore,
        \begin{equation}
          \exp(f_m') \geq \exp(f_{m + 5}') + p + 2
          \label{eq:fmfiveexp}
        \end{equation}
      Because $\exp(f_0') > ... > \exp(f_{n - 1}')$,
        \begin{equation}
          \sum\limits_{k = m + 5}^{n - 1} |f_k| < \sum\limits_{k = m + 5}^{n - 1} 2 \times 2^{\exp(f_m') - p - 2 - (k - (m + 5))} < \sum\limits_{k = 0}^{\infty} 2^{\exp(f_m') - p - 1 - k} = \epsilon \times 2^{\exp(f_m')}
          \label{eq:boundfmfivesum}
        \end{equation}
      And using Equation \ref{eq:sbound},
        \begin{equation}
          \sum\limits_{k = m + 5}^{n - 1} |f_k| < (\frac{\epsilon}{1 - 4 \times \sqrt\epsilon})|\sum\limits_{k = 0}^{n - 1} f_k|
          \label{eq:relsboundfmfivesum}
        \end{equation}

      By Equations \ref{eq:sbound}, \ref{eq:smfiveerror}, and \ref{eq:boundfmfivesum}.
        \begin{equation*}
          |\overline{S_{m + 4}}| \geq |S_{m + 4}| - |S_{m + 4} - \overline{S_m + 4}| \geq (|\sum\limits_{k = 0}^{n - 1} f_k| - \sum\limits_{k = m + 5}^{n - 1}|f_k|) - |S_{m + 4} - \overline{S_m + 4}|
        \end{equation*}
        \begin{equation*}
          \geq (1 - ((\frac{5\epsilon}{1 - 5 \epsilon})(\frac{2}{1 - 4 \times \sqrt\epsilon}) + (\frac{4\epsilon}{1 - 4 \epsilon})(\frac{1 + 4 \times \sqrt\epsilon}{1 - 4 \times \sqrt\epsilon})))|\sum\limits_{k = 0}^{n - 1}f_k| - \epsilon \times 2^{\exp(f_m')}
        \end{equation*}
        \begin{equation}
          \geq (1 - (\frac{10\epsilon}{1 - 5 \epsilon}) - (\frac{4\epsilon}{1 - 4 \epsilon})(1 + 4 \times \sqrt\epsilon) - \epsilon)\times 2^{\exp(f_m')}
          \label{eq:minsmfour}
        \end{equation}
        As we may assume $\epsilon << 1$, Equation \ref{eq:minsmfour} can be simplified to
        \begin{equation}
          |\overline{S_{m + 4}}| > 2^{\exp(f_m') - 1}
          \label{eq:minsmfoursimple}
        \end{equation}

        Using Equation \ref{eq:fmfiveexp},

        \begin{equation}
          |f_k| < 2 \times 2^{\exp(f_k')} \leq 2 \times 2^{\exp(f_m) - p - 2} = \epsilon \times 2^{\exp(f_m') - 1}
          \label{eq:maxfmfive}
        \end{equation}

        And by Equations \ref{eq:maxfmfive} and \ref{eq:minsmfour}, all additions after $f_{m + 4}$ have no effect and we have
        \begin{equation}
          \overline{S_{m + 4}} = \overline{S_{n - 1}}
          \label{eq:smfourcontinues}
        \end{equation}

        With Equations \ref{eq:relsboundfmfivesum} and \ref{eq:smfiveerror}

        \begin{equation*}
          |\sum \limits_{k = 0}^{n - 1} f_k - \fl(\sum \limits_{k = 0}^{n - 1} f_k)| = |S_{n - 1} - \overline{S_{n - 1}}| = |S_{n - 1} - \overline{S_{m + 4}}| \leq |S_{m + 4} - \overline{S_{m + 4}}| + \sum\limits_{k = m + 5}^{n - 1}|f_k|
        \end{equation*}
        \begin{equation*}
          \leq ((\frac{10\epsilon}{1 - 5 \epsilon}) + (\frac{4\epsilon}{1 - 4 \epsilon})(1 + 4 \times \sqrt\epsilon) + \epsilon)(\frac{1}{1 - 4 \times \sqrt\epsilon})|\sum\limits_{k = 0}^{n - 1}f_k|
        \end{equation*}

        And assuming $\epsilon << 1$,

        \begin{equation}
          |\sum \limits_{k = 0}^{n - 1} f_k - \fl(\sum \limits_{k = 0}^{n - 1} f_k)| < 16\epsilon|\sum\limits_{k = 0}^{n - 1}f_k|
          \label{eq:sortsum_finalbound}
        \end{equation}
    \end{proof}
    Consider the indexed sum $Y$ of fold $fold$ of floating point numbers $x_0, ..., x_{n - 1}$. We denote the true sum $\sum \limits_{k = 0}^{n - 1} x_k$ by $T$, the true value of the indexed sum as obtained using Equation \ref{eq:indexedvalue} by $\mathcal{Y}$, and the floating point approximation of $\mathcal{Y}$ obtained using an appropriate algorithm from section \ref{sec:convert} by $\overline{\mathcal{Y}}$.

    \cite{repsum} discusses the absolute error $|T - \mathcal{Y}|$ but does not give a method to construct $\overline{\mathcal{Y}}$ and therefore no error bound ($|T - \overline{\mathcal{Y}}|$) on the final floating point answer was given. Here we extend the error bound of \cite{repsum} all the way to the final return value of the algorithm.

    Throughout this section we assume that all input falls within a bin and that $T$ and $\mathcal{Y}$ are larger than the largest denormalized floating point number.

    To show an error bound for the conversion routine, we first state Theorem \ref{thm:sortsum}

    \begin{thm}
      This theorem is a restatement of statement 1 of Theorem $1$ in \cite{sortsum}.

      Assume we have a floating point format of precision $p$ and an intermediate format of precision $P$. Assume that the following hold:
      \begin{enumerate}
        \item $p \geq 2$ (there are at least 2 bits of precision in the bins)
        \item $P \geq p + 1$ (the intermediate precision is larger than the original)
        \item the intermediate exponent range is at least as large as the original
        \item rounding is in ``to nearest'' mode (breaking ties arbitrarily)
        \item underflow is gradual
        \item no overflow occurs
      \end{enumerate}
      Then, if we sum at most $1 + \floor{\frac{2^{P - p}}{(1 - 2^{-p})}}$ unnormalized numbers in order of decreasing exponent using the intermediate precision, the error is less than $(\frac{1}{1 - 2^{1 - p}} + \frac{1}{2})\times \ulp(T)$, where $T$ is the true sum.
      \label{thm:sortsum}
    \end{thm}

    It has been shown in \cite{repsum} that

    \begin{equation}
      \label{eq:repbound}
      |T - \mathcal{Y}| < n \times 2^{W \times (1 - fold) - 1} \times \max|x_k|
    \end{equation}

    Because the intermediate precision $P$ used to sum the values stored in the various fields of $Y$ is assumed greater than $p + \ceil{\log_2(2 \times fold)}$, we sum at most $2^{P - p} < 1 + \floor{\frac{2^{P - p}}{1 - 2^{-p}}}$ values that may be considered as unnormalized values in order of decreasing unnormalized exponent (the same argument is made in the proof of Theorem 2 of \cite{sortsum}), and Theorem \ref{thm:sortsum} applies to yield

    \begin{equation*}
      |\mathcal{Y} - \overline{\mathcal{Y}}| < (\frac{1}{1 - 2^{1 - p}} + \frac{1}{2})\times \ulp(\mathcal{Y})
    \end{equation*}

    By the triangle inequality

    \begin{equation*}
      |\mathcal{Y}| \leq |T| + |T - \mathcal{Y}| < |T| + n \times 2^{W \times (1 - fold) - 1} \times \max|x_k|
    \end{equation*}

    and because $T$ is assumed to be in the normalized range,

    \begin{equation*}
      \ulp(\mathcal{Y}) \leq \ulp(|T| + n \times 2^{W \times (1 - fold) - 1} \times \max|x_k|)
    \end{equation*}

    \begin{equation*}
    \leq (|T| + n \times 2^{W \times (1 - fold) - 1} \times \max|x_k|) \times 2 \times \epsilon
    \end{equation*}

    The above results can be used to obtain Equation \ref{eq:error}, the absolute error of the floating point approximation of an indexed sum $|T - \overline{\mathcal{Y}}|$.

    \begin{equation*}
      |T - \overline{\mathcal{Y}}| \leq |T - \mathcal{Y}| + |\mathcal{Y} - \overline{\mathcal{Y}}|
    \end{equation*}

    \begin{equation*}
      < n \times 2^{W \times (1 - fold) - 1} \times \max|x_k| + (\frac{1}{1 - 2^{1 - p}} + \frac{1}{2})\times \ulp(\mathcal{Y})
    \end{equation*}

    \begin{equation*}
      < n \times 2^{W \times (1 - fold) - 1} \times \max|x_k| + (\frac{1}{1 - 2^{1 - p}} + \frac{1}{2})\times(|T| + n \times 2^{W \times (1 - fold) - 1} \times \max|x_k|) \times 2 \times \epsilon
    \end{equation*}

    \begin{equation}
      < n \times \max|x_k|\times 2^{W \times (1 - fold) - 1} \times(1 + (\frac{1}{1 - 2^{1 - p}} + \frac{1}{2}) \times 2 \times \epsilon) + (\frac{1}{1 - 2^{1 - p}} + \frac{1}{2})\times |T| \times 2 \times \epsilon
      \label{eq:error}
    \end{equation}

    Equation \ref{eq:error} can be approximated as Equation \ref{eq:errorapprox}.

    \begin{equation*}
      |T - \overline{\mathcal{Y}}| < n \times \max|x_k|\times 2^{W \times (1 - fold) - 1} \times(1 + (\frac{1}{1 - 2^{1 - p}} + \frac{1}{2}) \times 2 \times \epsilon) + (\frac{1}{1 - 2^{1 - p}} + \frac{1}{2})\times |T| \times 2 \times \epsilon
    \end{equation*}

    \begin{equation}
    \approx n \times \max|x_k|\times 2^{W \times (1 - fold) - 1} + 3 \times \epsilon \times |T|
      \label{eq:errorapprox}
    \end{equation}

    A perhaps more useful mathematical construction is the error expressed relative to the result $\overline{\mathcal{Y}}$, and not the theoretical sum $T$. Again by the triangle inequality,

    \begin{equation*}
      |\mathcal{Y}| \leq |\overline{\mathcal{Y}}| + |\mathcal{Y} - \overline{\mathcal{Y}}|
    \end{equation*}

    Applying the bound on $|\mathcal{Y} - \overline{\mathcal{Y}}|$ yields

    \begin{equation*}
      |\mathcal{Y}| < |\overline{\mathcal{Y}}| + (\frac{1}{1 - 2^{1 - p}} + \frac{1}{2})\times \ulp(\mathcal{Y})
    \end{equation*}

    and because $\mathcal{Y}$ is assumed to be in the normalized range,

    \begin{equation*}
      |\mathcal{Y}| < |\overline{\mathcal{Y}}| + (\frac{1}{1 - 2^{1 - p}} + \frac{1}{2})\times|\mathcal{Y}|\times 2 \times \epsilon
    \end{equation*}

    After simplification,

    \begin{equation*}
      |\mathcal{Y}| < |\overline{\mathcal{Y}}|\times (1 - (\frac{1}{1 - 2^{1 - p}} + \frac{1}{2})\times 2 \times \epsilon)^{-1}
    \end{equation*}

    Again because $\mathcal{Y}$ is assumed to be in the normalized range,

    \begin{equation*}
      \ulp(\mathcal{Y}) \leq \ulp(|\overline{\mathcal{Y}}|\times (1 - (\frac{1}{1 - 2^{1 - p}} + \frac{1}{2})\times 2 \times \epsilon)^{-1})
    \end{equation*}

    \begin{equation*}
      \leq |\overline{\mathcal{Y}}|\times (1 - (\frac{1}{1 - 2^{1 - p}} + \frac{1}{2})\times 2 \times \epsilon)^{-1} \times 2 \times \epsilon
    \end{equation*}

    The above results can be used to obtain Equation \ref{eq:error2}, the absolute error of the floating point approximation of an indexed sum $|T - \overline{\mathcal{Y}}|$.

    \begin{equation*}
      |T - \overline{\mathcal{Y}}| \leq |T - \mathcal{Y}| + |\mathcal{Y} - \overline{\mathcal{Y}}|
    \end{equation*}

    \begin{equation*}
      < n \times 2^{W \times (1 - fold) - 1} \times \max|x_k| + (\frac{1}{1 - 2^{1 - p}} + \frac{1}{2})\times \ulp(\mathcal{Y})
    \end{equation*}

    \begin{equation}
      < n \times 2^{W \times (1 - fold) - 1} \times \max|x_k| + (\frac{1}{1 - 2^{1 - p}} + \frac{1}{2})\times|\overline{\mathcal{Y}}|\times (1 - (\frac{1}{1 - 2^{1 - p}} + \frac{1}{2})\times 2 \times \epsilon)^{-1} \times 2 \times \epsilon
      \label{eq:error2}
    \end{equation}

    Equation \ref{eq:error2} can be approximated as Equation \ref{eq:error2approx}.

    \begin{equation*}
      |T - \overline{\mathcal{Y}}| < n \times 2^{W \times (1 - fold) - 1} \times \max|x_k| + (\frac{1}{1 - 2^{1 - p}} + \frac{1}{2})\times|\overline{\mathcal{Y}}|\times (1 - (\frac{1}{1 - 2^{1 - p}} + \frac{1}{2})\times 2 \times \epsilon)^{-1} \times 2 \times \epsilon
    \end{equation*}

    \begin{equation}
      \approx n \times \max|x_k|\times 2^{W \times (1 - fold) - 1} + 3 \times \epsilon \times |\overline{\mathcal{Y}}|
      \label{eq:error2approx}
    \end{equation}

    We can compare Equation \ref{eq:errorapprox} to the error bound obtained if the accumulator fields were summed without extra precision. In this case, only the standard summation bound would apply and the absolute error would be bounded by

    \begin{equation*}
    n \times \max|x_k|\times 2^{W \times (1 - fold) - 1} + (2 \times fold - 1) \times \epsilon \times \sum\limits_0^{2 \times fold - 1}|\gamma_k|
    \end{equation*}

    \begin{equation*}
    = n \times \max|x_k|\times 2^{W \times (1 - fold) - 1} + (2 \times fold - 1) \times \epsilon \times |\mathcal{Y}|
    \end{equation*}

    \begin{equation*}
    < n \times \max|x_k|\times 2^{W \times (1 - fold) - 1} + (2 \times fold - 1) \times \epsilon \times (|T| + n \times \max|x_k|\times 2^{W \times (1 - fold) - 1})
    \end{equation*}

    \begin{equation*}
    = n \times \max|x_k|\times 2^{W \times (1 - fold) - 1}\times (1 + (2 \times fold - 1)\times \epsilon) + (2 \times fold - 1) \times \epsilon \times |T|
    \end{equation*}

    \begin{equation*}
    \approx n \times \max|x_k|\times 2^{W \times (1 - fold) - 1} + (2 \times fold - 1) \times \epsilon \times |T|
    \end{equation*}

    Which is not as tight a bound as Equation $\ref{eq:errorapprox}$, and begins to grow linearly as the user increases the fold in an attempt to increase accuracy.

  \subsection{Limits}
    \label{sec:limits}
    As discussed previously, the minimum fold accepted by ReproBLAS is 2. The maximum useful fold is $\floor{(e_{\max} - e_{\min} + p - 1)/W}$, as this covers all of the bins.

    As discussed in \cite{repsum}, $W < p - 2$. As discussed in section \ref{sec:indexed_implementation_overflow}, $2\times W > p + 1$.

    ReproBLAS uses the values $W = 40$ for indexed \verb|double| and $W = 13$ for indexed \verb|float|. $W$ is available as the \verb|XIWIDTH| macro.

    As discussed in section \ref{sec:indexed_implementation_underflow_gradual}, only input corresponding to the bin $(e_{\min} - 1, e_{\max} + 1]$ is guaranteed to be accumulated.

    As absolute value of individual quantities added to ${Y_j}_P$ are not in excess of $2^{b_{I + j}}$, a maximum of $0.25\epsilon^{-1}2^{-W}$ elements may be deposited into ${Y_j}_P$ between renormalizations, as discussed in section \ref{sec:renormalize}. For indexed \verb|double| this number is $2^{11}$, whereas for indexed \verb|float| this number is $2^9$. This number is supplied programmatically using the \verb|XIENDURANCE| macro.

    As ${Y_j}_C$ must be able to record additions of absolute value 1 without error, ${Y_j}_C$ must stay in the range $(-\epsilon^{-1}, \epsilon^{-1})$. As each renormalization results in addition not in excess absolute value of 1 to ${Y_j}_C$, a maximum of $(\epsilon^{-1} - 1)$ renormalizations may be performed, meaning that an indexed sum represents the sum of a maximum of $0.25\epsilon^{-1}2^{-W} \times (\epsilon^{-1} - 1) \approx 2^{p + p - W - 2}$. For indexed \verb|double| this number is almost $2^{64}$, whereas for indexed \verb|float| this number is almost $2^{33}$. This number is supplied programmatically using the \verb|XICAPACITY| macro.

    The indexed types provided by ReproBLAS will, when used correctly, avoid intermediate overflow.

\section{Interface}
  \subsection{Section Summary}
  \subsection{Fold}
  \subsection{Complex Types}
  \subsection{Naming Conventions}
  \subsection{Build System}
\section{indexed.h}
  \subsection{Section Summary}
  \subsection{Types}
  \subsection{Functions}
    \subsubsection{xixadd}
    \subsubsection{xixiadd}
    \subsubsection{xscale}
    \subsubsection{xixiaddsq}
\section{idxdBLAS.h}
  \subsection{Section Summary}
  \subsection{Functions}
    \subsubsection{xixdot}
    \subsubsection{xixnrm2}
    \subsubsection{xixgemv}
    \subsubsection{xixgemm}
  \subsection{Optimization}
    Due to the proportion of time spent in the deposit routine, optimization of the deposit routine was prioritized. In the event that multiple $x$ need to be added to $Y$, the deposit routine can be vectorized by accumulating the $x$ in multiple copies of $Y$. The number of copies of $Y$ to make, $c$, and the number of unrolled loop iterations, $u$ are tuning parameters.
\section{repBLAS.h}
  \subsection{Section Summary}
\section{idxdMPI.h}
  \subsection{Section Summary}
  \subsection{Types}
  \subsection{Functions}
    \subsubsection{XIXIREDUCE}
\section{Applications}
  \subsection{Section Summary}
  \subsection{Parallel Reproducible Dot Product}
  \subsection{Parallel Reproducible Vector Norm}
  \subsection{Parallel Reproducible Matrix-Vector Multiply}
  \subsection{Parallel Reproducible Matrix-Matrix Multiply}
\begin{thebibliography}{9}
  \bibitem{repsum}
    Demmel, James, and Hong Diep Nguyen. Parallel Reproducible Summation. IEEE Transactions on Computers, 2014.
  \bibitem{ieee754}
    IEEE Standard for Floating-Point Arithmetic, IEEE Std 754-2008 , vol., no., pp.1,70, Aug. 29 2008
  \bibitem{c89}
    ANSI/ISO 9899-1990 American National Standard for Programming Languages - C, section 6.1.2.5
  \bibitem{higham}
    Higham, Nicholas J. The accuracy of floating point summation. SIAM Journal on Scientific Computing 14, no. 4 (1993): 783-799.
  \bibitem{sortsum}
    Demmel, James W., and Yozo Hida. Accurate floating point summation. Computer Science Division, University of California, 2002.
  \bibitem{doubledouble}
    Hida, Yozo, Xiaoye S. Li, and David H. Bailey. Quad-double arithmetic: Algorithms, implementation, and application. 15th IEEE Symposium on Computer Arithmetic, 2000.
\end{thebibliography}
\end{document}
