\section{Conclusions And Future Work}
  The algorithms we have presented have been shown to accurately sum binary IEEE 754-2008 floating point reproducibly. The algorithms behave sensibly in overflow and underflow situations and work on exceptional cases such as \texttt{Inf}, \texttt{-Inf}, and \texttt{NaN}. Reproducibility is independent of the ordering of data, blocking schemes, or reduction tree shape. Our algorithms remain reproducible on heterogeneous systems independent of the number of processors.

   We have specified all of the necessary steps to carry out reproducible summation in practice, including a conversion from the intermediate indexed type to a floating point result. Our method allows for user-specified accuracy in the result, improving significantly on the accuracy of \cite{repsum}. We have also specified methods for reproducible absolute sums, dot products, norms, matrix-vector products, and matrix-matrix products. We have created an optimized library for reproducible summation, tested it, and shown that the performance is comparable to industry standards. We have included in our library methods for distributed memory reductions so that reproducible parallel routines may be built from our library.

  Allowing the user to adjust accuracy yields an interesting trade off between performance and accuracy. Using only the existing ReproBLAS interface, a basic long accumulator can be built by setting $K$ to its maximum value so that the entire exponent range is summed. A careful examination of the error bound \eqref{eq:error} shows that this would give almost exact results regardless of the dynamic range of the sum. However, because ReproBLAS was not designed with such a use case in mind, this would be very slow. Future work could also involve optimizing some of the routines for a high-accuracy use case. In such a scheme, when a number is deposited (as in Algorithm \ref{alg:deposit}), it would only be added to the 3 bins its slices fit in. Only accumulators with nonzero value would need to be stored, allowing for another possible optimization.

  In the future, we plan to add PBLAS routines to our library so that users may benefit from tested reproducible parallel BLAS routines without having to code them themselves. We will extend the existing interface to ReproBLAS with parallel BLAS function signatures.
