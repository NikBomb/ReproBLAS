\section{Composite Operations}
  \label{sec:compositeops}
  Although several reproducible summation algorithms can be built from the set of primitive operations defined in section \ref{sec:primitiveops}, it may be unclear how these operations can be composed to form such algorithms. What follows are the specifications for common summation-based algorithms in terms of the primitive operations.
  To obtain a general completely reproducible algorithm for summation that is, one must design for reproducibility under both data permutation and reduction tree shape.
  Section \ref{sec:compositeops_sum} details a general reproducible summation algorithm that is independent of input data ordering, providing analysis of its correctness and runtime. Section \ref{sec:compositeops_reduce} shows how the algorithm in Section \ref{sec:compositops_sum} may be extended to arbitrary reduction tree shapes, and gives another proof of correctness. Sections \ref{sec:compositeops_nrm}, \ref{sec:compositeops_dot}, \ref{sec:compositeops_gemv}, and \ref{sec:compositeops_gemm} explain how to obtain reasonably performant reproducible versions of representative operations from the BLAS.
  Implementation details of these routines are discussed in Section \ref{sec:implementation}.
  \subsection{Sum}
    \label{sec:compositeops_sum}
    Algorithm \ref{alg:sum} is an indexed summation algorithm that produces the indexed sum of a vector of floating point numbers $x_0, ..., x_{n - 1} \in \F$.
    The indexed sum of a list of numbers is independent of the ordering of the list.
    Algorithm \ref{alg:sum} uses only one indexed type to hold the intermediate result of the recursive summation, and the vast majority of time in the algorithm is spent in the deposit routine.
    \begin{samepage}
    \begin{alg}
      Return the $K$-fold indexed sum of $x_0, ..., x_{n-1}$.
      This is similar to Algorithm $6$ in \cite{repsum}, but requires no restrictions on the inputs $x_0, ..., x_{n - 1}$.
      \begin{algorithmic}[1]
        \Function{Sum}{K, [$x_0, ..., x_{n-1}$]}
          \State $Y = 0$
          \State $block = 0$
          \State $j = 0$ \label{alg:sum:setj}
          \While{$j < n$}\label{alg:sum:outerloop}
            \State $m = \min(n, j + 0.25\epsilon^{-1}2^{-W})$
            \State \Call{Update}{K, $\max([|x_j|, ..., |x_{m - 1}|)$, Y}\label{alg:sum:update}
            \While{$j < m$}
              \State \Call{Deposit}{K, $x_j$, Y}
              \State $j = j + 1$
            \EndWhile
            \State \Call{Renorm}{K, Y}\label{alg:sum:renorm}
          \EndWhile
          \State \Return $Y$
        \EndFunction
        \Ensure
        \Statex $Y$ is the unique indexed sum of $x_0, ..., x_{n - 1}$. This is equivalent to the following three statements.
        \Statex The index $I$ of $Y$ is the greatest integer such that $\max(|x_j|) < 2^{b_I}$.
        \Statex ${Y_k}_P \in [1.5  \epsilon^{-1} 2^{a_{I + k}}, 1.75  \epsilon^{-1} 2^{a_{I + k}})$ unless $I + k = 0$, in which case ${Y_0}_P \in (2^{e_{\max}}, 2 \cdot 2^{e_{\max}})$
        \Statex $\mathcal{Y}_k = \sum\limits_{j = 0}^{n - 1}d(x_j, I + k)$
      \end{algorithmic}
      \label{alg:sum}
    \end{alg}
    \end{samepage}
    In less specific terms, what this algorithm ensures is that the fields of $Y$ represent the unique indexed sum of $Y$. If a floating point result is desired, it may be obtained from $Y$ using Algorithm \ref{alg:conv2floatoverflow}
    \begin{samepage}
    \begin{thm}
      Assume that we have run Algorithm \ref{alg:sum} on $n$ floating point numbers $x_0, ..., x_{n - 1} \in \F$ with some $K$. If all requirements of the algorithm are satisfied, then the ``Ensure'' claim at the end of the algorithm holds.
      \label{thm:sum}
    \end{thm}
    \end{samepage}
    \begin{proof}

      We first show that after line \ref{alg:sum:renorm}, the index $I$ of $Y$ is the greatest integer such that $\max(|x_0|, ..., |x_{j - 1}|) < 2^{b_I}$.
      \Statex ${Y_k}_P \in [1.5  \epsilon^{-1} 2^{a_{I + k}}, 1.75  \epsilon^{-1} 2^{a_{I + k}})$ unless $I + k = 0$, in which case ${Y_0}_P \in (2^{e_{\max}}, 2 \cdot 2^{e_{\max}})$
      \Statex $\mathcal{Y}_k = \sum\limits_{j = 0}^{n - 1}d(x_j, I + k)$

      We show inductively that after each execution of line \ref{alg:sum:renorm}, $Y$ is the unique indexed sum of $x_0, ..., x_{j - 1}$.

      In the first iteration of the loop on line \ref{alg:sum:outerloop}, $Y$ is set to 0, meaning all of its fields are set to zero.
      In subsequent iterations of the loop, since we have at line \ref{alg:sum:update} that $Y$ is the indexed sum of $x_0, ..., x_{j - 1}$, we know that ${Y_k}_P \in [1.5  \epsilon^{-1} 2^{a_{I + k}}, 1.75  \epsilon^{-1} 2^{a_{I + k}})$ unless $I + k = 0$, in which case ${Y_0}_P \in (2^{e_{\max}}, 2 \cdot 2^{e_{\max}})$.

      In either case, the ``Require'' clause of Algorithm \ref{alg:update} is satisfied. After executing line \ref{alg:sum:update}, the following statements hold:
      \begin{enumerate}
      \item
        The index of $Y$ is $I$ where $I$ is the greatest integer such that $\max(|x_0|, ..., |x_{m - 1}|) < 2^{b_I}$
      \item
        $\mathcal{Y}_k = d(x_0, I + k) + ... + d(x_{j - 1}, I + k)$
      \item
      \end{enumerate}

 By the ``Ensure'' claim of Algorithm \ref{alg:update}, ${Y_k}_P \in (\epsilon^{-1}  2^{a_{I + k}}, 2  \epsilon^{-1}  2^{a_{I + k}})$ unless $I + k = 0$, in which case ${Y_0}_P \in (2^{e_{\max}}, 2 \cdot 2^{e_{\max}})$. Between lines \ref{alg:sum:innerloop} and \ref{alg:sum:innerloopend}, at most $0.25\epsilon^{-1}2^{-W}$ calls to \ref{alg:sum:deposit} are made, and by Theorem \ref{thm:depositfreq}, the requirements of Algorithm \ref{alg:deposit} are metat each call. Therefore, after line \ref{alg:sum:innerloopend}, we have that $\mathcal{Y}_k = \Statex $\mathcal{Y}_k = \sum\limits_{j = 0}^{n - 1}d(x_j, I + k)$
    \end{proof}

  \subsection{Euclidean Norm}
    \label{sec:compositeops_nrm}
