\section{Composite Operations}
  \label{sec:compositeops}
  The ultimate goal of the ReproBLAS library is to support reproducible linear algebra functions on many widely-used architectures.

  We must account for any combination of several sources of
  nonreproducibility. These sources include arbitrary data
  permutation or layout, differing numbers of processors, and arbitrary
  reduction tree shape. The methods we will describe can be used to
  account for any or all of these sources of nonreproducibility. However, if there is only one potential source of nonreproducibility then it may be more efficient to use a method that deals specifically with that source. For example, if there is an arbitrary reduction tree shape (due to the underlying MPI implementation) but not arbitrary data layout (because this is fixed by the application) then it is probably cheaper to just to use a method
that deals with arbitrary reduction trees shapes.

  Reproducibility is a concern for several parallel architectures. Apart from the typical sequential environment, linear algebra packages can be implemented on shared-memory systems (where multiple processors operate independently and share the same memory space), distributed memory systems (where each processor has its own private memory), or even cloud computing systems (remote internet-based computing where resources are provided to users on-demand).

  The Basic Linear Algebra Subprograms (BLAS) \cite{BLAS} are widely
  used as an efficient, portable, and reliable sequential linear algebra library.
  The BLAS are divided into three categories.
  \begin{enumerate}
    \item The Level 1 BLAS (BLAS1) is a set of vector operations on
      strided arrays. Several of these operations are already
      reproducible, such as the \texttt{xscal} operation, which scales a vector by a scalar value or the \texttt{xsaxpy} operation, which adds a scaled copy of one vector to another. These operations are reproducible provided they are implemented in the same way. For example \texttt{xsaxpy} might not be reproducible if one implementation uses a fused multiply-add instruction and the other does not. Other operations, such as
      the dot product or vector norm, are not already reproducible with
      respect to data permutation, and must be modified to have this quality. These operations
      are \texttt{xasum}, \texttt{xnrm2}, and 
      \texttt{xdot}.
    \item The Level 2 BLAS (BLAS2) is a set of matrix-vector operations
      including matrix-vector multiplication and a solver for $\vec{x}$ in $T\vec{x} = \vec{y}$ where $\vec{x}$ and $\vec{y}$ are vectors and $T$ is a triangular matrix. These operations can all be made reproducible, but for brevity we discuss only the representative operation \texttt{xgemv}, matrix-vector multiplication.
    \item The Level 3 BLAS (BLAS3) is a set of matrix-matrix operations
      including matrix-matrix multiplication and routines to perform $B = \alpha T^{-1}B$ where $\alpha$ is a scalar, $B$ is a matrix, and $T$ is a triangular matrix. These operations can all be made reproducible, but for brevity we discuss only the representative operation \texttt{xgemm}, matrix-matrix multiplication.
  \end{enumerate}

  The Linear Algebra PACKage (LAPACK) \cite{LAPACK}, is a set of higher-level
  routines for linear algebra, providing routines for operations like solving linear equations, linear least squares, eigenvalue problems, and singular value decomposition. LAPACK does most floating point computation using the BLAS.

  The BLAS and LAPACK have been extended to distributed-memory systems in the PBLAS \cite{PBLAS} and SCALAPACK \cite{SCALAPACK} libraries. While most BLAS and PBLAS operations can clearly be made reproducible, it is an open question as to which LAPACK and SCALAPACK routines can be made reproducible and how this would be done. This is future work.

  Out of the large design space outlined above, we describe in this paper and implement in ReproBLAS sequential versions of key operations from the BLAS, and a distributed-memory reduction operation which can be used to parallelize these sequential operations in a reproducible way. We described the BLAS1 operations \texttt{xasum} and \texttt{xdot} at the end of Section \ref{sec:primitiveops_sum} previously. The BLAS1 operation \texttt{xnrm2} is discussed in Section \ref{sec:compositeops_nrm}. The BLAS2 and BLAS3 operations \texttt{xgemv} and \texttt{xgemm} are discussed in Sections \ref{sec:compositeops_gemv} and \ref{sec:compositeops_gemm} respectively. Eventually, we hope to extend ReproBLAS to contain reproducible versions of all BLAS and PBLAS routines.

    \input{op_reduce}
    \input{op_nrm}
    \input{op_gemv}
    \input{op_gemm}
