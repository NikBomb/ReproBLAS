\subsection{Matrix-Matrix Product}
  \label{sec:compositeops_gemm}
  The $m \times n$ matrix-matrix product $C$ of an $m \times k$ matrix $A$ and a $k \times n$ matrix $B$ where $\alpha, \beta \in \R$ (referred to as $C = AB$ and computed in the BLAS by \texttt{xgemm}) is defined as follows (where $A_{i, j}$ is the entry in the $i^{th}$ row and $j^{th}$ column of $A$, indexing from zero for consistency):
  \[
    C_{i, j} = \beta C_{i, j} + \sum\limits_{l = 0}^{k - 1} \alpha A_{i, l}B_{l, j}
  \]
  The matrix-matrix product is a similar construction to the matrix-vector product, and discussion of reproducibility proceeds similarly.
  Computation of the matrix-matrix product can be distributed in a myriad of ways. SUMMA (Scalable Universal Matrix Multiply Algorithm) \cite{SUMMA} is used in the PBLAS, the most popular parallel BLAS implementation. In SUMMA and almost all other parallel matrix-matrix algorithm, the computation is broken up into rectangular blocks of A, B, and C. Although SUMMA has a static reduction tree (assuming a fixed number of processors), this cannot be assumed for all parallel matrix multiply algorithms, so it may be necessary to use a reproducible reduction step (discussed in Section \ref{sec:compositeops_reduce}).
  Even if the reduction tree is reproducible for a particular blocking pattern, the blocking pattern may not be reproducible (for instance, if more or less processors are used). Therefore, we must compute the matrix-matrix product using indexed summation.

  It is for this reason that we need a local version of \texttt{xgemm} that returns a matrix of indexed sums, the matrix-matrix product with all sums calculated using indexed summation.

  A consequence of calculating the indexed matrix-matrix product using indexed sums is that it will be reproducible with respect to permutation of the columns of $A$ together with rows of $B$.
  Let $\sigma_0, ..., \sigma_{k - 1}$ be some permutation of the first $k$ nonnegative integers such that $\{\sigma_0, ..., \sigma_{k - 1}\} = \{0, ..., n - 1\}$ as sets. Then we have
  \[
    C_{i, j} = \sum\limits_{l = 0}^{k - 1} A_{i, l}B_{l, j} = \sum\limits_{l = 0}^{k - 1} A_{i, \sigma_l}B_{\sigma_l, j}
  \]

  More importantly, the matrix-vector product should remain reproducible under any reduction tree shape.
  If $A = [A_{(0)}, A_{(1)}]$ where $A_{(0)}$ and $A_{(1)}$ are submatrices of size $m \times k_{(0)}$ and $m \times k_{(1)}$ and if $B = \left[\begin{array}{c}B_{(0)}\\ B_{(1)}\end{array}\right]$ where $B_{(0)}$ and $B_{(1)}$ are submatrices of size $k_{(0)} \times n$ and $k_{(1)} \times n$ then we have 
  \[
    AB = A_{(0)}B_{(0)} + A_{(1)}B_{(1)}
  \]

  It is clear from Theorems \ref{thm:indexed_sum_unique} and the ``Ensure'' claim of Algorithm \ref{alg:addindexedtoindexed} that if the matrix-matrix product is computed using indexed summation, the result is reproducible.

  Like the matrix-vector product, we can compute the matrix-matrix product using indexed summation with some function calls to \texttt{idxdBLAS\_xixdot}. In ReproBLAS, we mirror the function definition of \texttt{xgemm} in the BLAS as closely as possible, adding two additional parameters $\alpha$ and $\beta$ so that the entire operation performs the update $C \gets \alpha AB + \beta C$ (we also add the standard parameters for transposing the matrices and selecting row-major or column-major ordering of all matrices).

  At the core, ReproBLAS provides the function \texttt{idxdBLAS\_xixgemm} in \texttt{idxdBLAS.h} (see Section \ref{sec:reproBLAS} for details). This version of the function adds to the matrix of indexed sums $C$ the previously mentioned indexed matrix-matrix product of the floating point $A$ and $B$, where $x$ is first scaled by $\alpha$ as is done in the reference BLAS implementation. 
To be clear, \texttt{idxdBLAS\_xixgemm} assumes that $C$ is a matrix of indexed types and that all other inputs are floating point. A version (\texttt{reproBLAS\_rxgemm}) of the matrix matrix product routine that assumes $C$ to be a floating point matrix is discussed later. 
Again the parameter $\beta$ is excluded when $C$ is composed of indexed types (but not when $C$ is composed of floating point numbers which will be discussed below), as we do not yet know how to scale indexed types by different values (other than $0$, $1$, or $-1$) while maintaining reproducibility.

  Again because the reproducible dot product is so compute-heavy, we can get good performance implementing the matrix-matrix product using the \texttt{idxdBLAS\_diddot} routine at the core. The blocking strategy is more complicated this time, however, as computation can proceed under several loop orderings. Because the matrices $A$ and $B$ are composed of single floating point entries and $C$ is composed of the much larger indexed types (each indexed type usually contains at least $6 = 2 * K$ of its constituent floating-point types), we chose to completely compute blocks of $C$ by iterating over the matrices $A$ and $B$. This strategy avoids having to perform multiple iterations over the matrix $C$ composed of much larger data types. 
  Again, to keep the dot product running smoothly we first transpose blocks of $A$ and/or $B$ (depending on the transpose and row-major or column-major ordering options) when it is necessary to obtain contiguous sequences of cached data.

  Built on top of \texttt{idxdBLAS\_xixgemm} is the routine \texttt{reproBLAS\_rxgemm} in \texttt{reproBLAS.h} (see Section \ref{sec:reproBLAS} for details), which takes as input floating point $A$, $B$, $C$, $\alpha$, and $\beta$. $C$ is scaled by $\beta$, then converted to a vector of indexed types. The matrix-matrix product is computed using indexed summation with a user specified number of accumulators, and the output is then converted back to floating point and returned. The routine \texttt{reproBLAS\_xgemm} uses the default number of accumulators.
