\subsection{Matrix-Matrix Product}
  \label{sec:compositeops_gemm}
  The $m \times n$ matrix-matrix product $C$ of an $m \times k$ matrix $A$ and a $k \times n$ matrix $B$(referred to as $C = AB$ and computed in the BLAS by \texttt{xgemm} \cite{BLAS}) is defined as follows (where $A_{i, j}$ is the entry in the $i^{th}$ row and $j^{th}$ column of $A$, indexing from zero for consistency):
  \[
    C_{i, j} = \sum\limits_{l = 0}^{k - 1} A_{i, l}B_{l, j}
  \]
  The matrix-matrix product is a similar construction to the matrix-vector product, and discussion of reproducibility proceeds similarly.
  Computation of the matrix-matrix product can be distributed in a myriad of ways. SUMMA (Scalable Universal Matrix Multiply Algorithm) is used in the PBLAS, the most popular parallel BLAS implementation. In SUMMA and almost all other parallel matrix-matrix algorithm, the computation is broken up into rectangular blocks of A, B, and C. Although SUMMA has a static reduction tree, this cannot be assumed for all parallel matrix multiply algorithms, so it may be necessary to use a reproducible reduction step (discussed in Section \ref{sec:compositeops_reduce}).
  Even if the reduction tree is reproducible, the blocking pattern may not be (for instance, if more or less processors are used). Therefore, we must compute the matrix-matrix product using indexed summation.

  It is for this reason that we need a local version of \texttt{xgemm} that returns a matrix of indexed sums, the matrix-matrix product with all sums calculated using indexed summation.

  A consequence of calculating the indexed matrix-matrix product using indexed sums is that it will be reproducible with respect to permutation of the columns of $A$ together with rows of $B$.
  Let $\sigma_0, ..., \sigma_{k - 1}$ be some permutation of the first $k$ nonnegative integers such that $\{\sigma_0, ..., \sigma_{k - 1}\} = \{0, ..., n - 1\}$ as sets. Then we have
  \[
    C_{i, j} = \sum\limits_{l = 0}^{k - 1} A_{i, l}B_{l, j} = \sum\limits_{l = 0}^{k - 1} A_{i, \sigma_l}B_{\sigma_l, j}
  \]

  More importantly, the matrix-vector product should remain reproducible under any reduction tree shape.
  If $A = [A_{(0)}, A_{(1)}]$ where $A_{(0)}$ and $A_{(1)}$ are submatrices of size $m \times k_{(0)}$ and $m \times k_{(1)}$ and if $B = \left[\begin{array}{c}B_{(0)}\\ B_{(1)}\end{array}\right]$ where $B_{(0)}$ and $B_{(1)}$ are submatrices of size $k_{(0)} \times n$ and $k_{(1)} \times n$ then we have 
  \[
    AB = A_{(0)}B_{(0)} + A_{(1)}B_{(1)}
  \]

  It is clear from Theorems \ref{thm:indexed_sum_unique} and the ``Ensure'' claim of Algorithm \ref{alg:addindexedtoindexed} that if the matrix-matrix product is computed using indexed summation, the necessary properties hold.

  Like the matrix-vector product, we can compute the matrix-matrix product using indexed summation with some function calls to \texttt{idxdBLAS\_xixdot}. In ReproBLAS, we mirror the function definition of \text{xgemm} in the BLAS as closely as possible, adding two additional parameters $\alpha$ and $\beta$ so that the entire operation performs the update $C \gets \alpha AB + \beta C$ (we also add the standard parameters for transposing the matrices and selecting ordering of all matrices).

  At the core, ReproBLAS provides the function \texttt{idxdBLAS\_xixgemm} in \texttt{idxdBLAS.h}. This version of the function adds to the matrix of indexed sums $C$ the previously mentioned indexed matrix-matrix product of the floating point $A$ and $B$, where $x$ is first scaled by $\alpha$ as is done in the reference BLAS implementation \cite{BLAS}. Again the parameter $\beta$ is excluded, as we do not yet know how to scale indexed types by different values while maintaining reproducibility.

  Again because the reproducible dot product is so compute-heavy, we can get good performance implementing the matrix-matrix product using the \texttt{idxdBLAS\_diddot} routine at the core. The blocking strategy is more complicated this time, however, as computation can proceed under several loop orderings. Because the matrices $A$ and $B$ are composed of single floating point entries and $C$ is composed of the much larger indexed types (each indexed type usually contains at least 6 of its constituent floating-point types), we chose to completely compute blocks of $C$ by iterating over the matrices $A$ and $B$. This strategy avoids having to perform multiple iterations over the matrix $C$ composed of much larger data types. Again, to keep the dot product running smoothly we first transpose blocks of the matrix when it is necessary to obtain sequential chunks of memory.

  Built on top of \texttt{idxdBLAS\_xixgemm} is the routine \texttt{reproBLAS\_rxgemm} in \texttt{reproBLAS.h}, which takes as input floating point $A$, $B$, $C$, $\alpha$, and $\beta$. $C$ is scaled by $\beta$, then converted to a vector of indexed types. The matrix-matrix product is computed using indexed summation with a user specified number of accumulators, and the output is then converted back to floating point and returned. The routine \texttt{reproBLAS\_xgemm} uses the default number of accumulators.
