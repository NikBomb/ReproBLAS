\subsection{Sum}
  \label{sec:primitiveops_sum}
    Algorithm \ref{alg:sum} is an indexed summation algorithm that allows the user to efficiently add a vector of floating point numbers $x_m, ..., x_{m + n - 1} \in \F$ to the indexed sum $Y$ of some $x_0, ..., x_{m - 1} \in F$.

    As mentioned in Section~\ref{sec:primitiveops_renormalize}, it is not necessary to perform
    a renormalization for every deposit, as would be done if Algorithm \ref{alg:addfloattoindexed} were applied iteratively on each element of $x_m, ..., x_{m + n -1}$. At most $2^{p-W-2}$ values can be deposited in the indexed type before having to perform the renormalization.
    Therefore we have an improved version of Algorithm~\ref{alg:addfloattoindexed} when we need to sum a vector of floating point numbers.
    Algorithm~\ref{alg:sum} summarizes the optimized version for the reproducible local sum. It is available as \texttt{idxdBLAS\_xixsum} in \texttt{idxdBLAS.h} (see Section \ref{sec:reproBLAS} for details).
    As Algorithm~\ref{alg:sum} computes an indexed sum, it can be performed on the $x_m, ..., x_{m + n - 1}$ in any order. However, for the simplicity of presenting the algorithm, it is depicted as running linearly from $m$ to $m + n -1$.
    Algorithm \ref{alg:sum} uses only one indexed type to hold the intermediate result of the recursive summation, and the vast majority of time in the algorithm is spent in \textproc{Deposit} (Algorithm \ref{alg:deposit}).

    \begin{samepage}
    \begin{alg}
      If $Y$ is the $K$-fold indexed sum of some $x_0, ..., x_{m - 1} \in \F$, produce the $K$-fold indexed sum of $x_0, ..., x_{m + n - 1} \in \F$. (If $m = 0$, this implies that all fields of $Y$ are 0 and $Y$ will be initialized in line \ref{alg:sum:update})
      This is similar to Algorithm $6$ in \cite{repsum}, but requires no restrictions on the size or type (exceptional or finite) of inputs $x_0, ..., x_{m + n - 1}$.
      \begin{algorithmic}[1]
        \Function{Sum}{$K$, [$x_m, ..., x_{m + n-1}$], $Y$}
          \State $j = 0$ \label{alg:sum:setj}
          \While{$j < n$}\label{alg:sum:outerloop}
            \State $nb = \min(n, j + 2^{p - W - 2})$ \label{alg:sum:nb}
            \State \Call{Update}{$K$, $\max(|x_{m + j}|, ..., |x_{m + nb - 1}|)$, Y}\label{alg:sum:update}
            \While{$j < nb$}
              \State \Call{Deposit}{$K$, $x_{m + j}$, $Y$}\label{alg:sum:deposit}
              \State $j = j + 1$
            \EndWhile
            \State \Call{Renorm}{$K$, $Y$}\label{alg:sum:renorm}
          \EndWhile
          \State \Return $Y$
        \EndFunction
        \Ensure
        \Statex $Y$ is the unique indexed sum of $x_0, ..., x_{m + n - 1}$.
      \end{algorithmic}
      \label{alg:sum}
    \end{alg}
    \end{samepage}

    If a single floating point result is desired, it may be obtained from $Y$ using Algorithm \ref{alg:conv2floatoverflow} described in Section \ref{sec:primitiveops_conv2float}.
    \begin{samepage}
    \begin{thm}
      Assume that we have run Algorithm \ref{alg:sum} on the $K$-fold indexed sum $Y$ of $x_0, ..., x_{m - 1} \in \F$ and on $x_m, ..., x_{m + n - 1} \in \F$. If all requirements of the algorithm are satisfied, then the ``Ensure'' claim at the end of the algorithm holds.
      \label{thm:sum}
    \end{thm}
    \end{samepage}
    \begin{proof}
      We show inductively that after each execution of line \ref{alg:sum:renorm}, $Y$ is the indexed sum of $x_0, ..., x_{m + j - 1}$. Throughout the proof, assume that the value of all variables are specific to the given stage of execution.

      As a base case, on the first iteration of the loop on line \ref{alg:sum:outerloop}, $j$ is 0 and $Y$ is given to be the indexed sum of $x_0, ..., x_{m - 1}$.

      In subsequent iterations of the loop, we assume that at line \ref{alg:sum:update}, $Y$ is the indexed sum of $x_0, ..., x_{m + j - 1}$.

      In this case, the proof of Theorem \ref{thm:addfloattoindexed} applies to lines \ref{alg:sum:update} to \ref{alg:sum:renorm} (keeping in mind that at most $2^{p - W - 2}$ deposits are performed and by the ``Ensure'' claim of Algorithm \ref{alg:update}, each finite $x_{m + j}$ deposited satisfies $|x_{m + j}| < 2^{b_I}$). Therefore, after line \ref{alg:sum:renorm}, $Y$ is the indexed sum of $x_0, ..., x_{m + j - 1}$

      \begin{comment}
        Therefore ${Y_k}_P \in [1.5  \epsilon^{-1} 2^{a_{I + k}}, 1.75  \epsilon^{-1} 2^{a_{I + k}})$ unless $I + k = 0$, in which case ${Y_0}_P \in (2^{e_{\max}}, 2 \cdot 2^{e_{\max}})$.

      In either case, the ``Require'' clause of Algorithm \ref{alg:update} is satisfied. Therefore, after executing line \ref{alg:sum:update}, the following statements hold:
      \begin{enumerate}
      \item
        The index of $Y$ is $I$ where $I$ is the greatest integer such that $\max(|x_0|, ..., |x_{m - 1}|) < 2^{b_I}$
      \item
        $\mathcal{Y}_k = d(x_0, I + k) + ... + d(x_{j - 1}, I + k)$
      \item
        \Statex ${Y_k}_P \in [1.5  \epsilon^{-1} 2^{a_{I + k}}, 1.75  \epsilon^{-1} 2^{a_{I + k}})$ unless $I + k = 0$, in which case ${Y_0}_P \in (2^{e_{\max}}, 2 \cdot 2^{e_{\max}})$
      \end{enumerate}
      Between lines \ref{alg:sum:innerloop} and \ref{alg:sum:innerloopend}, at most $0.25\epsilon^{-1}2^{-W}$ calls to \ref{alg:sum:deposit} are made, and by Theorem \ref{thm:depositfreq}, and the above initial properties, the requirements of Algorithm \ref{alg:deposit} are met at each call. Therefore, after line \ref{alg:sum:innerloopend}, we have that $\mathcal{Y}_k = d(x_0, I + k) + ... + d(x_{j-1}, I + k)$.

      Again by Theorem \ref{thm:depositfreq}, the requirements of Algorithm \ref{alg:renorm} are met in line \ref{alg:sum:renorm}. Therefore, after execution of line \ref{alg:sum:renorm}, we have that ${Y_k}_P \in [1.5  \epsilon^{-1} 2^{a_{I + k}}, 1.75  \epsilon^{-1} 2^{a_{I + k}})$ unless $I + k = 0$, in which case ${Y_0}_P \in (1.5 \cdot 2^{e_{\max}}, 1.75 \cdot 2^{e_{\max}})$. Therefore, $Y$ is the indexed sum of $x_0, ..., x_{j - 1}$ after line \ref{alg:sum:renorm}, completing the induction.
      \end{comment}
    \end{proof}
    Note that after computing $\max()$ in line \ref{alg:sum:update}, we know whether or not the indexed type will have index 0, and can call either \textproc{Deposit} or \textproc{DepositRestricted} accordingly (the latter being a faster routine). We also know from the max whether or not an \texttt{Inf}, \texttt{-Inf}, or \texttt{NaN} is present, and can skip the \textproc{Deposit} and \textproc{Renorm} procedures and compute the exceptional result directly. Also note that the constant $2^{p - W - 2}$ in line \ref{alg:sum:nb} is at its maximum value, and smaller values may be used to fit data into a cache. In ReproBLAS, this constant is autotuned (as discussed in Section \ref{sec:reproBLAS}).

    As the indexed sum is unique and independent of the ordering of its summands (Theorem \ref{thm:indexed_sum_unique}), Algorithm \ref{alg:sum} is reproducible for any permutation of its inputs.

    At this point, an operation count should be considered. Since Algorithm~\ref{alg:sum} only performs the update and renormalization
  once for every $2^{p-W-2}$ times the deposit operation is performed (that
is $2^{53-40-2}=2^{11}$ times for double precision and $2^{24-13-2} = 2^9$ times for single precision in the current implementation of
  ReproBLAS), the cost of Algorithm~\ref{alg:sum} is mostly due to the
  deposit operation.
  Therefore, in the absence of overflow, Algorithm~\ref{alg:sum} costs
  $\approx (3K-1)n$ FLOPs counting the maximum absolute value operation as 1 FLOP,
  which is $\approx 8n$ FLOPs for the default value of $K$ ($K=3$) used by ReproBLAS.
  In the rare case of index 0 for the first bin,
  the cost is slightly higher since the first bin
  needs to be scaled down to avoid overflow, which increases the
  total cost of Algorithm~\ref{alg:sum} to $\approx (3K+2)n$ FLOPs.

  We make the observation that it is possible to implement a reproducible absolute sum by applying Algorithm~\ref{alg:sum} to the the absolute values of entries of an input vector.

  It is also possible to compute a reproducible dot product of vectors $\vec{x}$ and $\vec{y}$. To modify Algorithm \ref{alg:sum} for this purpose, we need only to change line  \ref{alg:sum:deposit} to deposit the product of the $j^{th}$ entry of $\vec{x}$ and the $j^{th}$ entry of $\vec{y}$ and line \ref{alg:sum:update} to calculate the maximum absolute value of these products. More examples of how this routine can be used for reproducible operations are given in Section \ref{sec:compositeops}.

  A few implementation details should be covered regarding our implementation
  of the complex dot product. When multiplying the complex numbers $(a + bi)$ and
  $(c + di)$, we assume that the product is obtained by evaluating $(ac - bd) +
  (ad + bc)i$ straightforwardly. This means that $(\texttt{Inf} +
  \texttt{Inf}i)(\texttt{Inf} + 0i)$ produces $\texttt{NaN} + \texttt{NaN}i$.
  This is the Fortran convention for complex multiplication, not the C
  convention. In C, complex multiplication is implemented with a function call to
  catch the exceptional cases and give more sensible results. In the example
  above, we get $\texttt{Inf} + \texttt{Inf}i$. We refer the curious reader to
  part G.5.1 of the C99 standard \cite{c99} where this issue is discussed. We
  choose the Fortran definition of multiplication because the Reference BLAS is
  written in Fortran.
