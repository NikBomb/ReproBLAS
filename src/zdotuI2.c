#include <complex.h>
#include <stdio.h>
#include <stdlib.h>
#include <float.h>
#include <math.h>
#include "config.h"
#include "Common/Common.h"
#include <immintrin.h>
#include <emmintrin.h>

/*[[[cog
import cog
import sys, os
from gen import generate
from gen import dataTypes
from gen import vectorizations
import dotuI2
]]]*/
//[[[end]]]

#if defined( __AVX__ )
  void zdotuI2(int n, double complex* v, int incv, double complex* y, int incy, int fold, double complex* sum){
    /*[[[cog
    cog.out(generate.generate(dotuI2.DotUI2(dataTypes.DoubleComplex, vectorizations.AVX), args, params))
    ]]]*/
    __m256d mask_NCONJ; AVX_NCONJ_MASKD(mask_NCONJ);
    __m256d mask_BLP; AVX_BLP_MASKD(mask_BLP);
    double complex tmp_cons[2] __attribute__((aligned(32)));
    SET_DAZ_FLAG;
    switch(fold){
      case 3:{
        int i;

        double* sum_base = (double*) sum;
        double* v_base = (double*) v;
        double* y_base = (double*) y;
        __m256d v_0, v_1, v_2, v_3, v_4, v_5, v_6, v_7;
        __m256d y_0, y_1, y_2, y_3;
        __m256d q_0, q_1, q_2, q_3;
        __m256d s_0_0, s_0_1, s_0_2, s_0_3;
        __m256d s_1_0, s_1_1, s_1_2, s_1_3;
        __m256d s_2_0, s_2_1, s_2_2, s_2_3;

        s_0_0 = s_0_1 = s_0_2 = s_0_3 = _mm256_broadcast_pd((__m128d *)(sum_base));
        s_1_0 = s_1_1 = s_1_2 = s_1_3 = _mm256_broadcast_pd((__m128d *)(sum_base + 2));
        s_2_0 = s_2_1 = s_2_2 = s_2_3 = _mm256_broadcast_pd((__m128d *)(sum_base + 4));
        if(incv == 1){
          if(incy == 1){

            for(i = 0; i + 8 <= n; i += 8, v_base += 16, y_base += 16){
              v_0 = _mm256_loadu_pd(v_base);
              v_1 = _mm256_loadu_pd(v_base + 4);
              v_2 = _mm256_loadu_pd(v_base + 8);
              v_3 = _mm256_loadu_pd(v_base + 12);
              y_0 = _mm256_loadu_pd(y_base);
              y_1 = _mm256_loadu_pd(y_base + 4);
              y_2 = _mm256_loadu_pd(y_base + 8);
              y_3 = _mm256_loadu_pd(y_base + 12);
              v_4 = _mm256_xor_pd(_mm256_mul_pd(_mm256_permute_pd(v_0, 0b0101), _mm256_permute_pd(y_0, 0b1111)), mask_NCONJ);
              v_5 = _mm256_xor_pd(_mm256_mul_pd(_mm256_permute_pd(v_1, 0b0101), _mm256_permute_pd(y_1, 0b1111)), mask_NCONJ);
              v_6 = _mm256_xor_pd(_mm256_mul_pd(_mm256_permute_pd(v_2, 0b0101), _mm256_permute_pd(y_2, 0b1111)), mask_NCONJ);
              v_7 = _mm256_xor_pd(_mm256_mul_pd(_mm256_permute_pd(v_3, 0b0101), _mm256_permute_pd(y_3, 0b1111)), mask_NCONJ);
              v_0 = _mm256_mul_pd(v_0, _mm256_permute_pd(y_0, 0b0000));
              v_1 = _mm256_mul_pd(v_1, _mm256_permute_pd(y_1, 0b0000));
              v_2 = _mm256_mul_pd(v_2, _mm256_permute_pd(y_2, 0b0000));
              v_3 = _mm256_mul_pd(v_3, _mm256_permute_pd(y_3, 0b0000));
              q_0 = s_0_0;
              q_1 = s_0_1;
              q_2 = s_0_2;
              q_3 = s_0_3;
              s_0_0 = _mm256_add_pd(s_0_0, _mm256_or_pd(v_0, mask_BLP));
              s_0_1 = _mm256_add_pd(s_0_1, _mm256_or_pd(v_1, mask_BLP));
              s_0_2 = _mm256_add_pd(s_0_2, _mm256_or_pd(v_2, mask_BLP));
              s_0_3 = _mm256_add_pd(s_0_3, _mm256_or_pd(v_3, mask_BLP));
              q_0 = _mm256_sub_pd(q_0, s_0_0);
              q_1 = _mm256_sub_pd(q_1, s_0_1);
              q_2 = _mm256_sub_pd(q_2, s_0_2);
              q_3 = _mm256_sub_pd(q_3, s_0_3);
              v_0 = _mm256_add_pd(v_0, q_0);
              v_1 = _mm256_add_pd(v_1, q_1);
              v_2 = _mm256_add_pd(v_2, q_2);
              v_3 = _mm256_add_pd(v_3, q_3);
              q_0 = s_1_0;
              q_1 = s_1_1;
              q_2 = s_1_2;
              q_3 = s_1_3;
              s_1_0 = _mm256_add_pd(s_1_0, _mm256_or_pd(v_0, mask_BLP));
              s_1_1 = _mm256_add_pd(s_1_1, _mm256_or_pd(v_1, mask_BLP));
              s_1_2 = _mm256_add_pd(s_1_2, _mm256_or_pd(v_2, mask_BLP));
              s_1_3 = _mm256_add_pd(s_1_3, _mm256_or_pd(v_3, mask_BLP));
              q_0 = _mm256_sub_pd(q_0, s_1_0);
              q_1 = _mm256_sub_pd(q_1, s_1_1);
              q_2 = _mm256_sub_pd(q_2, s_1_2);
              q_3 = _mm256_sub_pd(q_3, s_1_3);
              v_0 = _mm256_add_pd(v_0, q_0);
              v_1 = _mm256_add_pd(v_1, q_1);
              v_2 = _mm256_add_pd(v_2, q_2);
              v_3 = _mm256_add_pd(v_3, q_3);
              s_2_0 = _mm256_add_pd(s_2_0, _mm256_or_pd(v_0, mask_BLP));
              s_2_1 = _mm256_add_pd(s_2_1, _mm256_or_pd(v_1, mask_BLP));
              s_2_2 = _mm256_add_pd(s_2_2, _mm256_or_pd(v_2, mask_BLP));
              s_2_3 = _mm256_add_pd(s_2_3, _mm256_or_pd(v_3, mask_BLP));
              q_0 = s_0_0;
              q_1 = s_0_1;
              q_2 = s_0_2;
              q_3 = s_0_3;
              s_0_0 = _mm256_add_pd(s_0_0, _mm256_or_pd(v_4, mask_BLP));
              s_0_1 = _mm256_add_pd(s_0_1, _mm256_or_pd(v_5, mask_BLP));
              s_0_2 = _mm256_add_pd(s_0_2, _mm256_or_pd(v_6, mask_BLP));
              s_0_3 = _mm256_add_pd(s_0_3, _mm256_or_pd(v_7, mask_BLP));
              q_0 = _mm256_sub_pd(q_0, s_0_0);
              q_1 = _mm256_sub_pd(q_1, s_0_1);
              q_2 = _mm256_sub_pd(q_2, s_0_2);
              q_3 = _mm256_sub_pd(q_3, s_0_3);
              v_4 = _mm256_add_pd(v_4, q_0);
              v_5 = _mm256_add_pd(v_5, q_1);
              v_6 = _mm256_add_pd(v_6, q_2);
              v_7 = _mm256_add_pd(v_7, q_3);
              q_0 = s_1_0;
              q_1 = s_1_1;
              q_2 = s_1_2;
              q_3 = s_1_3;
              s_1_0 = _mm256_add_pd(s_1_0, _mm256_or_pd(v_4, mask_BLP));
              s_1_1 = _mm256_add_pd(s_1_1, _mm256_or_pd(v_5, mask_BLP));
              s_1_2 = _mm256_add_pd(s_1_2, _mm256_or_pd(v_6, mask_BLP));
              s_1_3 = _mm256_add_pd(s_1_3, _mm256_or_pd(v_7, mask_BLP));
              q_0 = _mm256_sub_pd(q_0, s_1_0);
              q_1 = _mm256_sub_pd(q_1, s_1_1);
              q_2 = _mm256_sub_pd(q_2, s_1_2);
              q_3 = _mm256_sub_pd(q_3, s_1_3);
              v_4 = _mm256_add_pd(v_4, q_0);
              v_5 = _mm256_add_pd(v_5, q_1);
              v_6 = _mm256_add_pd(v_6, q_2);
              v_7 = _mm256_add_pd(v_7, q_3);
              s_2_0 = _mm256_add_pd(s_2_0, _mm256_or_pd(v_4, mask_BLP));
              s_2_1 = _mm256_add_pd(s_2_1, _mm256_or_pd(v_5, mask_BLP));
              s_2_2 = _mm256_add_pd(s_2_2, _mm256_or_pd(v_6, mask_BLP));
              s_2_3 = _mm256_add_pd(s_2_3, _mm256_or_pd(v_7, mask_BLP));
            }
            if(i + 4 <= n){
              v_0 = _mm256_loadu_pd(v_base);
              v_1 = _mm256_loadu_pd(v_base + 4);
              y_0 = _mm256_loadu_pd(y_base);
              y_1 = _mm256_loadu_pd(y_base + 4);
              v_2 = _mm256_xor_pd(_mm256_mul_pd(_mm256_permute_pd(v_0, 0b0101), _mm256_permute_pd(y_0, 0b1111)), mask_NCONJ);
              v_3 = _mm256_xor_pd(_mm256_mul_pd(_mm256_permute_pd(v_1, 0b0101), _mm256_permute_pd(y_1, 0b1111)), mask_NCONJ);
              v_0 = _mm256_mul_pd(v_0, _mm256_permute_pd(y_0, 0b0000));
              v_1 = _mm256_mul_pd(v_1, _mm256_permute_pd(y_1, 0b0000));
              q_0 = s_0_0;
              q_1 = s_0_1;
              q_2 = s_0_2;
              q_3 = s_0_3;
              s_0_0 = _mm256_add_pd(s_0_0, _mm256_or_pd(v_0, mask_BLP));
              s_0_1 = _mm256_add_pd(s_0_1, _mm256_or_pd(v_1, mask_BLP));
              s_0_2 = _mm256_add_pd(s_0_2, _mm256_or_pd(v_2, mask_BLP));
              s_0_3 = _mm256_add_pd(s_0_3, _mm256_or_pd(v_3, mask_BLP));
              q_0 = _mm256_sub_pd(q_0, s_0_0);
              q_1 = _mm256_sub_pd(q_1, s_0_1);
              q_2 = _mm256_sub_pd(q_2, s_0_2);
              q_3 = _mm256_sub_pd(q_3, s_0_3);
              v_0 = _mm256_add_pd(v_0, q_0);
              v_1 = _mm256_add_pd(v_1, q_1);
              v_2 = _mm256_add_pd(v_2, q_2);
              v_3 = _mm256_add_pd(v_3, q_3);
              q_0 = s_1_0;
              q_1 = s_1_1;
              q_2 = s_1_2;
              q_3 = s_1_3;
              s_1_0 = _mm256_add_pd(s_1_0, _mm256_or_pd(v_0, mask_BLP));
              s_1_1 = _mm256_add_pd(s_1_1, _mm256_or_pd(v_1, mask_BLP));
              s_1_2 = _mm256_add_pd(s_1_2, _mm256_or_pd(v_2, mask_BLP));
              s_1_3 = _mm256_add_pd(s_1_3, _mm256_or_pd(v_3, mask_BLP));
              q_0 = _mm256_sub_pd(q_0, s_1_0);
              q_1 = _mm256_sub_pd(q_1, s_1_1);
              q_2 = _mm256_sub_pd(q_2, s_1_2);
              q_3 = _mm256_sub_pd(q_3, s_1_3);
              v_0 = _mm256_add_pd(v_0, q_0);
              v_1 = _mm256_add_pd(v_1, q_1);
              v_2 = _mm256_add_pd(v_2, q_2);
              v_3 = _mm256_add_pd(v_3, q_3);
              s_2_0 = _mm256_add_pd(s_2_0, _mm256_or_pd(v_0, mask_BLP));
              s_2_1 = _mm256_add_pd(s_2_1, _mm256_or_pd(v_1, mask_BLP));
              s_2_2 = _mm256_add_pd(s_2_2, _mm256_or_pd(v_2, mask_BLP));
              s_2_3 = _mm256_add_pd(s_2_3, _mm256_or_pd(v_3, mask_BLP));
              i += 4, v_base += 8, y_base += 8;
            }
            if(i + 2 <= n){
              v_0 = _mm256_loadu_pd(v_base);
              y_0 = _mm256_loadu_pd(y_base);
              v_1 = _mm256_xor_pd(_mm256_mul_pd(_mm256_permute_pd(v_0, 0b0101), _mm256_permute_pd(y_0, 0b1111)), mask_NCONJ);
              v_0 = _mm256_mul_pd(v_0, _mm256_permute_pd(y_0, 0b0000));
              q_0 = s_0_0;
              q_1 = s_0_1;
              s_0_0 = _mm256_add_pd(s_0_0, _mm256_or_pd(v_0, mask_BLP));
              s_0_1 = _mm256_add_pd(s_0_1, _mm256_or_pd(v_1, mask_BLP));
              q_0 = _mm256_sub_pd(q_0, s_0_0);
              q_1 = _mm256_sub_pd(q_1, s_0_1);
              v_0 = _mm256_add_pd(v_0, q_0);
              v_1 = _mm256_add_pd(v_1, q_1);
              q_0 = s_1_0;
              q_1 = s_1_1;
              s_1_0 = _mm256_add_pd(s_1_0, _mm256_or_pd(v_0, mask_BLP));
              s_1_1 = _mm256_add_pd(s_1_1, _mm256_or_pd(v_1, mask_BLP));
              q_0 = _mm256_sub_pd(q_0, s_1_0);
              q_1 = _mm256_sub_pd(q_1, s_1_1);
              v_0 = _mm256_add_pd(v_0, q_0);
              v_1 = _mm256_add_pd(v_1, q_1);
              s_2_0 = _mm256_add_pd(s_2_0, _mm256_or_pd(v_0, mask_BLP));
              s_2_1 = _mm256_add_pd(s_2_1, _mm256_or_pd(v_1, mask_BLP));
              i += 2, v_base += 4, y_base += 4;
            }
            if(i < n){
              v_0 = _mm256_set_pd(0, 0, v_base[1], v_base[0]);
              y_0 = _mm256_set_pd(0, 0, y_base[1], y_base[0]);
              v_1 = _mm256_xor_pd(_mm256_mul_pd(_mm256_permute_pd(v_0, 0b0101), _mm256_permute_pd(y_0, 0b1111)), mask_NCONJ);
              v_0 = _mm256_mul_pd(v_0, _mm256_permute_pd(y_0, 0b0000));
              q_0 = s_0_0;
              q_1 = s_0_1;
              s_0_0 = _mm256_add_pd(s_0_0, _mm256_or_pd(v_0, mask_BLP));
              s_0_1 = _mm256_add_pd(s_0_1, _mm256_or_pd(v_1, mask_BLP));
              q_0 = _mm256_sub_pd(q_0, s_0_0);
              q_1 = _mm256_sub_pd(q_1, s_0_1);
              v_0 = _mm256_add_pd(v_0, q_0);
              v_1 = _mm256_add_pd(v_1, q_1);
              q_0 = s_1_0;
              q_1 = s_1_1;
              s_1_0 = _mm256_add_pd(s_1_0, _mm256_or_pd(v_0, mask_BLP));
              s_1_1 = _mm256_add_pd(s_1_1, _mm256_or_pd(v_1, mask_BLP));
              q_0 = _mm256_sub_pd(q_0, s_1_0);
              q_1 = _mm256_sub_pd(q_1, s_1_1);
              v_0 = _mm256_add_pd(v_0, q_0);
              v_1 = _mm256_add_pd(v_1, q_1);
              s_2_0 = _mm256_add_pd(s_2_0, _mm256_or_pd(v_0, mask_BLP));
              s_2_1 = _mm256_add_pd(s_2_1, _mm256_or_pd(v_1, mask_BLP));
            }
          }else{

            for(i = 0; i + 8 <= n; i += 8, v_base += 16, y_base += (incy * 16)){
              v_0 = _mm256_loadu_pd(v_base);
              v_1 = _mm256_loadu_pd(v_base + 4);
              v_2 = _mm256_loadu_pd(v_base + 8);
              v_3 = _mm256_loadu_pd(v_base + 12);
              y_0 = _mm256_set_pd(y_base[((incy * 2) + 1)], y_base[(incy * 2)], y_base[1], y_base[0]);
              y_1 = _mm256_set_pd(y_base[((incy * 6) + 1)], y_base[(incy * 6)], y_base[((incy * 4) + 1)], y_base[(incy * 4)]);
              y_2 = _mm256_set_pd(y_base[((incy * 10) + 1)], y_base[(incy * 10)], y_base[((incy * 8) + 1)], y_base[(incy * 8)]);
              y_3 = _mm256_set_pd(y_base[((incy * 14) + 1)], y_base[(incy * 14)], y_base[((incy * 12) + 1)], y_base[(incy * 12)]);
              v_4 = _mm256_xor_pd(_mm256_mul_pd(_mm256_permute_pd(v_0, 0b0101), _mm256_permute_pd(y_0, 0b1111)), mask_NCONJ);
              v_5 = _mm256_xor_pd(_mm256_mul_pd(_mm256_permute_pd(v_1, 0b0101), _mm256_permute_pd(y_1, 0b1111)), mask_NCONJ);
              v_6 = _mm256_xor_pd(_mm256_mul_pd(_mm256_permute_pd(v_2, 0b0101), _mm256_permute_pd(y_2, 0b1111)), mask_NCONJ);
              v_7 = _mm256_xor_pd(_mm256_mul_pd(_mm256_permute_pd(v_3, 0b0101), _mm256_permute_pd(y_3, 0b1111)), mask_NCONJ);
              v_0 = _mm256_mul_pd(v_0, _mm256_permute_pd(y_0, 0b0000));
              v_1 = _mm256_mul_pd(v_1, _mm256_permute_pd(y_1, 0b0000));
              v_2 = _mm256_mul_pd(v_2, _mm256_permute_pd(y_2, 0b0000));
              v_3 = _mm256_mul_pd(v_3, _mm256_permute_pd(y_3, 0b0000));
              q_0 = s_0_0;
              q_1 = s_0_1;
              q_2 = s_0_2;
              q_3 = s_0_3;
              s_0_0 = _mm256_add_pd(s_0_0, _mm256_or_pd(v_0, mask_BLP));
              s_0_1 = _mm256_add_pd(s_0_1, _mm256_or_pd(v_1, mask_BLP));
              s_0_2 = _mm256_add_pd(s_0_2, _mm256_or_pd(v_2, mask_BLP));
              s_0_3 = _mm256_add_pd(s_0_3, _mm256_or_pd(v_3, mask_BLP));
              q_0 = _mm256_sub_pd(q_0, s_0_0);
              q_1 = _mm256_sub_pd(q_1, s_0_1);
              q_2 = _mm256_sub_pd(q_2, s_0_2);
              q_3 = _mm256_sub_pd(q_3, s_0_3);
              v_0 = _mm256_add_pd(v_0, q_0);
              v_1 = _mm256_add_pd(v_1, q_1);
              v_2 = _mm256_add_pd(v_2, q_2);
              v_3 = _mm256_add_pd(v_3, q_3);
              q_0 = s_1_0;
              q_1 = s_1_1;
              q_2 = s_1_2;
              q_3 = s_1_3;
              s_1_0 = _mm256_add_pd(s_1_0, _mm256_or_pd(v_0, mask_BLP));
              s_1_1 = _mm256_add_pd(s_1_1, _mm256_or_pd(v_1, mask_BLP));
              s_1_2 = _mm256_add_pd(s_1_2, _mm256_or_pd(v_2, mask_BLP));
              s_1_3 = _mm256_add_pd(s_1_3, _mm256_or_pd(v_3, mask_BLP));
              q_0 = _mm256_sub_pd(q_0, s_1_0);
              q_1 = _mm256_sub_pd(q_1, s_1_1);
              q_2 = _mm256_sub_pd(q_2, s_1_2);
              q_3 = _mm256_sub_pd(q_3, s_1_3);
              v_0 = _mm256_add_pd(v_0, q_0);
              v_1 = _mm256_add_pd(v_1, q_1);
              v_2 = _mm256_add_pd(v_2, q_2);
              v_3 = _mm256_add_pd(v_3, q_3);
              s_2_0 = _mm256_add_pd(s_2_0, _mm256_or_pd(v_0, mask_BLP));
              s_2_1 = _mm256_add_pd(s_2_1, _mm256_or_pd(v_1, mask_BLP));
              s_2_2 = _mm256_add_pd(s_2_2, _mm256_or_pd(v_2, mask_BLP));
              s_2_3 = _mm256_add_pd(s_2_3, _mm256_or_pd(v_3, mask_BLP));
              q_0 = s_0_0;
              q_1 = s_0_1;
              q_2 = s_0_2;
              q_3 = s_0_3;
              s_0_0 = _mm256_add_pd(s_0_0, _mm256_or_pd(v_4, mask_BLP));
              s_0_1 = _mm256_add_pd(s_0_1, _mm256_or_pd(v_5, mask_BLP));
              s_0_2 = _mm256_add_pd(s_0_2, _mm256_or_pd(v_6, mask_BLP));
              s_0_3 = _mm256_add_pd(s_0_3, _mm256_or_pd(v_7, mask_BLP));
              q_0 = _mm256_sub_pd(q_0, s_0_0);
              q_1 = _mm256_sub_pd(q_1, s_0_1);
              q_2 = _mm256_sub_pd(q_2, s_0_2);
              q_3 = _mm256_sub_pd(q_3, s_0_3);
              v_4 = _mm256_add_pd(v_4, q_0);
              v_5 = _mm256_add_pd(v_5, q_1);
              v_6 = _mm256_add_pd(v_6, q_2);
              v_7 = _mm256_add_pd(v_7, q_3);
              q_0 = s_1_0;
              q_1 = s_1_1;
              q_2 = s_1_2;
              q_3 = s_1_3;
              s_1_0 = _mm256_add_pd(s_1_0, _mm256_or_pd(v_4, mask_BLP));
              s_1_1 = _mm256_add_pd(s_1_1, _mm256_or_pd(v_5, mask_BLP));
              s_1_2 = _mm256_add_pd(s_1_2, _mm256_or_pd(v_6, mask_BLP));
              s_1_3 = _mm256_add_pd(s_1_3, _mm256_or_pd(v_7, mask_BLP));
              q_0 = _mm256_sub_pd(q_0, s_1_0);
              q_1 = _mm256_sub_pd(q_1, s_1_1);
              q_2 = _mm256_sub_pd(q_2, s_1_2);
              q_3 = _mm256_sub_pd(q_3, s_1_3);
              v_4 = _mm256_add_pd(v_4, q_0);
              v_5 = _mm256_add_pd(v_5, q_1);
              v_6 = _mm256_add_pd(v_6, q_2);
              v_7 = _mm256_add_pd(v_7, q_3);
              s_2_0 = _mm256_add_pd(s_2_0, _mm256_or_pd(v_4, mask_BLP));
              s_2_1 = _mm256_add_pd(s_2_1, _mm256_or_pd(v_5, mask_BLP));
              s_2_2 = _mm256_add_pd(s_2_2, _mm256_or_pd(v_6, mask_BLP));
              s_2_3 = _mm256_add_pd(s_2_3, _mm256_or_pd(v_7, mask_BLP));
            }
            if(i + 4 <= n){
              v_0 = _mm256_loadu_pd(v_base);
              v_1 = _mm256_loadu_pd(v_base + 4);
              y_0 = _mm256_set_pd(y_base[((incy * 2) + 1)], y_base[(incy * 2)], y_base[1], y_base[0]);
              y_1 = _mm256_set_pd(y_base[((incy * 6) + 1)], y_base[(incy * 6)], y_base[((incy * 4) + 1)], y_base[(incy * 4)]);
              v_2 = _mm256_xor_pd(_mm256_mul_pd(_mm256_permute_pd(v_0, 0b0101), _mm256_permute_pd(y_0, 0b1111)), mask_NCONJ);
              v_3 = _mm256_xor_pd(_mm256_mul_pd(_mm256_permute_pd(v_1, 0b0101), _mm256_permute_pd(y_1, 0b1111)), mask_NCONJ);
              v_0 = _mm256_mul_pd(v_0, _mm256_permute_pd(y_0, 0b0000));
              v_1 = _mm256_mul_pd(v_1, _mm256_permute_pd(y_1, 0b0000));
              q_0 = s_0_0;
              q_1 = s_0_1;
              q_2 = s_0_2;
              q_3 = s_0_3;
              s_0_0 = _mm256_add_pd(s_0_0, _mm256_or_pd(v_0, mask_BLP));
              s_0_1 = _mm256_add_pd(s_0_1, _mm256_or_pd(v_1, mask_BLP));
              s_0_2 = _mm256_add_pd(s_0_2, _mm256_or_pd(v_2, mask_BLP));
              s_0_3 = _mm256_add_pd(s_0_3, _mm256_or_pd(v_3, mask_BLP));
              q_0 = _mm256_sub_pd(q_0, s_0_0);
              q_1 = _mm256_sub_pd(q_1, s_0_1);
              q_2 = _mm256_sub_pd(q_2, s_0_2);
              q_3 = _mm256_sub_pd(q_3, s_0_3);
              v_0 = _mm256_add_pd(v_0, q_0);
              v_1 = _mm256_add_pd(v_1, q_1);
              v_2 = _mm256_add_pd(v_2, q_2);
              v_3 = _mm256_add_pd(v_3, q_3);
              q_0 = s_1_0;
              q_1 = s_1_1;
              q_2 = s_1_2;
              q_3 = s_1_3;
              s_1_0 = _mm256_add_pd(s_1_0, _mm256_or_pd(v_0, mask_BLP));
              s_1_1 = _mm256_add_pd(s_1_1, _mm256_or_pd(v_1, mask_BLP));
              s_1_2 = _mm256_add_pd(s_1_2, _mm256_or_pd(v_2, mask_BLP));
              s_1_3 = _mm256_add_pd(s_1_3, _mm256_or_pd(v_3, mask_BLP));
              q_0 = _mm256_sub_pd(q_0, s_1_0);
              q_1 = _mm256_sub_pd(q_1, s_1_1);
              q_2 = _mm256_sub_pd(q_2, s_1_2);
              q_3 = _mm256_sub_pd(q_3, s_1_3);
              v_0 = _mm256_add_pd(v_0, q_0);
              v_1 = _mm256_add_pd(v_1, q_1);
              v_2 = _mm256_add_pd(v_2, q_2);
              v_3 = _mm256_add_pd(v_3, q_3);
              s_2_0 = _mm256_add_pd(s_2_0, _mm256_or_pd(v_0, mask_BLP));
              s_2_1 = _mm256_add_pd(s_2_1, _mm256_or_pd(v_1, mask_BLP));
              s_2_2 = _mm256_add_pd(s_2_2, _mm256_or_pd(v_2, mask_BLP));
              s_2_3 = _mm256_add_pd(s_2_3, _mm256_or_pd(v_3, mask_BLP));
              i += 4, v_base += 8, y_base += (incy * 8);
            }
            if(i + 2 <= n){
              v_0 = _mm256_loadu_pd(v_base);
              y_0 = _mm256_set_pd(y_base[((incy * 2) + 1)], y_base[(incy * 2)], y_base[1], y_base[0]);
              v_1 = _mm256_xor_pd(_mm256_mul_pd(_mm256_permute_pd(v_0, 0b0101), _mm256_permute_pd(y_0, 0b1111)), mask_NCONJ);
              v_0 = _mm256_mul_pd(v_0, _mm256_permute_pd(y_0, 0b0000));
              q_0 = s_0_0;
              q_1 = s_0_1;
              s_0_0 = _mm256_add_pd(s_0_0, _mm256_or_pd(v_0, mask_BLP));
              s_0_1 = _mm256_add_pd(s_0_1, _mm256_or_pd(v_1, mask_BLP));
              q_0 = _mm256_sub_pd(q_0, s_0_0);
              q_1 = _mm256_sub_pd(q_1, s_0_1);
              v_0 = _mm256_add_pd(v_0, q_0);
              v_1 = _mm256_add_pd(v_1, q_1);
              q_0 = s_1_0;
              q_1 = s_1_1;
              s_1_0 = _mm256_add_pd(s_1_0, _mm256_or_pd(v_0, mask_BLP));
              s_1_1 = _mm256_add_pd(s_1_1, _mm256_or_pd(v_1, mask_BLP));
              q_0 = _mm256_sub_pd(q_0, s_1_0);
              q_1 = _mm256_sub_pd(q_1, s_1_1);
              v_0 = _mm256_add_pd(v_0, q_0);
              v_1 = _mm256_add_pd(v_1, q_1);
              s_2_0 = _mm256_add_pd(s_2_0, _mm256_or_pd(v_0, mask_BLP));
              s_2_1 = _mm256_add_pd(s_2_1, _mm256_or_pd(v_1, mask_BLP));
              i += 2, v_base += 4, y_base += (incy * 4);
            }
            if(i < n){
              v_0 = _mm256_set_pd(0, 0, v_base[1], v_base[0]);
              y_0 = _mm256_set_pd(0, 0, y_base[1], y_base[0]);
              v_1 = _mm256_xor_pd(_mm256_mul_pd(_mm256_permute_pd(v_0, 0b0101), _mm256_permute_pd(y_0, 0b1111)), mask_NCONJ);
              v_0 = _mm256_mul_pd(v_0, _mm256_permute_pd(y_0, 0b0000));
              q_0 = s_0_0;
              q_1 = s_0_1;
              s_0_0 = _mm256_add_pd(s_0_0, _mm256_or_pd(v_0, mask_BLP));
              s_0_1 = _mm256_add_pd(s_0_1, _mm256_or_pd(v_1, mask_BLP));
              q_0 = _mm256_sub_pd(q_0, s_0_0);
              q_1 = _mm256_sub_pd(q_1, s_0_1);
              v_0 = _mm256_add_pd(v_0, q_0);
              v_1 = _mm256_add_pd(v_1, q_1);
              q_0 = s_1_0;
              q_1 = s_1_1;
              s_1_0 = _mm256_add_pd(s_1_0, _mm256_or_pd(v_0, mask_BLP));
              s_1_1 = _mm256_add_pd(s_1_1, _mm256_or_pd(v_1, mask_BLP));
              q_0 = _mm256_sub_pd(q_0, s_1_0);
              q_1 = _mm256_sub_pd(q_1, s_1_1);
              v_0 = _mm256_add_pd(v_0, q_0);
              v_1 = _mm256_add_pd(v_1, q_1);
              s_2_0 = _mm256_add_pd(s_2_0, _mm256_or_pd(v_0, mask_BLP));
              s_2_1 = _mm256_add_pd(s_2_1, _mm256_or_pd(v_1, mask_BLP));
            }
          }
        }else{
          if(incy == 1){

            for(i = 0; i + 8 <= n; i += 8, v_base += (incv * 16), y_base += 16){
              v_0 = _mm256_set_pd(v_base[((incv * 2) + 1)], v_base[(incv * 2)], v_base[1], v_base[0]);
              v_1 = _mm256_set_pd(v_base[((incv * 6) + 1)], v_base[(incv * 6)], v_base[((incv * 4) + 1)], v_base[(incv * 4)]);
              v_2 = _mm256_set_pd(v_base[((incv * 10) + 1)], v_base[(incv * 10)], v_base[((incv * 8) + 1)], v_base[(incv * 8)]);
              v_3 = _mm256_set_pd(v_base[((incv * 14) + 1)], v_base[(incv * 14)], v_base[((incv * 12) + 1)], v_base[(incv * 12)]);
              y_0 = _mm256_loadu_pd(y_base);
              y_1 = _mm256_loadu_pd(y_base + 4);
              y_2 = _mm256_loadu_pd(y_base + 8);
              y_3 = _mm256_loadu_pd(y_base + 12);
              v_4 = _mm256_xor_pd(_mm256_mul_pd(_mm256_permute_pd(v_0, 0b0101), _mm256_permute_pd(y_0, 0b1111)), mask_NCONJ);
              v_5 = _mm256_xor_pd(_mm256_mul_pd(_mm256_permute_pd(v_1, 0b0101), _mm256_permute_pd(y_1, 0b1111)), mask_NCONJ);
              v_6 = _mm256_xor_pd(_mm256_mul_pd(_mm256_permute_pd(v_2, 0b0101), _mm256_permute_pd(y_2, 0b1111)), mask_NCONJ);
              v_7 = _mm256_xor_pd(_mm256_mul_pd(_mm256_permute_pd(v_3, 0b0101), _mm256_permute_pd(y_3, 0b1111)), mask_NCONJ);
              v_0 = _mm256_mul_pd(v_0, _mm256_permute_pd(y_0, 0b0000));
              v_1 = _mm256_mul_pd(v_1, _mm256_permute_pd(y_1, 0b0000));
              v_2 = _mm256_mul_pd(v_2, _mm256_permute_pd(y_2, 0b0000));
              v_3 = _mm256_mul_pd(v_3, _mm256_permute_pd(y_3, 0b0000));
              q_0 = s_0_0;
              q_1 = s_0_1;
              q_2 = s_0_2;
              q_3 = s_0_3;
              s_0_0 = _mm256_add_pd(s_0_0, _mm256_or_pd(v_0, mask_BLP));
              s_0_1 = _mm256_add_pd(s_0_1, _mm256_or_pd(v_1, mask_BLP));
              s_0_2 = _mm256_add_pd(s_0_2, _mm256_or_pd(v_2, mask_BLP));
              s_0_3 = _mm256_add_pd(s_0_3, _mm256_or_pd(v_3, mask_BLP));
              q_0 = _mm256_sub_pd(q_0, s_0_0);
              q_1 = _mm256_sub_pd(q_1, s_0_1);
              q_2 = _mm256_sub_pd(q_2, s_0_2);
              q_3 = _mm256_sub_pd(q_3, s_0_3);
              v_0 = _mm256_add_pd(v_0, q_0);
              v_1 = _mm256_add_pd(v_1, q_1);
              v_2 = _mm256_add_pd(v_2, q_2);
              v_3 = _mm256_add_pd(v_3, q_3);
              q_0 = s_1_0;
              q_1 = s_1_1;
              q_2 = s_1_2;
              q_3 = s_1_3;
              s_1_0 = _mm256_add_pd(s_1_0, _mm256_or_pd(v_0, mask_BLP));
              s_1_1 = _mm256_add_pd(s_1_1, _mm256_or_pd(v_1, mask_BLP));
              s_1_2 = _mm256_add_pd(s_1_2, _mm256_or_pd(v_2, mask_BLP));
              s_1_3 = _mm256_add_pd(s_1_3, _mm256_or_pd(v_3, mask_BLP));
              q_0 = _mm256_sub_pd(q_0, s_1_0);
              q_1 = _mm256_sub_pd(q_1, s_1_1);
              q_2 = _mm256_sub_pd(q_2, s_1_2);
              q_3 = _mm256_sub_pd(q_3, s_1_3);
              v_0 = _mm256_add_pd(v_0, q_0);
              v_1 = _mm256_add_pd(v_1, q_1);
              v_2 = _mm256_add_pd(v_2, q_2);
              v_3 = _mm256_add_pd(v_3, q_3);
              s_2_0 = _mm256_add_pd(s_2_0, _mm256_or_pd(v_0, mask_BLP));
              s_2_1 = _mm256_add_pd(s_2_1, _mm256_or_pd(v_1, mask_BLP));
              s_2_2 = _mm256_add_pd(s_2_2, _mm256_or_pd(v_2, mask_BLP));
              s_2_3 = _mm256_add_pd(s_2_3, _mm256_or_pd(v_3, mask_BLP));
              q_0 = s_0_0;
              q_1 = s_0_1;
              q_2 = s_0_2;
              q_3 = s_0_3;
              s_0_0 = _mm256_add_pd(s_0_0, _mm256_or_pd(v_4, mask_BLP));
              s_0_1 = _mm256_add_pd(s_0_1, _mm256_or_pd(v_5, mask_BLP));
              s_0_2 = _mm256_add_pd(s_0_2, _mm256_or_pd(v_6, mask_BLP));
              s_0_3 = _mm256_add_pd(s_0_3, _mm256_or_pd(v_7, mask_BLP));
              q_0 = _mm256_sub_pd(q_0, s_0_0);
              q_1 = _mm256_sub_pd(q_1, s_0_1);
              q_2 = _mm256_sub_pd(q_2, s_0_2);
              q_3 = _mm256_sub_pd(q_3, s_0_3);
              v_4 = _mm256_add_pd(v_4, q_0);
              v_5 = _mm256_add_pd(v_5, q_1);
              v_6 = _mm256_add_pd(v_6, q_2);
              v_7 = _mm256_add_pd(v_7, q_3);
              q_0 = s_1_0;
              q_1 = s_1_1;
              q_2 = s_1_2;
              q_3 = s_1_3;
              s_1_0 = _mm256_add_pd(s_1_0, _mm256_or_pd(v_4, mask_BLP));
              s_1_1 = _mm256_add_pd(s_1_1, _mm256_or_pd(v_5, mask_BLP));
              s_1_2 = _mm256_add_pd(s_1_2, _mm256_or_pd(v_6, mask_BLP));
              s_1_3 = _mm256_add_pd(s_1_3, _mm256_or_pd(v_7, mask_BLP));
              q_0 = _mm256_sub_pd(q_0, s_1_0);
              q_1 = _mm256_sub_pd(q_1, s_1_1);
              q_2 = _mm256_sub_pd(q_2, s_1_2);
              q_3 = _mm256_sub_pd(q_3, s_1_3);
              v_4 = _mm256_add_pd(v_4, q_0);
              v_5 = _mm256_add_pd(v_5, q_1);
              v_6 = _mm256_add_pd(v_6, q_2);
              v_7 = _mm256_add_pd(v_7, q_3);
              s_2_0 = _mm256_add_pd(s_2_0, _mm256_or_pd(v_4, mask_BLP));
              s_2_1 = _mm256_add_pd(s_2_1, _mm256_or_pd(v_5, mask_BLP));
              s_2_2 = _mm256_add_pd(s_2_2, _mm256_or_pd(v_6, mask_BLP));
              s_2_3 = _mm256_add_pd(s_2_3, _mm256_or_pd(v_7, mask_BLP));
            }
            if(i + 4 <= n){
              v_0 = _mm256_set_pd(v_base[((incv * 2) + 1)], v_base[(incv * 2)], v_base[1], v_base[0]);
              v_1 = _mm256_set_pd(v_base[((incv * 6) + 1)], v_base[(incv * 6)], v_base[((incv * 4) + 1)], v_base[(incv * 4)]);
              y_0 = _mm256_loadu_pd(y_base);
              y_1 = _mm256_loadu_pd(y_base + 4);
              v_2 = _mm256_xor_pd(_mm256_mul_pd(_mm256_permute_pd(v_0, 0b0101), _mm256_permute_pd(y_0, 0b1111)), mask_NCONJ);
              v_3 = _mm256_xor_pd(_mm256_mul_pd(_mm256_permute_pd(v_1, 0b0101), _mm256_permute_pd(y_1, 0b1111)), mask_NCONJ);
              v_0 = _mm256_mul_pd(v_0, _mm256_permute_pd(y_0, 0b0000));
              v_1 = _mm256_mul_pd(v_1, _mm256_permute_pd(y_1, 0b0000));
              q_0 = s_0_0;
              q_1 = s_0_1;
              q_2 = s_0_2;
              q_3 = s_0_3;
              s_0_0 = _mm256_add_pd(s_0_0, _mm256_or_pd(v_0, mask_BLP));
              s_0_1 = _mm256_add_pd(s_0_1, _mm256_or_pd(v_1, mask_BLP));
              s_0_2 = _mm256_add_pd(s_0_2, _mm256_or_pd(v_2, mask_BLP));
              s_0_3 = _mm256_add_pd(s_0_3, _mm256_or_pd(v_3, mask_BLP));
              q_0 = _mm256_sub_pd(q_0, s_0_0);
              q_1 = _mm256_sub_pd(q_1, s_0_1);
              q_2 = _mm256_sub_pd(q_2, s_0_2);
              q_3 = _mm256_sub_pd(q_3, s_0_3);
              v_0 = _mm256_add_pd(v_0, q_0);
              v_1 = _mm256_add_pd(v_1, q_1);
              v_2 = _mm256_add_pd(v_2, q_2);
              v_3 = _mm256_add_pd(v_3, q_3);
              q_0 = s_1_0;
              q_1 = s_1_1;
              q_2 = s_1_2;
              q_3 = s_1_3;
              s_1_0 = _mm256_add_pd(s_1_0, _mm256_or_pd(v_0, mask_BLP));
              s_1_1 = _mm256_add_pd(s_1_1, _mm256_or_pd(v_1, mask_BLP));
              s_1_2 = _mm256_add_pd(s_1_2, _mm256_or_pd(v_2, mask_BLP));
              s_1_3 = _mm256_add_pd(s_1_3, _mm256_or_pd(v_3, mask_BLP));
              q_0 = _mm256_sub_pd(q_0, s_1_0);
              q_1 = _mm256_sub_pd(q_1, s_1_1);
              q_2 = _mm256_sub_pd(q_2, s_1_2);
              q_3 = _mm256_sub_pd(q_3, s_1_3);
              v_0 = _mm256_add_pd(v_0, q_0);
              v_1 = _mm256_add_pd(v_1, q_1);
              v_2 = _mm256_add_pd(v_2, q_2);
              v_3 = _mm256_add_pd(v_3, q_3);
              s_2_0 = _mm256_add_pd(s_2_0, _mm256_or_pd(v_0, mask_BLP));
              s_2_1 = _mm256_add_pd(s_2_1, _mm256_or_pd(v_1, mask_BLP));
              s_2_2 = _mm256_add_pd(s_2_2, _mm256_or_pd(v_2, mask_BLP));
              s_2_3 = _mm256_add_pd(s_2_3, _mm256_or_pd(v_3, mask_BLP));
              i += 4, v_base += (incv * 8), y_base += 8;
            }
            if(i + 2 <= n){
              v_0 = _mm256_set_pd(v_base[((incv * 2) + 1)], v_base[(incv * 2)], v_base[1], v_base[0]);
              y_0 = _mm256_loadu_pd(y_base);
              v_1 = _mm256_xor_pd(_mm256_mul_pd(_mm256_permute_pd(v_0, 0b0101), _mm256_permute_pd(y_0, 0b1111)), mask_NCONJ);
              v_0 = _mm256_mul_pd(v_0, _mm256_permute_pd(y_0, 0b0000));
              q_0 = s_0_0;
              q_1 = s_0_1;
              s_0_0 = _mm256_add_pd(s_0_0, _mm256_or_pd(v_0, mask_BLP));
              s_0_1 = _mm256_add_pd(s_0_1, _mm256_or_pd(v_1, mask_BLP));
              q_0 = _mm256_sub_pd(q_0, s_0_0);
              q_1 = _mm256_sub_pd(q_1, s_0_1);
              v_0 = _mm256_add_pd(v_0, q_0);
              v_1 = _mm256_add_pd(v_1, q_1);
              q_0 = s_1_0;
              q_1 = s_1_1;
              s_1_0 = _mm256_add_pd(s_1_0, _mm256_or_pd(v_0, mask_BLP));
              s_1_1 = _mm256_add_pd(s_1_1, _mm256_or_pd(v_1, mask_BLP));
              q_0 = _mm256_sub_pd(q_0, s_1_0);
              q_1 = _mm256_sub_pd(q_1, s_1_1);
              v_0 = _mm256_add_pd(v_0, q_0);
              v_1 = _mm256_add_pd(v_1, q_1);
              s_2_0 = _mm256_add_pd(s_2_0, _mm256_or_pd(v_0, mask_BLP));
              s_2_1 = _mm256_add_pd(s_2_1, _mm256_or_pd(v_1, mask_BLP));
              i += 2, v_base += (incv * 4), y_base += 4;
            }
            if(i < n){
              v_0 = _mm256_set_pd(0, 0, v_base[1], v_base[0]);
              y_0 = _mm256_set_pd(0, 0, y_base[1], y_base[0]);
              v_1 = _mm256_xor_pd(_mm256_mul_pd(_mm256_permute_pd(v_0, 0b0101), _mm256_permute_pd(y_0, 0b1111)), mask_NCONJ);
              v_0 = _mm256_mul_pd(v_0, _mm256_permute_pd(y_0, 0b0000));
              q_0 = s_0_0;
              q_1 = s_0_1;
              s_0_0 = _mm256_add_pd(s_0_0, _mm256_or_pd(v_0, mask_BLP));
              s_0_1 = _mm256_add_pd(s_0_1, _mm256_or_pd(v_1, mask_BLP));
              q_0 = _mm256_sub_pd(q_0, s_0_0);
              q_1 = _mm256_sub_pd(q_1, s_0_1);
              v_0 = _mm256_add_pd(v_0, q_0);
              v_1 = _mm256_add_pd(v_1, q_1);
              q_0 = s_1_0;
              q_1 = s_1_1;
              s_1_0 = _mm256_add_pd(s_1_0, _mm256_or_pd(v_0, mask_BLP));
              s_1_1 = _mm256_add_pd(s_1_1, _mm256_or_pd(v_1, mask_BLP));
              q_0 = _mm256_sub_pd(q_0, s_1_0);
              q_1 = _mm256_sub_pd(q_1, s_1_1);
              v_0 = _mm256_add_pd(v_0, q_0);
              v_1 = _mm256_add_pd(v_1, q_1);
              s_2_0 = _mm256_add_pd(s_2_0, _mm256_or_pd(v_0, mask_BLP));
              s_2_1 = _mm256_add_pd(s_2_1, _mm256_or_pd(v_1, mask_BLP));
            }
          }else{

            for(i = 0; i + 8 <= n; i += 8, v_base += (incv * 16), y_base += (incy * 16)){
              v_0 = _mm256_set_pd(v_base[((incv * 2) + 1)], v_base[(incv * 2)], v_base[1], v_base[0]);
              v_1 = _mm256_set_pd(v_base[((incv * 6) + 1)], v_base[(incv * 6)], v_base[((incv * 4) + 1)], v_base[(incv * 4)]);
              v_2 = _mm256_set_pd(v_base[((incv * 10) + 1)], v_base[(incv * 10)], v_base[((incv * 8) + 1)], v_base[(incv * 8)]);
              v_3 = _mm256_set_pd(v_base[((incv * 14) + 1)], v_base[(incv * 14)], v_base[((incv * 12) + 1)], v_base[(incv * 12)]);
              y_0 = _mm256_set_pd(y_base[((incy * 2) + 1)], y_base[(incy * 2)], y_base[1], y_base[0]);
              y_1 = _mm256_set_pd(y_base[((incy * 6) + 1)], y_base[(incy * 6)], y_base[((incy * 4) + 1)], y_base[(incy * 4)]);
              y_2 = _mm256_set_pd(y_base[((incy * 10) + 1)], y_base[(incy * 10)], y_base[((incy * 8) + 1)], y_base[(incy * 8)]);
              y_3 = _mm256_set_pd(y_base[((incy * 14) + 1)], y_base[(incy * 14)], y_base[((incy * 12) + 1)], y_base[(incy * 12)]);
              v_4 = _mm256_xor_pd(_mm256_mul_pd(_mm256_permute_pd(v_0, 0b0101), _mm256_permute_pd(y_0, 0b1111)), mask_NCONJ);
              v_5 = _mm256_xor_pd(_mm256_mul_pd(_mm256_permute_pd(v_1, 0b0101), _mm256_permute_pd(y_1, 0b1111)), mask_NCONJ);
              v_6 = _mm256_xor_pd(_mm256_mul_pd(_mm256_permute_pd(v_2, 0b0101), _mm256_permute_pd(y_2, 0b1111)), mask_NCONJ);
              v_7 = _mm256_xor_pd(_mm256_mul_pd(_mm256_permute_pd(v_3, 0b0101), _mm256_permute_pd(y_3, 0b1111)), mask_NCONJ);
              v_0 = _mm256_mul_pd(v_0, _mm256_permute_pd(y_0, 0b0000));
              v_1 = _mm256_mul_pd(v_1, _mm256_permute_pd(y_1, 0b0000));
              v_2 = _mm256_mul_pd(v_2, _mm256_permute_pd(y_2, 0b0000));
              v_3 = _mm256_mul_pd(v_3, _mm256_permute_pd(y_3, 0b0000));
              q_0 = s_0_0;
              q_1 = s_0_1;
              q_2 = s_0_2;
              q_3 = s_0_3;
              s_0_0 = _mm256_add_pd(s_0_0, _mm256_or_pd(v_0, mask_BLP));
              s_0_1 = _mm256_add_pd(s_0_1, _mm256_or_pd(v_1, mask_BLP));
              s_0_2 = _mm256_add_pd(s_0_2, _mm256_or_pd(v_2, mask_BLP));
              s_0_3 = _mm256_add_pd(s_0_3, _mm256_or_pd(v_3, mask_BLP));
              q_0 = _mm256_sub_pd(q_0, s_0_0);
              q_1 = _mm256_sub_pd(q_1, s_0_1);
              q_2 = _mm256_sub_pd(q_2, s_0_2);
              q_3 = _mm256_sub_pd(q_3, s_0_3);
              v_0 = _mm256_add_pd(v_0, q_0);
              v_1 = _mm256_add_pd(v_1, q_1);
              v_2 = _mm256_add_pd(v_2, q_2);
              v_3 = _mm256_add_pd(v_3, q_3);
              q_0 = s_1_0;
              q_1 = s_1_1;
              q_2 = s_1_2;
              q_3 = s_1_3;
              s_1_0 = _mm256_add_pd(s_1_0, _mm256_or_pd(v_0, mask_BLP));
              s_1_1 = _mm256_add_pd(s_1_1, _mm256_or_pd(v_1, mask_BLP));
              s_1_2 = _mm256_add_pd(s_1_2, _mm256_or_pd(v_2, mask_BLP));
              s_1_3 = _mm256_add_pd(s_1_3, _mm256_or_pd(v_3, mask_BLP));
              q_0 = _mm256_sub_pd(q_0, s_1_0);
              q_1 = _mm256_sub_pd(q_1, s_1_1);
              q_2 = _mm256_sub_pd(q_2, s_1_2);
              q_3 = _mm256_sub_pd(q_3, s_1_3);
              v_0 = _mm256_add_pd(v_0, q_0);
              v_1 = _mm256_add_pd(v_1, q_1);
              v_2 = _mm256_add_pd(v_2, q_2);
              v_3 = _mm256_add_pd(v_3, q_3);
              s_2_0 = _mm256_add_pd(s_2_0, _mm256_or_pd(v_0, mask_BLP));
              s_2_1 = _mm256_add_pd(s_2_1, _mm256_or_pd(v_1, mask_BLP));
              s_2_2 = _mm256_add_pd(s_2_2, _mm256_or_pd(v_2, mask_BLP));
              s_2_3 = _mm256_add_pd(s_2_3, _mm256_or_pd(v_3, mask_BLP));
              q_0 = s_0_0;
              q_1 = s_0_1;
              q_2 = s_0_2;
              q_3 = s_0_3;
              s_0_0 = _mm256_add_pd(s_0_0, _mm256_or_pd(v_4, mask_BLP));
              s_0_1 = _mm256_add_pd(s_0_1, _mm256_or_pd(v_5, mask_BLP));
              s_0_2 = _mm256_add_pd(s_0_2, _mm256_or_pd(v_6, mask_BLP));
              s_0_3 = _mm256_add_pd(s_0_3, _mm256_or_pd(v_7, mask_BLP));
              q_0 = _mm256_sub_pd(q_0, s_0_0);
              q_1 = _mm256_sub_pd(q_1, s_0_1);
              q_2 = _mm256_sub_pd(q_2, s_0_2);
              q_3 = _mm256_sub_pd(q_3, s_0_3);
              v_4 = _mm256_add_pd(v_4, q_0);
              v_5 = _mm256_add_pd(v_5, q_1);
              v_6 = _mm256_add_pd(v_6, q_2);
              v_7 = _mm256_add_pd(v_7, q_3);
              q_0 = s_1_0;
              q_1 = s_1_1;
              q_2 = s_1_2;
              q_3 = s_1_3;
              s_1_0 = _mm256_add_pd(s_1_0, _mm256_or_pd(v_4, mask_BLP));
              s_1_1 = _mm256_add_pd(s_1_1, _mm256_or_pd(v_5, mask_BLP));
              s_1_2 = _mm256_add_pd(s_1_2, _mm256_or_pd(v_6, mask_BLP));
              s_1_3 = _mm256_add_pd(s_1_3, _mm256_or_pd(v_7, mask_BLP));
              q_0 = _mm256_sub_pd(q_0, s_1_0);
              q_1 = _mm256_sub_pd(q_1, s_1_1);
              q_2 = _mm256_sub_pd(q_2, s_1_2);
              q_3 = _mm256_sub_pd(q_3, s_1_3);
              v_4 = _mm256_add_pd(v_4, q_0);
              v_5 = _mm256_add_pd(v_5, q_1);
              v_6 = _mm256_add_pd(v_6, q_2);
              v_7 = _mm256_add_pd(v_7, q_3);
              s_2_0 = _mm256_add_pd(s_2_0, _mm256_or_pd(v_4, mask_BLP));
              s_2_1 = _mm256_add_pd(s_2_1, _mm256_or_pd(v_5, mask_BLP));
              s_2_2 = _mm256_add_pd(s_2_2, _mm256_or_pd(v_6, mask_BLP));
              s_2_3 = _mm256_add_pd(s_2_3, _mm256_or_pd(v_7, mask_BLP));
            }
            if(i + 4 <= n){
              v_0 = _mm256_set_pd(v_base[((incv * 2) + 1)], v_base[(incv * 2)], v_base[1], v_base[0]);
              v_1 = _mm256_set_pd(v_base[((incv * 6) + 1)], v_base[(incv * 6)], v_base[((incv * 4) + 1)], v_base[(incv * 4)]);
              y_0 = _mm256_set_pd(y_base[((incy * 2) + 1)], y_base[(incy * 2)], y_base[1], y_base[0]);
              y_1 = _mm256_set_pd(y_base[((incy * 6) + 1)], y_base[(incy * 6)], y_base[((incy * 4) + 1)], y_base[(incy * 4)]);
              v_2 = _mm256_xor_pd(_mm256_mul_pd(_mm256_permute_pd(v_0, 0b0101), _mm256_permute_pd(y_0, 0b1111)), mask_NCONJ);
              v_3 = _mm256_xor_pd(_mm256_mul_pd(_mm256_permute_pd(v_1, 0b0101), _mm256_permute_pd(y_1, 0b1111)), mask_NCONJ);
              v_0 = _mm256_mul_pd(v_0, _mm256_permute_pd(y_0, 0b0000));
              v_1 = _mm256_mul_pd(v_1, _mm256_permute_pd(y_1, 0b0000));
              q_0 = s_0_0;
              q_1 = s_0_1;
              q_2 = s_0_2;
              q_3 = s_0_3;
              s_0_0 = _mm256_add_pd(s_0_0, _mm256_or_pd(v_0, mask_BLP));
              s_0_1 = _mm256_add_pd(s_0_1, _mm256_or_pd(v_1, mask_BLP));
              s_0_2 = _mm256_add_pd(s_0_2, _mm256_or_pd(v_2, mask_BLP));
              s_0_3 = _mm256_add_pd(s_0_3, _mm256_or_pd(v_3, mask_BLP));
              q_0 = _mm256_sub_pd(q_0, s_0_0);
              q_1 = _mm256_sub_pd(q_1, s_0_1);
              q_2 = _mm256_sub_pd(q_2, s_0_2);
              q_3 = _mm256_sub_pd(q_3, s_0_3);
              v_0 = _mm256_add_pd(v_0, q_0);
              v_1 = _mm256_add_pd(v_1, q_1);
              v_2 = _mm256_add_pd(v_2, q_2);
              v_3 = _mm256_add_pd(v_3, q_3);
              q_0 = s_1_0;
              q_1 = s_1_1;
              q_2 = s_1_2;
              q_3 = s_1_3;
              s_1_0 = _mm256_add_pd(s_1_0, _mm256_or_pd(v_0, mask_BLP));
              s_1_1 = _mm256_add_pd(s_1_1, _mm256_or_pd(v_1, mask_BLP));
              s_1_2 = _mm256_add_pd(s_1_2, _mm256_or_pd(v_2, mask_BLP));
              s_1_3 = _mm256_add_pd(s_1_3, _mm256_or_pd(v_3, mask_BLP));
              q_0 = _mm256_sub_pd(q_0, s_1_0);
              q_1 = _mm256_sub_pd(q_1, s_1_1);
              q_2 = _mm256_sub_pd(q_2, s_1_2);
              q_3 = _mm256_sub_pd(q_3, s_1_3);
              v_0 = _mm256_add_pd(v_0, q_0);
              v_1 = _mm256_add_pd(v_1, q_1);
              v_2 = _mm256_add_pd(v_2, q_2);
              v_3 = _mm256_add_pd(v_3, q_3);
              s_2_0 = _mm256_add_pd(s_2_0, _mm256_or_pd(v_0, mask_BLP));
              s_2_1 = _mm256_add_pd(s_2_1, _mm256_or_pd(v_1, mask_BLP));
              s_2_2 = _mm256_add_pd(s_2_2, _mm256_or_pd(v_2, mask_BLP));
              s_2_3 = _mm256_add_pd(s_2_3, _mm256_or_pd(v_3, mask_BLP));
              i += 4, v_base += (incv * 8), y_base += (incy * 8);
            }
            if(i + 2 <= n){
              v_0 = _mm256_set_pd(v_base[((incv * 2) + 1)], v_base[(incv * 2)], v_base[1], v_base[0]);
              y_0 = _mm256_set_pd(y_base[((incy * 2) + 1)], y_base[(incy * 2)], y_base[1], y_base[0]);
              v_1 = _mm256_xor_pd(_mm256_mul_pd(_mm256_permute_pd(v_0, 0b0101), _mm256_permute_pd(y_0, 0b1111)), mask_NCONJ);
              v_0 = _mm256_mul_pd(v_0, _mm256_permute_pd(y_0, 0b0000));
              q_0 = s_0_0;
              q_1 = s_0_1;
              s_0_0 = _mm256_add_pd(s_0_0, _mm256_or_pd(v_0, mask_BLP));
              s_0_1 = _mm256_add_pd(s_0_1, _mm256_or_pd(v_1, mask_BLP));
              q_0 = _mm256_sub_pd(q_0, s_0_0);
              q_1 = _mm256_sub_pd(q_1, s_0_1);
              v_0 = _mm256_add_pd(v_0, q_0);
              v_1 = _mm256_add_pd(v_1, q_1);
              q_0 = s_1_0;
              q_1 = s_1_1;
              s_1_0 = _mm256_add_pd(s_1_0, _mm256_or_pd(v_0, mask_BLP));
              s_1_1 = _mm256_add_pd(s_1_1, _mm256_or_pd(v_1, mask_BLP));
              q_0 = _mm256_sub_pd(q_0, s_1_0);
              q_1 = _mm256_sub_pd(q_1, s_1_1);
              v_0 = _mm256_add_pd(v_0, q_0);
              v_1 = _mm256_add_pd(v_1, q_1);
              s_2_0 = _mm256_add_pd(s_2_0, _mm256_or_pd(v_0, mask_BLP));
              s_2_1 = _mm256_add_pd(s_2_1, _mm256_or_pd(v_1, mask_BLP));
              i += 2, v_base += (incv * 4), y_base += (incy * 4);
            }
            if(i < n){
              v_0 = _mm256_set_pd(0, 0, v_base[1], v_base[0]);
              y_0 = _mm256_set_pd(0, 0, y_base[1], y_base[0]);
              v_1 = _mm256_xor_pd(_mm256_mul_pd(_mm256_permute_pd(v_0, 0b0101), _mm256_permute_pd(y_0, 0b1111)), mask_NCONJ);
              v_0 = _mm256_mul_pd(v_0, _mm256_permute_pd(y_0, 0b0000));
              q_0 = s_0_0;
              q_1 = s_0_1;
              s_0_0 = _mm256_add_pd(s_0_0, _mm256_or_pd(v_0, mask_BLP));
              s_0_1 = _mm256_add_pd(s_0_1, _mm256_or_pd(v_1, mask_BLP));
              q_0 = _mm256_sub_pd(q_0, s_0_0);
              q_1 = _mm256_sub_pd(q_1, s_0_1);
              v_0 = _mm256_add_pd(v_0, q_0);
              v_1 = _mm256_add_pd(v_1, q_1);
              q_0 = s_1_0;
              q_1 = s_1_1;
              s_1_0 = _mm256_add_pd(s_1_0, _mm256_or_pd(v_0, mask_BLP));
              s_1_1 = _mm256_add_pd(s_1_1, _mm256_or_pd(v_1, mask_BLP));
              q_0 = _mm256_sub_pd(q_0, s_1_0);
              q_1 = _mm256_sub_pd(q_1, s_1_1);
              v_0 = _mm256_add_pd(v_0, q_0);
              v_1 = _mm256_add_pd(v_1, q_1);
              s_2_0 = _mm256_add_pd(s_2_0, _mm256_or_pd(v_0, mask_BLP));
              s_2_1 = _mm256_add_pd(s_2_1, _mm256_or_pd(v_1, mask_BLP));
            }
          }
        }
        s_0_0 = _mm256_sub_pd(s_0_0, _mm256_set_pd(sum_base[1], sum_base[0], 0, 0));
        q_0 = _mm256_broadcast_pd((__m128d *)(sum_base));
        s_0_0 = _mm256_add_pd(s_0_0, _mm256_sub_pd(s_0_1, q_0));
        s_0_0 = _mm256_add_pd(s_0_0, _mm256_sub_pd(s_0_2, q_0));
        s_0_0 = _mm256_add_pd(s_0_0, _mm256_sub_pd(s_0_3, q_0));
        _mm256_store_pd((double*)tmp_cons, s_0_0);
        sum[0] = tmp_cons[0] + tmp_cons[1];
        s_1_0 = _mm256_sub_pd(s_1_0, _mm256_set_pd(sum_base[3], sum_base[2], 0, 0));
        q_0 = _mm256_broadcast_pd((__m128d *)(sum_base + 2));
        s_1_0 = _mm256_add_pd(s_1_0, _mm256_sub_pd(s_1_1, q_0));
        s_1_0 = _mm256_add_pd(s_1_0, _mm256_sub_pd(s_1_2, q_0));
        s_1_0 = _mm256_add_pd(s_1_0, _mm256_sub_pd(s_1_3, q_0));
        _mm256_store_pd((double*)tmp_cons, s_1_0);
        sum[1] = tmp_cons[0] + tmp_cons[1];
        s_2_0 = _mm256_sub_pd(s_2_0, _mm256_set_pd(sum_base[5], sum_base[4], 0, 0));
        q_0 = _mm256_broadcast_pd((__m128d *)(sum_base + 4));
        s_2_0 = _mm256_add_pd(s_2_0, _mm256_sub_pd(s_2_1, q_0));
        s_2_0 = _mm256_add_pd(s_2_0, _mm256_sub_pd(s_2_2, q_0));
        s_2_0 = _mm256_add_pd(s_2_0, _mm256_sub_pd(s_2_3, q_0));
        _mm256_store_pd((double*)tmp_cons, s_2_0);
        sum[2] = tmp_cons[0] + tmp_cons[1];
        RESET_DAZ_FLAG
        return;
      }
      default:{
        int i, j;

        double* sum_base = (double*) sum;
        double* v_base = (double*) v;
        double* y_base = (double*) y;
        __m256d v_0, v_1, v_2, v_3, v_4, v_5, v_6, v_7;
        __m256d y_0, y_1, y_2, y_3;
        __m256d q_0, q_1, q_2, q_3;
        __m256d s_0, s_1, s_2, s_3;
        __m256d s_buffer[(MAX_FOLD * 4)];

        for(j = 0; j < fold; j += 1){
          s_buffer[(j * 4)] = s_buffer[((j * 4) + 1)] = s_buffer[((j * 4) + 2)] = s_buffer[((j * 4) + 3)] = _mm256_broadcast_pd((__m128d *)(sum_base + (j * 2)));
        }
        if(incv == 1){
          if(incy == 1){

            for(i = 0; i + 8 <= n; i += 8, v_base += 16, y_base += 16){
              v_0 = _mm256_loadu_pd(v_base);
              v_1 = _mm256_loadu_pd(v_base + 4);
              v_2 = _mm256_loadu_pd(v_base + 8);
              v_3 = _mm256_loadu_pd(v_base + 12);
              y_0 = _mm256_loadu_pd(y_base);
              y_1 = _mm256_loadu_pd(y_base + 4);
              y_2 = _mm256_loadu_pd(y_base + 8);
              y_3 = _mm256_loadu_pd(y_base + 12);
              v_4 = _mm256_xor_pd(_mm256_mul_pd(_mm256_permute_pd(v_0, 0b0101), _mm256_permute_pd(y_0, 0b1111)), mask_NCONJ);
              v_5 = _mm256_xor_pd(_mm256_mul_pd(_mm256_permute_pd(v_1, 0b0101), _mm256_permute_pd(y_1, 0b1111)), mask_NCONJ);
              v_6 = _mm256_xor_pd(_mm256_mul_pd(_mm256_permute_pd(v_2, 0b0101), _mm256_permute_pd(y_2, 0b1111)), mask_NCONJ);
              v_7 = _mm256_xor_pd(_mm256_mul_pd(_mm256_permute_pd(v_3, 0b0101), _mm256_permute_pd(y_3, 0b1111)), mask_NCONJ);
              v_0 = _mm256_mul_pd(v_0, _mm256_permute_pd(y_0, 0b0000));
              v_1 = _mm256_mul_pd(v_1, _mm256_permute_pd(y_1, 0b0000));
              v_2 = _mm256_mul_pd(v_2, _mm256_permute_pd(y_2, 0b0000));
              v_3 = _mm256_mul_pd(v_3, _mm256_permute_pd(y_3, 0b0000));
              for(j = 0; j < fold - 1; j++){
                s_0 = s_buffer[(j * 4)];
                s_1 = s_buffer[((j * 4) + 1)];
                s_2 = s_buffer[((j * 4) + 2)];
                s_3 = s_buffer[((j * 4) + 3)];
                q_0 = _mm256_add_pd(s_0, _mm256_or_pd(v_0, mask_BLP));
                q_1 = _mm256_add_pd(s_1, _mm256_or_pd(v_1, mask_BLP));
                q_2 = _mm256_add_pd(s_2, _mm256_or_pd(v_2, mask_BLP));
                q_3 = _mm256_add_pd(s_3, _mm256_or_pd(v_3, mask_BLP));
                s_buffer[(j * 4)] = q_0;
                s_buffer[((j * 4) + 1)] = q_1;
                s_buffer[((j * 4) + 2)] = q_2;
                s_buffer[((j * 4) + 3)] = q_3;
                q_0 = _mm256_sub_pd(s_0, q_0);
                q_1 = _mm256_sub_pd(s_1, q_1);
                q_2 = _mm256_sub_pd(s_2, q_2);
                q_3 = _mm256_sub_pd(s_3, q_3);
                v_0 = _mm256_add_pd(v_0, q_0);
                v_1 = _mm256_add_pd(v_1, q_1);
                v_2 = _mm256_add_pd(v_2, q_2);
                v_3 = _mm256_add_pd(v_3, q_3);
                s_0 = s_buffer[(j * 4)];
                s_1 = s_buffer[((j * 4) + 1)];
                s_2 = s_buffer[((j * 4) + 2)];
                s_3 = s_buffer[((j * 4) + 3)];
                q_0 = _mm256_add_pd(s_0, _mm256_or_pd(v_4, mask_BLP));
                q_1 = _mm256_add_pd(s_1, _mm256_or_pd(v_5, mask_BLP));
                q_2 = _mm256_add_pd(s_2, _mm256_or_pd(v_6, mask_BLP));
                q_3 = _mm256_add_pd(s_3, _mm256_or_pd(v_7, mask_BLP));
                s_buffer[(j * 4)] = q_0;
                s_buffer[((j * 4) + 1)] = q_1;
                s_buffer[((j * 4) + 2)] = q_2;
                s_buffer[((j * 4) + 3)] = q_3;
                q_0 = _mm256_sub_pd(s_0, q_0);
                q_1 = _mm256_sub_pd(s_1, q_1);
                q_2 = _mm256_sub_pd(s_2, q_2);
                q_3 = _mm256_sub_pd(s_3, q_3);
                v_4 = _mm256_add_pd(v_4, q_0);
                v_5 = _mm256_add_pd(v_5, q_1);
                v_6 = _mm256_add_pd(v_6, q_2);
                v_7 = _mm256_add_pd(v_7, q_3);
              }
              s_buffer[(j * 4)] = _mm256_add_pd(s_buffer[(j * 4)], _mm256_or_pd(v_4, mask_BLP));
              s_buffer[((j * 4) + 1)] = _mm256_add_pd(s_buffer[((j * 4) + 1)], _mm256_or_pd(v_5, mask_BLP));
              s_buffer[((j * 4) + 2)] = _mm256_add_pd(s_buffer[((j * 4) + 2)], _mm256_or_pd(v_6, mask_BLP));
              s_buffer[((j * 4) + 3)] = _mm256_add_pd(s_buffer[((j * 4) + 3)], _mm256_or_pd(v_7, mask_BLP));
            }
            if(i + 4 <= n){
              v_0 = _mm256_loadu_pd(v_base);
              v_1 = _mm256_loadu_pd(v_base + 4);
              y_0 = _mm256_loadu_pd(y_base);
              y_1 = _mm256_loadu_pd(y_base + 4);
              v_2 = _mm256_xor_pd(_mm256_mul_pd(_mm256_permute_pd(v_0, 0b0101), _mm256_permute_pd(y_0, 0b1111)), mask_NCONJ);
              v_3 = _mm256_xor_pd(_mm256_mul_pd(_mm256_permute_pd(v_1, 0b0101), _mm256_permute_pd(y_1, 0b1111)), mask_NCONJ);
              v_0 = _mm256_mul_pd(v_0, _mm256_permute_pd(y_0, 0b0000));
              v_1 = _mm256_mul_pd(v_1, _mm256_permute_pd(y_1, 0b0000));
              for(j = 0; j < fold - 1; j++){
                s_0 = s_buffer[(j * 4)];
                s_1 = s_buffer[((j * 4) + 1)];
                s_2 = s_buffer[((j * 4) + 2)];
                s_3 = s_buffer[((j * 4) + 3)];
                q_0 = _mm256_add_pd(s_0, _mm256_or_pd(v_0, mask_BLP));
                q_1 = _mm256_add_pd(s_1, _mm256_or_pd(v_1, mask_BLP));
                q_2 = _mm256_add_pd(s_2, _mm256_or_pd(v_2, mask_BLP));
                q_3 = _mm256_add_pd(s_3, _mm256_or_pd(v_3, mask_BLP));
                s_buffer[(j * 4)] = q_0;
                s_buffer[((j * 4) + 1)] = q_1;
                s_buffer[((j * 4) + 2)] = q_2;
                s_buffer[((j * 4) + 3)] = q_3;
                q_0 = _mm256_sub_pd(s_0, q_0);
                q_1 = _mm256_sub_pd(s_1, q_1);
                q_2 = _mm256_sub_pd(s_2, q_2);
                q_3 = _mm256_sub_pd(s_3, q_3);
                v_0 = _mm256_add_pd(v_0, q_0);
                v_1 = _mm256_add_pd(v_1, q_1);
                v_2 = _mm256_add_pd(v_2, q_2);
                v_3 = _mm256_add_pd(v_3, q_3);
              }
              s_buffer[(j * 4)] = _mm256_add_pd(s_buffer[(j * 4)], _mm256_or_pd(v_0, mask_BLP));
              s_buffer[((j * 4) + 1)] = _mm256_add_pd(s_buffer[((j * 4) + 1)], _mm256_or_pd(v_1, mask_BLP));
              s_buffer[((j * 4) + 2)] = _mm256_add_pd(s_buffer[((j * 4) + 2)], _mm256_or_pd(v_2, mask_BLP));
              s_buffer[((j * 4) + 3)] = _mm256_add_pd(s_buffer[((j * 4) + 3)], _mm256_or_pd(v_3, mask_BLP));
              i += 4, v_base += 8, y_base += 8;
            }
            if(i + 2 <= n){
              v_0 = _mm256_loadu_pd(v_base);
              y_0 = _mm256_loadu_pd(y_base);
              v_1 = _mm256_xor_pd(_mm256_mul_pd(_mm256_permute_pd(v_0, 0b0101), _mm256_permute_pd(y_0, 0b1111)), mask_NCONJ);
              v_0 = _mm256_mul_pd(v_0, _mm256_permute_pd(y_0, 0b0000));
              for(j = 0; j < fold - 1; j++){
                s_0 = s_buffer[(j * 4)];
                s_1 = s_buffer[((j * 4) + 1)];
                q_0 = _mm256_add_pd(s_0, _mm256_or_pd(v_0, mask_BLP));
                q_1 = _mm256_add_pd(s_1, _mm256_or_pd(v_1, mask_BLP));
                s_buffer[(j * 4)] = q_0;
                s_buffer[((j * 4) + 1)] = q_1;
                q_0 = _mm256_sub_pd(s_0, q_0);
                q_1 = _mm256_sub_pd(s_1, q_1);
                v_0 = _mm256_add_pd(v_0, q_0);
                v_1 = _mm256_add_pd(v_1, q_1);
              }
              s_buffer[(j * 4)] = _mm256_add_pd(s_buffer[(j * 4)], _mm256_or_pd(v_0, mask_BLP));
              s_buffer[((j * 4) + 1)] = _mm256_add_pd(s_buffer[((j * 4) + 1)], _mm256_or_pd(v_1, mask_BLP));
              i += 2, v_base += 4, y_base += 4;
            }
            if(i < n){
              v_0 = _mm256_set_pd(0, 0, v_base[1], v_base[0]);
              y_0 = _mm256_set_pd(0, 0, y_base[1], y_base[0]);
              v_1 = _mm256_xor_pd(_mm256_mul_pd(_mm256_permute_pd(v_0, 0b0101), _mm256_permute_pd(y_0, 0b1111)), mask_NCONJ);
              v_0 = _mm256_mul_pd(v_0, _mm256_permute_pd(y_0, 0b0000));
              for(j = 0; j < fold - 1; j++){
                s_0 = s_buffer[(j * 4)];
                s_1 = s_buffer[((j * 4) + 1)];
                q_0 = _mm256_add_pd(s_0, _mm256_or_pd(v_0, mask_BLP));
                q_1 = _mm256_add_pd(s_1, _mm256_or_pd(v_1, mask_BLP));
                s_buffer[(j * 4)] = q_0;
                s_buffer[((j * 4) + 1)] = q_1;
                q_0 = _mm256_sub_pd(s_0, q_0);
                q_1 = _mm256_sub_pd(s_1, q_1);
                v_0 = _mm256_add_pd(v_0, q_0);
                v_1 = _mm256_add_pd(v_1, q_1);
              }
              s_buffer[(j * 4)] = _mm256_add_pd(s_buffer[(j * 4)], _mm256_or_pd(v_0, mask_BLP));
              s_buffer[((j * 4) + 1)] = _mm256_add_pd(s_buffer[((j * 4) + 1)], _mm256_or_pd(v_1, mask_BLP));
            }
          }else{

            for(i = 0; i + 8 <= n; i += 8, v_base += 16, y_base += (incy * 16)){
              v_0 = _mm256_loadu_pd(v_base);
              v_1 = _mm256_loadu_pd(v_base + 4);
              v_2 = _mm256_loadu_pd(v_base + 8);
              v_3 = _mm256_loadu_pd(v_base + 12);
              y_0 = _mm256_set_pd(y_base[((incy * 2) + 1)], y_base[(incy * 2)], y_base[1], y_base[0]);
              y_1 = _mm256_set_pd(y_base[((incy * 6) + 1)], y_base[(incy * 6)], y_base[((incy * 4) + 1)], y_base[(incy * 4)]);
              y_2 = _mm256_set_pd(y_base[((incy * 10) + 1)], y_base[(incy * 10)], y_base[((incy * 8) + 1)], y_base[(incy * 8)]);
              y_3 = _mm256_set_pd(y_base[((incy * 14) + 1)], y_base[(incy * 14)], y_base[((incy * 12) + 1)], y_base[(incy * 12)]);
              v_4 = _mm256_xor_pd(_mm256_mul_pd(_mm256_permute_pd(v_0, 0b0101), _mm256_permute_pd(y_0, 0b1111)), mask_NCONJ);
              v_5 = _mm256_xor_pd(_mm256_mul_pd(_mm256_permute_pd(v_1, 0b0101), _mm256_permute_pd(y_1, 0b1111)), mask_NCONJ);
              v_6 = _mm256_xor_pd(_mm256_mul_pd(_mm256_permute_pd(v_2, 0b0101), _mm256_permute_pd(y_2, 0b1111)), mask_NCONJ);
              v_7 = _mm256_xor_pd(_mm256_mul_pd(_mm256_permute_pd(v_3, 0b0101), _mm256_permute_pd(y_3, 0b1111)), mask_NCONJ);
              v_0 = _mm256_mul_pd(v_0, _mm256_permute_pd(y_0, 0b0000));
              v_1 = _mm256_mul_pd(v_1, _mm256_permute_pd(y_1, 0b0000));
              v_2 = _mm256_mul_pd(v_2, _mm256_permute_pd(y_2, 0b0000));
              v_3 = _mm256_mul_pd(v_3, _mm256_permute_pd(y_3, 0b0000));
              for(j = 0; j < fold - 1; j++){
                s_0 = s_buffer[(j * 4)];
                s_1 = s_buffer[((j * 4) + 1)];
                s_2 = s_buffer[((j * 4) + 2)];
                s_3 = s_buffer[((j * 4) + 3)];
                q_0 = _mm256_add_pd(s_0, _mm256_or_pd(v_0, mask_BLP));
                q_1 = _mm256_add_pd(s_1, _mm256_or_pd(v_1, mask_BLP));
                q_2 = _mm256_add_pd(s_2, _mm256_or_pd(v_2, mask_BLP));
                q_3 = _mm256_add_pd(s_3, _mm256_or_pd(v_3, mask_BLP));
                s_buffer[(j * 4)] = q_0;
                s_buffer[((j * 4) + 1)] = q_1;
                s_buffer[((j * 4) + 2)] = q_2;
                s_buffer[((j * 4) + 3)] = q_3;
                q_0 = _mm256_sub_pd(s_0, q_0);
                q_1 = _mm256_sub_pd(s_1, q_1);
                q_2 = _mm256_sub_pd(s_2, q_2);
                q_3 = _mm256_sub_pd(s_3, q_3);
                v_0 = _mm256_add_pd(v_0, q_0);
                v_1 = _mm256_add_pd(v_1, q_1);
                v_2 = _mm256_add_pd(v_2, q_2);
                v_3 = _mm256_add_pd(v_3, q_3);
                s_0 = s_buffer[(j * 4)];
                s_1 = s_buffer[((j * 4) + 1)];
                s_2 = s_buffer[((j * 4) + 2)];
                s_3 = s_buffer[((j * 4) + 3)];
                q_0 = _mm256_add_pd(s_0, _mm256_or_pd(v_4, mask_BLP));
                q_1 = _mm256_add_pd(s_1, _mm256_or_pd(v_5, mask_BLP));
                q_2 = _mm256_add_pd(s_2, _mm256_or_pd(v_6, mask_BLP));
                q_3 = _mm256_add_pd(s_3, _mm256_or_pd(v_7, mask_BLP));
                s_buffer[(j * 4)] = q_0;
                s_buffer[((j * 4) + 1)] = q_1;
                s_buffer[((j * 4) + 2)] = q_2;
                s_buffer[((j * 4) + 3)] = q_3;
                q_0 = _mm256_sub_pd(s_0, q_0);
                q_1 = _mm256_sub_pd(s_1, q_1);
                q_2 = _mm256_sub_pd(s_2, q_2);
                q_3 = _mm256_sub_pd(s_3, q_3);
                v_4 = _mm256_add_pd(v_4, q_0);
                v_5 = _mm256_add_pd(v_5, q_1);
                v_6 = _mm256_add_pd(v_6, q_2);
                v_7 = _mm256_add_pd(v_7, q_3);
              }
              s_buffer[(j * 4)] = _mm256_add_pd(s_buffer[(j * 4)], _mm256_or_pd(v_4, mask_BLP));
              s_buffer[((j * 4) + 1)] = _mm256_add_pd(s_buffer[((j * 4) + 1)], _mm256_or_pd(v_5, mask_BLP));
              s_buffer[((j * 4) + 2)] = _mm256_add_pd(s_buffer[((j * 4) + 2)], _mm256_or_pd(v_6, mask_BLP));
              s_buffer[((j * 4) + 3)] = _mm256_add_pd(s_buffer[((j * 4) + 3)], _mm256_or_pd(v_7, mask_BLP));
            }
            if(i + 4 <= n){
              v_0 = _mm256_loadu_pd(v_base);
              v_1 = _mm256_loadu_pd(v_base + 4);
              y_0 = _mm256_set_pd(y_base[((incy * 2) + 1)], y_base[(incy * 2)], y_base[1], y_base[0]);
              y_1 = _mm256_set_pd(y_base[((incy * 6) + 1)], y_base[(incy * 6)], y_base[((incy * 4) + 1)], y_base[(incy * 4)]);
              v_2 = _mm256_xor_pd(_mm256_mul_pd(_mm256_permute_pd(v_0, 0b0101), _mm256_permute_pd(y_0, 0b1111)), mask_NCONJ);
              v_3 = _mm256_xor_pd(_mm256_mul_pd(_mm256_permute_pd(v_1, 0b0101), _mm256_permute_pd(y_1, 0b1111)), mask_NCONJ);
              v_0 = _mm256_mul_pd(v_0, _mm256_permute_pd(y_0, 0b0000));
              v_1 = _mm256_mul_pd(v_1, _mm256_permute_pd(y_1, 0b0000));
              for(j = 0; j < fold - 1; j++){
                s_0 = s_buffer[(j * 4)];
                s_1 = s_buffer[((j * 4) + 1)];
                s_2 = s_buffer[((j * 4) + 2)];
                s_3 = s_buffer[((j * 4) + 3)];
                q_0 = _mm256_add_pd(s_0, _mm256_or_pd(v_0, mask_BLP));
                q_1 = _mm256_add_pd(s_1, _mm256_or_pd(v_1, mask_BLP));
                q_2 = _mm256_add_pd(s_2, _mm256_or_pd(v_2, mask_BLP));
                q_3 = _mm256_add_pd(s_3, _mm256_or_pd(v_3, mask_BLP));
                s_buffer[(j * 4)] = q_0;
                s_buffer[((j * 4) + 1)] = q_1;
                s_buffer[((j * 4) + 2)] = q_2;
                s_buffer[((j * 4) + 3)] = q_3;
                q_0 = _mm256_sub_pd(s_0, q_0);
                q_1 = _mm256_sub_pd(s_1, q_1);
                q_2 = _mm256_sub_pd(s_2, q_2);
                q_3 = _mm256_sub_pd(s_3, q_3);
                v_0 = _mm256_add_pd(v_0, q_0);
                v_1 = _mm256_add_pd(v_1, q_1);
                v_2 = _mm256_add_pd(v_2, q_2);
                v_3 = _mm256_add_pd(v_3, q_3);
              }
              s_buffer[(j * 4)] = _mm256_add_pd(s_buffer[(j * 4)], _mm256_or_pd(v_0, mask_BLP));
              s_buffer[((j * 4) + 1)] = _mm256_add_pd(s_buffer[((j * 4) + 1)], _mm256_or_pd(v_1, mask_BLP));
              s_buffer[((j * 4) + 2)] = _mm256_add_pd(s_buffer[((j * 4) + 2)], _mm256_or_pd(v_2, mask_BLP));
              s_buffer[((j * 4) + 3)] = _mm256_add_pd(s_buffer[((j * 4) + 3)], _mm256_or_pd(v_3, mask_BLP));
              i += 4, v_base += 8, y_base += (incy * 8);
            }
            if(i + 2 <= n){
              v_0 = _mm256_loadu_pd(v_base);
              y_0 = _mm256_set_pd(y_base[((incy * 2) + 1)], y_base[(incy * 2)], y_base[1], y_base[0]);
              v_1 = _mm256_xor_pd(_mm256_mul_pd(_mm256_permute_pd(v_0, 0b0101), _mm256_permute_pd(y_0, 0b1111)), mask_NCONJ);
              v_0 = _mm256_mul_pd(v_0, _mm256_permute_pd(y_0, 0b0000));
              for(j = 0; j < fold - 1; j++){
                s_0 = s_buffer[(j * 4)];
                s_1 = s_buffer[((j * 4) + 1)];
                q_0 = _mm256_add_pd(s_0, _mm256_or_pd(v_0, mask_BLP));
                q_1 = _mm256_add_pd(s_1, _mm256_or_pd(v_1, mask_BLP));
                s_buffer[(j * 4)] = q_0;
                s_buffer[((j * 4) + 1)] = q_1;
                q_0 = _mm256_sub_pd(s_0, q_0);
                q_1 = _mm256_sub_pd(s_1, q_1);
                v_0 = _mm256_add_pd(v_0, q_0);
                v_1 = _mm256_add_pd(v_1, q_1);
              }
              s_buffer[(j * 4)] = _mm256_add_pd(s_buffer[(j * 4)], _mm256_or_pd(v_0, mask_BLP));
              s_buffer[((j * 4) + 1)] = _mm256_add_pd(s_buffer[((j * 4) + 1)], _mm256_or_pd(v_1, mask_BLP));
              i += 2, v_base += 4, y_base += (incy * 4);
            }
            if(i < n){
              v_0 = _mm256_set_pd(0, 0, v_base[1], v_base[0]);
              y_0 = _mm256_set_pd(0, 0, y_base[1], y_base[0]);
              v_1 = _mm256_xor_pd(_mm256_mul_pd(_mm256_permute_pd(v_0, 0b0101), _mm256_permute_pd(y_0, 0b1111)), mask_NCONJ);
              v_0 = _mm256_mul_pd(v_0, _mm256_permute_pd(y_0, 0b0000));
              for(j = 0; j < fold - 1; j++){
                s_0 = s_buffer[(j * 4)];
                s_1 = s_buffer[((j * 4) + 1)];
                q_0 = _mm256_add_pd(s_0, _mm256_or_pd(v_0, mask_BLP));
                q_1 = _mm256_add_pd(s_1, _mm256_or_pd(v_1, mask_BLP));
                s_buffer[(j * 4)] = q_0;
                s_buffer[((j * 4) + 1)] = q_1;
                q_0 = _mm256_sub_pd(s_0, q_0);
                q_1 = _mm256_sub_pd(s_1, q_1);
                v_0 = _mm256_add_pd(v_0, q_0);
                v_1 = _mm256_add_pd(v_1, q_1);
              }
              s_buffer[(j * 4)] = _mm256_add_pd(s_buffer[(j * 4)], _mm256_or_pd(v_0, mask_BLP));
              s_buffer[((j * 4) + 1)] = _mm256_add_pd(s_buffer[((j * 4) + 1)], _mm256_or_pd(v_1, mask_BLP));
            }
          }
        }else{
          if(incy == 1){

            for(i = 0; i + 8 <= n; i += 8, v_base += (incv * 16), y_base += 16){
              v_0 = _mm256_set_pd(v_base[((incv * 2) + 1)], v_base[(incv * 2)], v_base[1], v_base[0]);
              v_1 = _mm256_set_pd(v_base[((incv * 6) + 1)], v_base[(incv * 6)], v_base[((incv * 4) + 1)], v_base[(incv * 4)]);
              v_2 = _mm256_set_pd(v_base[((incv * 10) + 1)], v_base[(incv * 10)], v_base[((incv * 8) + 1)], v_base[(incv * 8)]);
              v_3 = _mm256_set_pd(v_base[((incv * 14) + 1)], v_base[(incv * 14)], v_base[((incv * 12) + 1)], v_base[(incv * 12)]);
              y_0 = _mm256_loadu_pd(y_base);
              y_1 = _mm256_loadu_pd(y_base + 4);
              y_2 = _mm256_loadu_pd(y_base + 8);
              y_3 = _mm256_loadu_pd(y_base + 12);
              v_4 = _mm256_xor_pd(_mm256_mul_pd(_mm256_permute_pd(v_0, 0b0101), _mm256_permute_pd(y_0, 0b1111)), mask_NCONJ);
              v_5 = _mm256_xor_pd(_mm256_mul_pd(_mm256_permute_pd(v_1, 0b0101), _mm256_permute_pd(y_1, 0b1111)), mask_NCONJ);
              v_6 = _mm256_xor_pd(_mm256_mul_pd(_mm256_permute_pd(v_2, 0b0101), _mm256_permute_pd(y_2, 0b1111)), mask_NCONJ);
              v_7 = _mm256_xor_pd(_mm256_mul_pd(_mm256_permute_pd(v_3, 0b0101), _mm256_permute_pd(y_3, 0b1111)), mask_NCONJ);
              v_0 = _mm256_mul_pd(v_0, _mm256_permute_pd(y_0, 0b0000));
              v_1 = _mm256_mul_pd(v_1, _mm256_permute_pd(y_1, 0b0000));
              v_2 = _mm256_mul_pd(v_2, _mm256_permute_pd(y_2, 0b0000));
              v_3 = _mm256_mul_pd(v_3, _mm256_permute_pd(y_3, 0b0000));
              for(j = 0; j < fold - 1; j++){
                s_0 = s_buffer[(j * 4)];
                s_1 = s_buffer[((j * 4) + 1)];
                s_2 = s_buffer[((j * 4) + 2)];
                s_3 = s_buffer[((j * 4) + 3)];
                q_0 = _mm256_add_pd(s_0, _mm256_or_pd(v_0, mask_BLP));
                q_1 = _mm256_add_pd(s_1, _mm256_or_pd(v_1, mask_BLP));
                q_2 = _mm256_add_pd(s_2, _mm256_or_pd(v_2, mask_BLP));
                q_3 = _mm256_add_pd(s_3, _mm256_or_pd(v_3, mask_BLP));
                s_buffer[(j * 4)] = q_0;
                s_buffer[((j * 4) + 1)] = q_1;
                s_buffer[((j * 4) + 2)] = q_2;
                s_buffer[((j * 4) + 3)] = q_3;
                q_0 = _mm256_sub_pd(s_0, q_0);
                q_1 = _mm256_sub_pd(s_1, q_1);
                q_2 = _mm256_sub_pd(s_2, q_2);
                q_3 = _mm256_sub_pd(s_3, q_3);
                v_0 = _mm256_add_pd(v_0, q_0);
                v_1 = _mm256_add_pd(v_1, q_1);
                v_2 = _mm256_add_pd(v_2, q_2);
                v_3 = _mm256_add_pd(v_3, q_3);
                s_0 = s_buffer[(j * 4)];
                s_1 = s_buffer[((j * 4) + 1)];
                s_2 = s_buffer[((j * 4) + 2)];
                s_3 = s_buffer[((j * 4) + 3)];
                q_0 = _mm256_add_pd(s_0, _mm256_or_pd(v_4, mask_BLP));
                q_1 = _mm256_add_pd(s_1, _mm256_or_pd(v_5, mask_BLP));
                q_2 = _mm256_add_pd(s_2, _mm256_or_pd(v_6, mask_BLP));
                q_3 = _mm256_add_pd(s_3, _mm256_or_pd(v_7, mask_BLP));
                s_buffer[(j * 4)] = q_0;
                s_buffer[((j * 4) + 1)] = q_1;
                s_buffer[((j * 4) + 2)] = q_2;
                s_buffer[((j * 4) + 3)] = q_3;
                q_0 = _mm256_sub_pd(s_0, q_0);
                q_1 = _mm256_sub_pd(s_1, q_1);
                q_2 = _mm256_sub_pd(s_2, q_2);
                q_3 = _mm256_sub_pd(s_3, q_3);
                v_4 = _mm256_add_pd(v_4, q_0);
                v_5 = _mm256_add_pd(v_5, q_1);
                v_6 = _mm256_add_pd(v_6, q_2);
                v_7 = _mm256_add_pd(v_7, q_3);
              }
              s_buffer[(j * 4)] = _mm256_add_pd(s_buffer[(j * 4)], _mm256_or_pd(v_4, mask_BLP));
              s_buffer[((j * 4) + 1)] = _mm256_add_pd(s_buffer[((j * 4) + 1)], _mm256_or_pd(v_5, mask_BLP));
              s_buffer[((j * 4) + 2)] = _mm256_add_pd(s_buffer[((j * 4) + 2)], _mm256_or_pd(v_6, mask_BLP));
              s_buffer[((j * 4) + 3)] = _mm256_add_pd(s_buffer[((j * 4) + 3)], _mm256_or_pd(v_7, mask_BLP));
            }
            if(i + 4 <= n){
              v_0 = _mm256_set_pd(v_base[((incv * 2) + 1)], v_base[(incv * 2)], v_base[1], v_base[0]);
              v_1 = _mm256_set_pd(v_base[((incv * 6) + 1)], v_base[(incv * 6)], v_base[((incv * 4) + 1)], v_base[(incv * 4)]);
              y_0 = _mm256_loadu_pd(y_base);
              y_1 = _mm256_loadu_pd(y_base + 4);
              v_2 = _mm256_xor_pd(_mm256_mul_pd(_mm256_permute_pd(v_0, 0b0101), _mm256_permute_pd(y_0, 0b1111)), mask_NCONJ);
              v_3 = _mm256_xor_pd(_mm256_mul_pd(_mm256_permute_pd(v_1, 0b0101), _mm256_permute_pd(y_1, 0b1111)), mask_NCONJ);
              v_0 = _mm256_mul_pd(v_0, _mm256_permute_pd(y_0, 0b0000));
              v_1 = _mm256_mul_pd(v_1, _mm256_permute_pd(y_1, 0b0000));
              for(j = 0; j < fold - 1; j++){
                s_0 = s_buffer[(j * 4)];
                s_1 = s_buffer[((j * 4) + 1)];
                s_2 = s_buffer[((j * 4) + 2)];
                s_3 = s_buffer[((j * 4) + 3)];
                q_0 = _mm256_add_pd(s_0, _mm256_or_pd(v_0, mask_BLP));
                q_1 = _mm256_add_pd(s_1, _mm256_or_pd(v_1, mask_BLP));
                q_2 = _mm256_add_pd(s_2, _mm256_or_pd(v_2, mask_BLP));
                q_3 = _mm256_add_pd(s_3, _mm256_or_pd(v_3, mask_BLP));
                s_buffer[(j * 4)] = q_0;
                s_buffer[((j * 4) + 1)] = q_1;
                s_buffer[((j * 4) + 2)] = q_2;
                s_buffer[((j * 4) + 3)] = q_3;
                q_0 = _mm256_sub_pd(s_0, q_0);
                q_1 = _mm256_sub_pd(s_1, q_1);
                q_2 = _mm256_sub_pd(s_2, q_2);
                q_3 = _mm256_sub_pd(s_3, q_3);
                v_0 = _mm256_add_pd(v_0, q_0);
                v_1 = _mm256_add_pd(v_1, q_1);
                v_2 = _mm256_add_pd(v_2, q_2);
                v_3 = _mm256_add_pd(v_3, q_3);
              }
              s_buffer[(j * 4)] = _mm256_add_pd(s_buffer[(j * 4)], _mm256_or_pd(v_0, mask_BLP));
              s_buffer[((j * 4) + 1)] = _mm256_add_pd(s_buffer[((j * 4) + 1)], _mm256_or_pd(v_1, mask_BLP));
              s_buffer[((j * 4) + 2)] = _mm256_add_pd(s_buffer[((j * 4) + 2)], _mm256_or_pd(v_2, mask_BLP));
              s_buffer[((j * 4) + 3)] = _mm256_add_pd(s_buffer[((j * 4) + 3)], _mm256_or_pd(v_3, mask_BLP));
              i += 4, v_base += (incv * 8), y_base += 8;
            }
            if(i + 2 <= n){
              v_0 = _mm256_set_pd(v_base[((incv * 2) + 1)], v_base[(incv * 2)], v_base[1], v_base[0]);
              y_0 = _mm256_loadu_pd(y_base);
              v_1 = _mm256_xor_pd(_mm256_mul_pd(_mm256_permute_pd(v_0, 0b0101), _mm256_permute_pd(y_0, 0b1111)), mask_NCONJ);
              v_0 = _mm256_mul_pd(v_0, _mm256_permute_pd(y_0, 0b0000));
              for(j = 0; j < fold - 1; j++){
                s_0 = s_buffer[(j * 4)];
                s_1 = s_buffer[((j * 4) + 1)];
                q_0 = _mm256_add_pd(s_0, _mm256_or_pd(v_0, mask_BLP));
                q_1 = _mm256_add_pd(s_1, _mm256_or_pd(v_1, mask_BLP));
                s_buffer[(j * 4)] = q_0;
                s_buffer[((j * 4) + 1)] = q_1;
                q_0 = _mm256_sub_pd(s_0, q_0);
                q_1 = _mm256_sub_pd(s_1, q_1);
                v_0 = _mm256_add_pd(v_0, q_0);
                v_1 = _mm256_add_pd(v_1, q_1);
              }
              s_buffer[(j * 4)] = _mm256_add_pd(s_buffer[(j * 4)], _mm256_or_pd(v_0, mask_BLP));
              s_buffer[((j * 4) + 1)] = _mm256_add_pd(s_buffer[((j * 4) + 1)], _mm256_or_pd(v_1, mask_BLP));
              i += 2, v_base += (incv * 4), y_base += 4;
            }
            if(i < n){
              v_0 = _mm256_set_pd(0, 0, v_base[1], v_base[0]);
              y_0 = _mm256_set_pd(0, 0, y_base[1], y_base[0]);
              v_1 = _mm256_xor_pd(_mm256_mul_pd(_mm256_permute_pd(v_0, 0b0101), _mm256_permute_pd(y_0, 0b1111)), mask_NCONJ);
              v_0 = _mm256_mul_pd(v_0, _mm256_permute_pd(y_0, 0b0000));
              for(j = 0; j < fold - 1; j++){
                s_0 = s_buffer[(j * 4)];
                s_1 = s_buffer[((j * 4) + 1)];
                q_0 = _mm256_add_pd(s_0, _mm256_or_pd(v_0, mask_BLP));
                q_1 = _mm256_add_pd(s_1, _mm256_or_pd(v_1, mask_BLP));
                s_buffer[(j * 4)] = q_0;
                s_buffer[((j * 4) + 1)] = q_1;
                q_0 = _mm256_sub_pd(s_0, q_0);
                q_1 = _mm256_sub_pd(s_1, q_1);
                v_0 = _mm256_add_pd(v_0, q_0);
                v_1 = _mm256_add_pd(v_1, q_1);
              }
              s_buffer[(j * 4)] = _mm256_add_pd(s_buffer[(j * 4)], _mm256_or_pd(v_0, mask_BLP));
              s_buffer[((j * 4) + 1)] = _mm256_add_pd(s_buffer[((j * 4) + 1)], _mm256_or_pd(v_1, mask_BLP));
            }
          }else{

            for(i = 0; i + 8 <= n; i += 8, v_base += (incv * 16), y_base += (incy * 16)){
              v_0 = _mm256_set_pd(v_base[((incv * 2) + 1)], v_base[(incv * 2)], v_base[1], v_base[0]);
              v_1 = _mm256_set_pd(v_base[((incv * 6) + 1)], v_base[(incv * 6)], v_base[((incv * 4) + 1)], v_base[(incv * 4)]);
              v_2 = _mm256_set_pd(v_base[((incv * 10) + 1)], v_base[(incv * 10)], v_base[((incv * 8) + 1)], v_base[(incv * 8)]);
              v_3 = _mm256_set_pd(v_base[((incv * 14) + 1)], v_base[(incv * 14)], v_base[((incv * 12) + 1)], v_base[(incv * 12)]);
              y_0 = _mm256_set_pd(y_base[((incy * 2) + 1)], y_base[(incy * 2)], y_base[1], y_base[0]);
              y_1 = _mm256_set_pd(y_base[((incy * 6) + 1)], y_base[(incy * 6)], y_base[((incy * 4) + 1)], y_base[(incy * 4)]);
              y_2 = _mm256_set_pd(y_base[((incy * 10) + 1)], y_base[(incy * 10)], y_base[((incy * 8) + 1)], y_base[(incy * 8)]);
              y_3 = _mm256_set_pd(y_base[((incy * 14) + 1)], y_base[(incy * 14)], y_base[((incy * 12) + 1)], y_base[(incy * 12)]);
              v_4 = _mm256_xor_pd(_mm256_mul_pd(_mm256_permute_pd(v_0, 0b0101), _mm256_permute_pd(y_0, 0b1111)), mask_NCONJ);
              v_5 = _mm256_xor_pd(_mm256_mul_pd(_mm256_permute_pd(v_1, 0b0101), _mm256_permute_pd(y_1, 0b1111)), mask_NCONJ);
              v_6 = _mm256_xor_pd(_mm256_mul_pd(_mm256_permute_pd(v_2, 0b0101), _mm256_permute_pd(y_2, 0b1111)), mask_NCONJ);
              v_7 = _mm256_xor_pd(_mm256_mul_pd(_mm256_permute_pd(v_3, 0b0101), _mm256_permute_pd(y_3, 0b1111)), mask_NCONJ);
              v_0 = _mm256_mul_pd(v_0, _mm256_permute_pd(y_0, 0b0000));
              v_1 = _mm256_mul_pd(v_1, _mm256_permute_pd(y_1, 0b0000));
              v_2 = _mm256_mul_pd(v_2, _mm256_permute_pd(y_2, 0b0000));
              v_3 = _mm256_mul_pd(v_3, _mm256_permute_pd(y_3, 0b0000));
              for(j = 0; j < fold - 1; j++){
                s_0 = s_buffer[(j * 4)];
                s_1 = s_buffer[((j * 4) + 1)];
                s_2 = s_buffer[((j * 4) + 2)];
                s_3 = s_buffer[((j * 4) + 3)];
                q_0 = _mm256_add_pd(s_0, _mm256_or_pd(v_0, mask_BLP));
                q_1 = _mm256_add_pd(s_1, _mm256_or_pd(v_1, mask_BLP));
                q_2 = _mm256_add_pd(s_2, _mm256_or_pd(v_2, mask_BLP));
                q_3 = _mm256_add_pd(s_3, _mm256_or_pd(v_3, mask_BLP));
                s_buffer[(j * 4)] = q_0;
                s_buffer[((j * 4) + 1)] = q_1;
                s_buffer[((j * 4) + 2)] = q_2;
                s_buffer[((j * 4) + 3)] = q_3;
                q_0 = _mm256_sub_pd(s_0, q_0);
                q_1 = _mm256_sub_pd(s_1, q_1);
                q_2 = _mm256_sub_pd(s_2, q_2);
                q_3 = _mm256_sub_pd(s_3, q_3);
                v_0 = _mm256_add_pd(v_0, q_0);
                v_1 = _mm256_add_pd(v_1, q_1);
                v_2 = _mm256_add_pd(v_2, q_2);
                v_3 = _mm256_add_pd(v_3, q_3);
                s_0 = s_buffer[(j * 4)];
                s_1 = s_buffer[((j * 4) + 1)];
                s_2 = s_buffer[((j * 4) + 2)];
                s_3 = s_buffer[((j * 4) + 3)];
                q_0 = _mm256_add_pd(s_0, _mm256_or_pd(v_4, mask_BLP));
                q_1 = _mm256_add_pd(s_1, _mm256_or_pd(v_5, mask_BLP));
                q_2 = _mm256_add_pd(s_2, _mm256_or_pd(v_6, mask_BLP));
                q_3 = _mm256_add_pd(s_3, _mm256_or_pd(v_7, mask_BLP));
                s_buffer[(j * 4)] = q_0;
                s_buffer[((j * 4) + 1)] = q_1;
                s_buffer[((j * 4) + 2)] = q_2;
                s_buffer[((j * 4) + 3)] = q_3;
                q_0 = _mm256_sub_pd(s_0, q_0);
                q_1 = _mm256_sub_pd(s_1, q_1);
                q_2 = _mm256_sub_pd(s_2, q_2);
                q_3 = _mm256_sub_pd(s_3, q_3);
                v_4 = _mm256_add_pd(v_4, q_0);
                v_5 = _mm256_add_pd(v_5, q_1);
                v_6 = _mm256_add_pd(v_6, q_2);
                v_7 = _mm256_add_pd(v_7, q_3);
              }
              s_buffer[(j * 4)] = _mm256_add_pd(s_buffer[(j * 4)], _mm256_or_pd(v_4, mask_BLP));
              s_buffer[((j * 4) + 1)] = _mm256_add_pd(s_buffer[((j * 4) + 1)], _mm256_or_pd(v_5, mask_BLP));
              s_buffer[((j * 4) + 2)] = _mm256_add_pd(s_buffer[((j * 4) + 2)], _mm256_or_pd(v_6, mask_BLP));
              s_buffer[((j * 4) + 3)] = _mm256_add_pd(s_buffer[((j * 4) + 3)], _mm256_or_pd(v_7, mask_BLP));
            }
            if(i + 4 <= n){
              v_0 = _mm256_set_pd(v_base[((incv * 2) + 1)], v_base[(incv * 2)], v_base[1], v_base[0]);
              v_1 = _mm256_set_pd(v_base[((incv * 6) + 1)], v_base[(incv * 6)], v_base[((incv * 4) + 1)], v_base[(incv * 4)]);
              y_0 = _mm256_set_pd(y_base[((incy * 2) + 1)], y_base[(incy * 2)], y_base[1], y_base[0]);
              y_1 = _mm256_set_pd(y_base[((incy * 6) + 1)], y_base[(incy * 6)], y_base[((incy * 4) + 1)], y_base[(incy * 4)]);
              v_2 = _mm256_xor_pd(_mm256_mul_pd(_mm256_permute_pd(v_0, 0b0101), _mm256_permute_pd(y_0, 0b1111)), mask_NCONJ);
              v_3 = _mm256_xor_pd(_mm256_mul_pd(_mm256_permute_pd(v_1, 0b0101), _mm256_permute_pd(y_1, 0b1111)), mask_NCONJ);
              v_0 = _mm256_mul_pd(v_0, _mm256_permute_pd(y_0, 0b0000));
              v_1 = _mm256_mul_pd(v_1, _mm256_permute_pd(y_1, 0b0000));
              for(j = 0; j < fold - 1; j++){
                s_0 = s_buffer[(j * 4)];
                s_1 = s_buffer[((j * 4) + 1)];
                s_2 = s_buffer[((j * 4) + 2)];
                s_3 = s_buffer[((j * 4) + 3)];
                q_0 = _mm256_add_pd(s_0, _mm256_or_pd(v_0, mask_BLP));
                q_1 = _mm256_add_pd(s_1, _mm256_or_pd(v_1, mask_BLP));
                q_2 = _mm256_add_pd(s_2, _mm256_or_pd(v_2, mask_BLP));
                q_3 = _mm256_add_pd(s_3, _mm256_or_pd(v_3, mask_BLP));
                s_buffer[(j * 4)] = q_0;
                s_buffer[((j * 4) + 1)] = q_1;
                s_buffer[((j * 4) + 2)] = q_2;
                s_buffer[((j * 4) + 3)] = q_3;
                q_0 = _mm256_sub_pd(s_0, q_0);
                q_1 = _mm256_sub_pd(s_1, q_1);
                q_2 = _mm256_sub_pd(s_2, q_2);
                q_3 = _mm256_sub_pd(s_3, q_3);
                v_0 = _mm256_add_pd(v_0, q_0);
                v_1 = _mm256_add_pd(v_1, q_1);
                v_2 = _mm256_add_pd(v_2, q_2);
                v_3 = _mm256_add_pd(v_3, q_3);
              }
              s_buffer[(j * 4)] = _mm256_add_pd(s_buffer[(j * 4)], _mm256_or_pd(v_0, mask_BLP));
              s_buffer[((j * 4) + 1)] = _mm256_add_pd(s_buffer[((j * 4) + 1)], _mm256_or_pd(v_1, mask_BLP));
              s_buffer[((j * 4) + 2)] = _mm256_add_pd(s_buffer[((j * 4) + 2)], _mm256_or_pd(v_2, mask_BLP));
              s_buffer[((j * 4) + 3)] = _mm256_add_pd(s_buffer[((j * 4) + 3)], _mm256_or_pd(v_3, mask_BLP));
              i += 4, v_base += (incv * 8), y_base += (incy * 8);
            }
            if(i + 2 <= n){
              v_0 = _mm256_set_pd(v_base[((incv * 2) + 1)], v_base[(incv * 2)], v_base[1], v_base[0]);
              y_0 = _mm256_set_pd(y_base[((incy * 2) + 1)], y_base[(incy * 2)], y_base[1], y_base[0]);
              v_1 = _mm256_xor_pd(_mm256_mul_pd(_mm256_permute_pd(v_0, 0b0101), _mm256_permute_pd(y_0, 0b1111)), mask_NCONJ);
              v_0 = _mm256_mul_pd(v_0, _mm256_permute_pd(y_0, 0b0000));
              for(j = 0; j < fold - 1; j++){
                s_0 = s_buffer[(j * 4)];
                s_1 = s_buffer[((j * 4) + 1)];
                q_0 = _mm256_add_pd(s_0, _mm256_or_pd(v_0, mask_BLP));
                q_1 = _mm256_add_pd(s_1, _mm256_or_pd(v_1, mask_BLP));
                s_buffer[(j * 4)] = q_0;
                s_buffer[((j * 4) + 1)] = q_1;
                q_0 = _mm256_sub_pd(s_0, q_0);
                q_1 = _mm256_sub_pd(s_1, q_1);
                v_0 = _mm256_add_pd(v_0, q_0);
                v_1 = _mm256_add_pd(v_1, q_1);
              }
              s_buffer[(j * 4)] = _mm256_add_pd(s_buffer[(j * 4)], _mm256_or_pd(v_0, mask_BLP));
              s_buffer[((j * 4) + 1)] = _mm256_add_pd(s_buffer[((j * 4) + 1)], _mm256_or_pd(v_1, mask_BLP));
              i += 2, v_base += (incv * 4), y_base += (incy * 4);
            }
            if(i < n){
              v_0 = _mm256_set_pd(0, 0, v_base[1], v_base[0]);
              y_0 = _mm256_set_pd(0, 0, y_base[1], y_base[0]);
              v_1 = _mm256_xor_pd(_mm256_mul_pd(_mm256_permute_pd(v_0, 0b0101), _mm256_permute_pd(y_0, 0b1111)), mask_NCONJ);
              v_0 = _mm256_mul_pd(v_0, _mm256_permute_pd(y_0, 0b0000));
              for(j = 0; j < fold - 1; j++){
                s_0 = s_buffer[(j * 4)];
                s_1 = s_buffer[((j * 4) + 1)];
                q_0 = _mm256_add_pd(s_0, _mm256_or_pd(v_0, mask_BLP));
                q_1 = _mm256_add_pd(s_1, _mm256_or_pd(v_1, mask_BLP));
                s_buffer[(j * 4)] = q_0;
                s_buffer[((j * 4) + 1)] = q_1;
                q_0 = _mm256_sub_pd(s_0, q_0);
                q_1 = _mm256_sub_pd(s_1, q_1);
                v_0 = _mm256_add_pd(v_0, q_0);
                v_1 = _mm256_add_pd(v_1, q_1);
              }
              s_buffer[(j * 4)] = _mm256_add_pd(s_buffer[(j * 4)], _mm256_or_pd(v_0, mask_BLP));
              s_buffer[((j * 4) + 1)] = _mm256_add_pd(s_buffer[((j * 4) + 1)], _mm256_or_pd(v_1, mask_BLP));
            }
          }
        }
        for(j = 0; j < fold; j += 1){
          s_buffer[(j * 4)] = _mm256_sub_pd(s_buffer[(j * 4)], _mm256_set_pd(sum_base[((j * 2) + 1)], sum_base[(j * 2)], 0, 0));
          q_0 = _mm256_broadcast_pd((__m128d *)(sum_base + (j * 2)));
          s_buffer[(j * 4)] = _mm256_add_pd(s_buffer[(j * 4)], _mm256_sub_pd(s_buffer[((j * 4) + 1)], q_0));
          s_buffer[(j * 4)] = _mm256_add_pd(s_buffer[(j * 4)], _mm256_sub_pd(s_buffer[((j * 4) + 2)], q_0));
          s_buffer[(j * 4)] = _mm256_add_pd(s_buffer[(j * 4)], _mm256_sub_pd(s_buffer[((j * 4) + 3)], q_0));
          _mm256_store_pd((double*)tmp_cons, s_buffer[(j * 4)]);
          sum[j] = tmp_cons[0] + tmp_cons[1];
        }
        RESET_DAZ_FLAG
        return;
      }
    }
    //[[[end]]]
  }
#elif defined( __SSE2__ )
  void zdotuI2(int n, double complex* v, int incv, double complex* y, int incy, int fold, double complex* sum){
    /*[[[cog
    cog.out(generate.generate(dotuI2.DotUI2(dataTypes.DoubleComplex, vectorizations.SSE), args, params))
    ]]]*/
    __m128d mask_NCONJ; SSE_NCONJ_MASKD(mask_NCONJ);
    __m128d mask_BLP; SSE_BLP_MASKD(mask_BLP);
    double complex tmp_cons[1] __attribute__((aligned(16)));
    SET_DAZ_FLAG;
    switch(fold){
      case 3:{
        int i;

        double* sum_base = (double*) sum;
        double* v_base = (double*) v;
        double* y_base = (double*) y;
        __m128d v_0, v_1, v_2, v_3, v_4, v_5, v_6, v_7;
        __m128d y_0, y_1, y_2, y_3;
        __m128d q_0, q_1, q_2, q_3;
        __m128d s_0_0, s_0_1, s_0_2, s_0_3;
        __m128d s_1_0, s_1_1, s_1_2, s_1_3;
        __m128d s_2_0, s_2_1, s_2_2, s_2_3;

        s_0_0 = s_0_1 = s_0_2 = s_0_3 = _mm_loadu_pd(sum_base);
        s_1_0 = s_1_1 = s_1_2 = s_1_3 = _mm_loadu_pd(sum_base + 2);
        s_2_0 = s_2_1 = s_2_2 = s_2_3 = _mm_loadu_pd(sum_base + 4);
        if(incv == 1){
          if(incy == 1){

            for(i = 0; i + 4 <= n; i += 4, v_base += 8, y_base += 8){
              v_0 = _mm_loadu_pd(v_base);
              v_1 = _mm_loadu_pd(v_base + 2);
              v_2 = _mm_loadu_pd(v_base + 4);
              v_3 = _mm_loadu_pd(v_base + 6);
              y_0 = _mm_loadu_pd(y_base);
              y_1 = _mm_loadu_pd(y_base + 2);
              y_2 = _mm_loadu_pd(y_base + 4);
              y_3 = _mm_loadu_pd(y_base + 6);
              v_4 = _mm_xor_pd(_mm_mul_pd(_mm_shuffle_pd(v_0, v_0, 0b01), _mm_shuffle_pd(y_0, y_0, 0b11)), mask_NCONJ);
              v_5 = _mm_xor_pd(_mm_mul_pd(_mm_shuffle_pd(v_1, v_1, 0b01), _mm_shuffle_pd(y_1, y_1, 0b11)), mask_NCONJ);
              v_6 = _mm_xor_pd(_mm_mul_pd(_mm_shuffle_pd(v_2, v_2, 0b01), _mm_shuffle_pd(y_2, y_2, 0b11)), mask_NCONJ);
              v_7 = _mm_xor_pd(_mm_mul_pd(_mm_shuffle_pd(v_3, v_3, 0b01), _mm_shuffle_pd(y_3, y_3, 0b11)), mask_NCONJ);
              v_0 = _mm_mul_pd(v_0, _mm_shuffle_pd(y_0, y_0, 0b00));
              v_1 = _mm_mul_pd(v_1, _mm_shuffle_pd(y_1, y_1, 0b00));
              v_2 = _mm_mul_pd(v_2, _mm_shuffle_pd(y_2, y_2, 0b00));
              v_3 = _mm_mul_pd(v_3, _mm_shuffle_pd(y_3, y_3, 0b00));
              q_0 = s_0_0;
              q_1 = s_0_1;
              q_2 = s_0_2;
              q_3 = s_0_3;
              s_0_0 = _mm_add_pd(s_0_0, _mm_or_pd(v_0, mask_BLP));
              s_0_1 = _mm_add_pd(s_0_1, _mm_or_pd(v_1, mask_BLP));
              s_0_2 = _mm_add_pd(s_0_2, _mm_or_pd(v_2, mask_BLP));
              s_0_3 = _mm_add_pd(s_0_3, _mm_or_pd(v_3, mask_BLP));
              q_0 = _mm_sub_pd(q_0, s_0_0);
              q_1 = _mm_sub_pd(q_1, s_0_1);
              q_2 = _mm_sub_pd(q_2, s_0_2);
              q_3 = _mm_sub_pd(q_3, s_0_3);
              v_0 = _mm_add_pd(v_0, q_0);
              v_1 = _mm_add_pd(v_1, q_1);
              v_2 = _mm_add_pd(v_2, q_2);
              v_3 = _mm_add_pd(v_3, q_3);
              q_0 = s_1_0;
              q_1 = s_1_1;
              q_2 = s_1_2;
              q_3 = s_1_3;
              s_1_0 = _mm_add_pd(s_1_0, _mm_or_pd(v_0, mask_BLP));
              s_1_1 = _mm_add_pd(s_1_1, _mm_or_pd(v_1, mask_BLP));
              s_1_2 = _mm_add_pd(s_1_2, _mm_or_pd(v_2, mask_BLP));
              s_1_3 = _mm_add_pd(s_1_3, _mm_or_pd(v_3, mask_BLP));
              q_0 = _mm_sub_pd(q_0, s_1_0);
              q_1 = _mm_sub_pd(q_1, s_1_1);
              q_2 = _mm_sub_pd(q_2, s_1_2);
              q_3 = _mm_sub_pd(q_3, s_1_3);
              v_0 = _mm_add_pd(v_0, q_0);
              v_1 = _mm_add_pd(v_1, q_1);
              v_2 = _mm_add_pd(v_2, q_2);
              v_3 = _mm_add_pd(v_3, q_3);
              s_2_0 = _mm_add_pd(s_2_0, _mm_or_pd(v_0, mask_BLP));
              s_2_1 = _mm_add_pd(s_2_1, _mm_or_pd(v_1, mask_BLP));
              s_2_2 = _mm_add_pd(s_2_2, _mm_or_pd(v_2, mask_BLP));
              s_2_3 = _mm_add_pd(s_2_3, _mm_or_pd(v_3, mask_BLP));
              q_0 = s_0_0;
              q_1 = s_0_1;
              q_2 = s_0_2;
              q_3 = s_0_3;
              s_0_0 = _mm_add_pd(s_0_0, _mm_or_pd(v_4, mask_BLP));
              s_0_1 = _mm_add_pd(s_0_1, _mm_or_pd(v_5, mask_BLP));
              s_0_2 = _mm_add_pd(s_0_2, _mm_or_pd(v_6, mask_BLP));
              s_0_3 = _mm_add_pd(s_0_3, _mm_or_pd(v_7, mask_BLP));
              q_0 = _mm_sub_pd(q_0, s_0_0);
              q_1 = _mm_sub_pd(q_1, s_0_1);
              q_2 = _mm_sub_pd(q_2, s_0_2);
              q_3 = _mm_sub_pd(q_3, s_0_3);
              v_4 = _mm_add_pd(v_4, q_0);
              v_5 = _mm_add_pd(v_5, q_1);
              v_6 = _mm_add_pd(v_6, q_2);
              v_7 = _mm_add_pd(v_7, q_3);
              q_0 = s_1_0;
              q_1 = s_1_1;
              q_2 = s_1_2;
              q_3 = s_1_3;
              s_1_0 = _mm_add_pd(s_1_0, _mm_or_pd(v_4, mask_BLP));
              s_1_1 = _mm_add_pd(s_1_1, _mm_or_pd(v_5, mask_BLP));
              s_1_2 = _mm_add_pd(s_1_2, _mm_or_pd(v_6, mask_BLP));
              s_1_3 = _mm_add_pd(s_1_3, _mm_or_pd(v_7, mask_BLP));
              q_0 = _mm_sub_pd(q_0, s_1_0);
              q_1 = _mm_sub_pd(q_1, s_1_1);
              q_2 = _mm_sub_pd(q_2, s_1_2);
              q_3 = _mm_sub_pd(q_3, s_1_3);
              v_4 = _mm_add_pd(v_4, q_0);
              v_5 = _mm_add_pd(v_5, q_1);
              v_6 = _mm_add_pd(v_6, q_2);
              v_7 = _mm_add_pd(v_7, q_3);
              s_2_0 = _mm_add_pd(s_2_0, _mm_or_pd(v_4, mask_BLP));
              s_2_1 = _mm_add_pd(s_2_1, _mm_or_pd(v_5, mask_BLP));
              s_2_2 = _mm_add_pd(s_2_2, _mm_or_pd(v_6, mask_BLP));
              s_2_3 = _mm_add_pd(s_2_3, _mm_or_pd(v_7, mask_BLP));
            }
            if(i + 2 <= n){
              v_0 = _mm_loadu_pd(v_base);
              v_1 = _mm_loadu_pd(v_base + 2);
              y_0 = _mm_loadu_pd(y_base);
              y_1 = _mm_loadu_pd(y_base + 2);
              v_2 = _mm_xor_pd(_mm_mul_pd(_mm_shuffle_pd(v_0, v_0, 0b01), _mm_shuffle_pd(y_0, y_0, 0b11)), mask_NCONJ);
              v_3 = _mm_xor_pd(_mm_mul_pd(_mm_shuffle_pd(v_1, v_1, 0b01), _mm_shuffle_pd(y_1, y_1, 0b11)), mask_NCONJ);
              v_0 = _mm_mul_pd(v_0, _mm_shuffle_pd(y_0, y_0, 0b00));
              v_1 = _mm_mul_pd(v_1, _mm_shuffle_pd(y_1, y_1, 0b00));
              q_0 = s_0_0;
              q_1 = s_0_1;
              q_2 = s_0_2;
              q_3 = s_0_3;
              s_0_0 = _mm_add_pd(s_0_0, _mm_or_pd(v_0, mask_BLP));
              s_0_1 = _mm_add_pd(s_0_1, _mm_or_pd(v_1, mask_BLP));
              s_0_2 = _mm_add_pd(s_0_2, _mm_or_pd(v_2, mask_BLP));
              s_0_3 = _mm_add_pd(s_0_3, _mm_or_pd(v_3, mask_BLP));
              q_0 = _mm_sub_pd(q_0, s_0_0);
              q_1 = _mm_sub_pd(q_1, s_0_1);
              q_2 = _mm_sub_pd(q_2, s_0_2);
              q_3 = _mm_sub_pd(q_3, s_0_3);
              v_0 = _mm_add_pd(v_0, q_0);
              v_1 = _mm_add_pd(v_1, q_1);
              v_2 = _mm_add_pd(v_2, q_2);
              v_3 = _mm_add_pd(v_3, q_3);
              q_0 = s_1_0;
              q_1 = s_1_1;
              q_2 = s_1_2;
              q_3 = s_1_3;
              s_1_0 = _mm_add_pd(s_1_0, _mm_or_pd(v_0, mask_BLP));
              s_1_1 = _mm_add_pd(s_1_1, _mm_or_pd(v_1, mask_BLP));
              s_1_2 = _mm_add_pd(s_1_2, _mm_or_pd(v_2, mask_BLP));
              s_1_3 = _mm_add_pd(s_1_3, _mm_or_pd(v_3, mask_BLP));
              q_0 = _mm_sub_pd(q_0, s_1_0);
              q_1 = _mm_sub_pd(q_1, s_1_1);
              q_2 = _mm_sub_pd(q_2, s_1_2);
              q_3 = _mm_sub_pd(q_3, s_1_3);
              v_0 = _mm_add_pd(v_0, q_0);
              v_1 = _mm_add_pd(v_1, q_1);
              v_2 = _mm_add_pd(v_2, q_2);
              v_3 = _mm_add_pd(v_3, q_3);
              s_2_0 = _mm_add_pd(s_2_0, _mm_or_pd(v_0, mask_BLP));
              s_2_1 = _mm_add_pd(s_2_1, _mm_or_pd(v_1, mask_BLP));
              s_2_2 = _mm_add_pd(s_2_2, _mm_or_pd(v_2, mask_BLP));
              s_2_3 = _mm_add_pd(s_2_3, _mm_or_pd(v_3, mask_BLP));
              i += 2, v_base += 4, y_base += 4;
            }
            if(i + 1 <= n){
              v_0 = _mm_loadu_pd(v_base);
              y_0 = _mm_loadu_pd(y_base);
              v_1 = _mm_xor_pd(_mm_mul_pd(_mm_shuffle_pd(v_0, v_0, 0b01), _mm_shuffle_pd(y_0, y_0, 0b11)), mask_NCONJ);
              v_0 = _mm_mul_pd(v_0, _mm_shuffle_pd(y_0, y_0, 0b00));
              q_0 = s_0_0;
              q_1 = s_0_1;
              s_0_0 = _mm_add_pd(s_0_0, _mm_or_pd(v_0, mask_BLP));
              s_0_1 = _mm_add_pd(s_0_1, _mm_or_pd(v_1, mask_BLP));
              q_0 = _mm_sub_pd(q_0, s_0_0);
              q_1 = _mm_sub_pd(q_1, s_0_1);
              v_0 = _mm_add_pd(v_0, q_0);
              v_1 = _mm_add_pd(v_1, q_1);
              q_0 = s_1_0;
              q_1 = s_1_1;
              s_1_0 = _mm_add_pd(s_1_0, _mm_or_pd(v_0, mask_BLP));
              s_1_1 = _mm_add_pd(s_1_1, _mm_or_pd(v_1, mask_BLP));
              q_0 = _mm_sub_pd(q_0, s_1_0);
              q_1 = _mm_sub_pd(q_1, s_1_1);
              v_0 = _mm_add_pd(v_0, q_0);
              v_1 = _mm_add_pd(v_1, q_1);
              s_2_0 = _mm_add_pd(s_2_0, _mm_or_pd(v_0, mask_BLP));
              s_2_1 = _mm_add_pd(s_2_1, _mm_or_pd(v_1, mask_BLP));
              i += 1, v_base += 2, y_base += 2;
            }
          }else{

            for(i = 0; i + 4 <= n; i += 4, v_base += 8, y_base += (incy * 8)){
              v_0 = _mm_loadu_pd(v_base);
              v_1 = _mm_loadu_pd(v_base + 2);
              v_2 = _mm_loadu_pd(v_base + 4);
              v_3 = _mm_loadu_pd(v_base + 6);
              y_0 = _mm_loadu_pd(y_base);
              y_1 = _mm_loadu_pd(y_base + (incy * 2));
              y_2 = _mm_loadu_pd(y_base + (incy * 4));
              y_3 = _mm_loadu_pd(y_base + (incy * 6));
              v_4 = _mm_xor_pd(_mm_mul_pd(_mm_shuffle_pd(v_0, v_0, 0b01), _mm_shuffle_pd(y_0, y_0, 0b11)), mask_NCONJ);
              v_5 = _mm_xor_pd(_mm_mul_pd(_mm_shuffle_pd(v_1, v_1, 0b01), _mm_shuffle_pd(y_1, y_1, 0b11)), mask_NCONJ);
              v_6 = _mm_xor_pd(_mm_mul_pd(_mm_shuffle_pd(v_2, v_2, 0b01), _mm_shuffle_pd(y_2, y_2, 0b11)), mask_NCONJ);
              v_7 = _mm_xor_pd(_mm_mul_pd(_mm_shuffle_pd(v_3, v_3, 0b01), _mm_shuffle_pd(y_3, y_3, 0b11)), mask_NCONJ);
              v_0 = _mm_mul_pd(v_0, _mm_shuffle_pd(y_0, y_0, 0b00));
              v_1 = _mm_mul_pd(v_1, _mm_shuffle_pd(y_1, y_1, 0b00));
              v_2 = _mm_mul_pd(v_2, _mm_shuffle_pd(y_2, y_2, 0b00));
              v_3 = _mm_mul_pd(v_3, _mm_shuffle_pd(y_3, y_3, 0b00));
              q_0 = s_0_0;
              q_1 = s_0_1;
              q_2 = s_0_2;
              q_3 = s_0_3;
              s_0_0 = _mm_add_pd(s_0_0, _mm_or_pd(v_0, mask_BLP));
              s_0_1 = _mm_add_pd(s_0_1, _mm_or_pd(v_1, mask_BLP));
              s_0_2 = _mm_add_pd(s_0_2, _mm_or_pd(v_2, mask_BLP));
              s_0_3 = _mm_add_pd(s_0_3, _mm_or_pd(v_3, mask_BLP));
              q_0 = _mm_sub_pd(q_0, s_0_0);
              q_1 = _mm_sub_pd(q_1, s_0_1);
              q_2 = _mm_sub_pd(q_2, s_0_2);
              q_3 = _mm_sub_pd(q_3, s_0_3);
              v_0 = _mm_add_pd(v_0, q_0);
              v_1 = _mm_add_pd(v_1, q_1);
              v_2 = _mm_add_pd(v_2, q_2);
              v_3 = _mm_add_pd(v_3, q_3);
              q_0 = s_1_0;
              q_1 = s_1_1;
              q_2 = s_1_2;
              q_3 = s_1_3;
              s_1_0 = _mm_add_pd(s_1_0, _mm_or_pd(v_0, mask_BLP));
              s_1_1 = _mm_add_pd(s_1_1, _mm_or_pd(v_1, mask_BLP));
              s_1_2 = _mm_add_pd(s_1_2, _mm_or_pd(v_2, mask_BLP));
              s_1_3 = _mm_add_pd(s_1_3, _mm_or_pd(v_3, mask_BLP));
              q_0 = _mm_sub_pd(q_0, s_1_0);
              q_1 = _mm_sub_pd(q_1, s_1_1);
              q_2 = _mm_sub_pd(q_2, s_1_2);
              q_3 = _mm_sub_pd(q_3, s_1_3);
              v_0 = _mm_add_pd(v_0, q_0);
              v_1 = _mm_add_pd(v_1, q_1);
              v_2 = _mm_add_pd(v_2, q_2);
              v_3 = _mm_add_pd(v_3, q_3);
              s_2_0 = _mm_add_pd(s_2_0, _mm_or_pd(v_0, mask_BLP));
              s_2_1 = _mm_add_pd(s_2_1, _mm_or_pd(v_1, mask_BLP));
              s_2_2 = _mm_add_pd(s_2_2, _mm_or_pd(v_2, mask_BLP));
              s_2_3 = _mm_add_pd(s_2_3, _mm_or_pd(v_3, mask_BLP));
              q_0 = s_0_0;
              q_1 = s_0_1;
              q_2 = s_0_2;
              q_3 = s_0_3;
              s_0_0 = _mm_add_pd(s_0_0, _mm_or_pd(v_4, mask_BLP));
              s_0_1 = _mm_add_pd(s_0_1, _mm_or_pd(v_5, mask_BLP));
              s_0_2 = _mm_add_pd(s_0_2, _mm_or_pd(v_6, mask_BLP));
              s_0_3 = _mm_add_pd(s_0_3, _mm_or_pd(v_7, mask_BLP));
              q_0 = _mm_sub_pd(q_0, s_0_0);
              q_1 = _mm_sub_pd(q_1, s_0_1);
              q_2 = _mm_sub_pd(q_2, s_0_2);
              q_3 = _mm_sub_pd(q_3, s_0_3);
              v_4 = _mm_add_pd(v_4, q_0);
              v_5 = _mm_add_pd(v_5, q_1);
              v_6 = _mm_add_pd(v_6, q_2);
              v_7 = _mm_add_pd(v_7, q_3);
              q_0 = s_1_0;
              q_1 = s_1_1;
              q_2 = s_1_2;
              q_3 = s_1_3;
              s_1_0 = _mm_add_pd(s_1_0, _mm_or_pd(v_4, mask_BLP));
              s_1_1 = _mm_add_pd(s_1_1, _mm_or_pd(v_5, mask_BLP));
              s_1_2 = _mm_add_pd(s_1_2, _mm_or_pd(v_6, mask_BLP));
              s_1_3 = _mm_add_pd(s_1_3, _mm_or_pd(v_7, mask_BLP));
              q_0 = _mm_sub_pd(q_0, s_1_0);
              q_1 = _mm_sub_pd(q_1, s_1_1);
              q_2 = _mm_sub_pd(q_2, s_1_2);
              q_3 = _mm_sub_pd(q_3, s_1_3);
              v_4 = _mm_add_pd(v_4, q_0);
              v_5 = _mm_add_pd(v_5, q_1);
              v_6 = _mm_add_pd(v_6, q_2);
              v_7 = _mm_add_pd(v_7, q_3);
              s_2_0 = _mm_add_pd(s_2_0, _mm_or_pd(v_4, mask_BLP));
              s_2_1 = _mm_add_pd(s_2_1, _mm_or_pd(v_5, mask_BLP));
              s_2_2 = _mm_add_pd(s_2_2, _mm_or_pd(v_6, mask_BLP));
              s_2_3 = _mm_add_pd(s_2_3, _mm_or_pd(v_7, mask_BLP));
            }
            if(i + 2 <= n){
              v_0 = _mm_loadu_pd(v_base);
              v_1 = _mm_loadu_pd(v_base + 2);
              y_0 = _mm_loadu_pd(y_base);
              y_1 = _mm_loadu_pd(y_base + (incy * 2));
              v_2 = _mm_xor_pd(_mm_mul_pd(_mm_shuffle_pd(v_0, v_0, 0b01), _mm_shuffle_pd(y_0, y_0, 0b11)), mask_NCONJ);
              v_3 = _mm_xor_pd(_mm_mul_pd(_mm_shuffle_pd(v_1, v_1, 0b01), _mm_shuffle_pd(y_1, y_1, 0b11)), mask_NCONJ);
              v_0 = _mm_mul_pd(v_0, _mm_shuffle_pd(y_0, y_0, 0b00));
              v_1 = _mm_mul_pd(v_1, _mm_shuffle_pd(y_1, y_1, 0b00));
              q_0 = s_0_0;
              q_1 = s_0_1;
              q_2 = s_0_2;
              q_3 = s_0_3;
              s_0_0 = _mm_add_pd(s_0_0, _mm_or_pd(v_0, mask_BLP));
              s_0_1 = _mm_add_pd(s_0_1, _mm_or_pd(v_1, mask_BLP));
              s_0_2 = _mm_add_pd(s_0_2, _mm_or_pd(v_2, mask_BLP));
              s_0_3 = _mm_add_pd(s_0_3, _mm_or_pd(v_3, mask_BLP));
              q_0 = _mm_sub_pd(q_0, s_0_0);
              q_1 = _mm_sub_pd(q_1, s_0_1);
              q_2 = _mm_sub_pd(q_2, s_0_2);
              q_3 = _mm_sub_pd(q_3, s_0_3);
              v_0 = _mm_add_pd(v_0, q_0);
              v_1 = _mm_add_pd(v_1, q_1);
              v_2 = _mm_add_pd(v_2, q_2);
              v_3 = _mm_add_pd(v_3, q_3);
              q_0 = s_1_0;
              q_1 = s_1_1;
              q_2 = s_1_2;
              q_3 = s_1_3;
              s_1_0 = _mm_add_pd(s_1_0, _mm_or_pd(v_0, mask_BLP));
              s_1_1 = _mm_add_pd(s_1_1, _mm_or_pd(v_1, mask_BLP));
              s_1_2 = _mm_add_pd(s_1_2, _mm_or_pd(v_2, mask_BLP));
              s_1_3 = _mm_add_pd(s_1_3, _mm_or_pd(v_3, mask_BLP));
              q_0 = _mm_sub_pd(q_0, s_1_0);
              q_1 = _mm_sub_pd(q_1, s_1_1);
              q_2 = _mm_sub_pd(q_2, s_1_2);
              q_3 = _mm_sub_pd(q_3, s_1_3);
              v_0 = _mm_add_pd(v_0, q_0);
              v_1 = _mm_add_pd(v_1, q_1);
              v_2 = _mm_add_pd(v_2, q_2);
              v_3 = _mm_add_pd(v_3, q_3);
              s_2_0 = _mm_add_pd(s_2_0, _mm_or_pd(v_0, mask_BLP));
              s_2_1 = _mm_add_pd(s_2_1, _mm_or_pd(v_1, mask_BLP));
              s_2_2 = _mm_add_pd(s_2_2, _mm_or_pd(v_2, mask_BLP));
              s_2_3 = _mm_add_pd(s_2_3, _mm_or_pd(v_3, mask_BLP));
              i += 2, v_base += 4, y_base += (incy * 4);
            }
            if(i + 1 <= n){
              v_0 = _mm_loadu_pd(v_base);
              y_0 = _mm_loadu_pd(y_base);
              v_1 = _mm_xor_pd(_mm_mul_pd(_mm_shuffle_pd(v_0, v_0, 0b01), _mm_shuffle_pd(y_0, y_0, 0b11)), mask_NCONJ);
              v_0 = _mm_mul_pd(v_0, _mm_shuffle_pd(y_0, y_0, 0b00));
              q_0 = s_0_0;
              q_1 = s_0_1;
              s_0_0 = _mm_add_pd(s_0_0, _mm_or_pd(v_0, mask_BLP));
              s_0_1 = _mm_add_pd(s_0_1, _mm_or_pd(v_1, mask_BLP));
              q_0 = _mm_sub_pd(q_0, s_0_0);
              q_1 = _mm_sub_pd(q_1, s_0_1);
              v_0 = _mm_add_pd(v_0, q_0);
              v_1 = _mm_add_pd(v_1, q_1);
              q_0 = s_1_0;
              q_1 = s_1_1;
              s_1_0 = _mm_add_pd(s_1_0, _mm_or_pd(v_0, mask_BLP));
              s_1_1 = _mm_add_pd(s_1_1, _mm_or_pd(v_1, mask_BLP));
              q_0 = _mm_sub_pd(q_0, s_1_0);
              q_1 = _mm_sub_pd(q_1, s_1_1);
              v_0 = _mm_add_pd(v_0, q_0);
              v_1 = _mm_add_pd(v_1, q_1);
              s_2_0 = _mm_add_pd(s_2_0, _mm_or_pd(v_0, mask_BLP));
              s_2_1 = _mm_add_pd(s_2_1, _mm_or_pd(v_1, mask_BLP));
              i += 1, v_base += 2, y_base += (incy * 2);
            }
          }
        }else{
          if(incy == 1){

            for(i = 0; i + 4 <= n; i += 4, v_base += (incv * 8), y_base += 8){
              v_0 = _mm_loadu_pd(v_base);
              v_1 = _mm_loadu_pd(v_base + (incv * 2));
              v_2 = _mm_loadu_pd(v_base + (incv * 4));
              v_3 = _mm_loadu_pd(v_base + (incv * 6));
              y_0 = _mm_loadu_pd(y_base);
              y_1 = _mm_loadu_pd(y_base + 2);
              y_2 = _mm_loadu_pd(y_base + 4);
              y_3 = _mm_loadu_pd(y_base + 6);
              v_4 = _mm_xor_pd(_mm_mul_pd(_mm_shuffle_pd(v_0, v_0, 0b01), _mm_shuffle_pd(y_0, y_0, 0b11)), mask_NCONJ);
              v_5 = _mm_xor_pd(_mm_mul_pd(_mm_shuffle_pd(v_1, v_1, 0b01), _mm_shuffle_pd(y_1, y_1, 0b11)), mask_NCONJ);
              v_6 = _mm_xor_pd(_mm_mul_pd(_mm_shuffle_pd(v_2, v_2, 0b01), _mm_shuffle_pd(y_2, y_2, 0b11)), mask_NCONJ);
              v_7 = _mm_xor_pd(_mm_mul_pd(_mm_shuffle_pd(v_3, v_3, 0b01), _mm_shuffle_pd(y_3, y_3, 0b11)), mask_NCONJ);
              v_0 = _mm_mul_pd(v_0, _mm_shuffle_pd(y_0, y_0, 0b00));
              v_1 = _mm_mul_pd(v_1, _mm_shuffle_pd(y_1, y_1, 0b00));
              v_2 = _mm_mul_pd(v_2, _mm_shuffle_pd(y_2, y_2, 0b00));
              v_3 = _mm_mul_pd(v_3, _mm_shuffle_pd(y_3, y_3, 0b00));
              q_0 = s_0_0;
              q_1 = s_0_1;
              q_2 = s_0_2;
              q_3 = s_0_3;
              s_0_0 = _mm_add_pd(s_0_0, _mm_or_pd(v_0, mask_BLP));
              s_0_1 = _mm_add_pd(s_0_1, _mm_or_pd(v_1, mask_BLP));
              s_0_2 = _mm_add_pd(s_0_2, _mm_or_pd(v_2, mask_BLP));
              s_0_3 = _mm_add_pd(s_0_3, _mm_or_pd(v_3, mask_BLP));
              q_0 = _mm_sub_pd(q_0, s_0_0);
              q_1 = _mm_sub_pd(q_1, s_0_1);
              q_2 = _mm_sub_pd(q_2, s_0_2);
              q_3 = _mm_sub_pd(q_3, s_0_3);
              v_0 = _mm_add_pd(v_0, q_0);
              v_1 = _mm_add_pd(v_1, q_1);
              v_2 = _mm_add_pd(v_2, q_2);
              v_3 = _mm_add_pd(v_3, q_3);
              q_0 = s_1_0;
              q_1 = s_1_1;
              q_2 = s_1_2;
              q_3 = s_1_3;
              s_1_0 = _mm_add_pd(s_1_0, _mm_or_pd(v_0, mask_BLP));
              s_1_1 = _mm_add_pd(s_1_1, _mm_or_pd(v_1, mask_BLP));
              s_1_2 = _mm_add_pd(s_1_2, _mm_or_pd(v_2, mask_BLP));
              s_1_3 = _mm_add_pd(s_1_3, _mm_or_pd(v_3, mask_BLP));
              q_0 = _mm_sub_pd(q_0, s_1_0);
              q_1 = _mm_sub_pd(q_1, s_1_1);
              q_2 = _mm_sub_pd(q_2, s_1_2);
              q_3 = _mm_sub_pd(q_3, s_1_3);
              v_0 = _mm_add_pd(v_0, q_0);
              v_1 = _mm_add_pd(v_1, q_1);
              v_2 = _mm_add_pd(v_2, q_2);
              v_3 = _mm_add_pd(v_3, q_3);
              s_2_0 = _mm_add_pd(s_2_0, _mm_or_pd(v_0, mask_BLP));
              s_2_1 = _mm_add_pd(s_2_1, _mm_or_pd(v_1, mask_BLP));
              s_2_2 = _mm_add_pd(s_2_2, _mm_or_pd(v_2, mask_BLP));
              s_2_3 = _mm_add_pd(s_2_3, _mm_or_pd(v_3, mask_BLP));
              q_0 = s_0_0;
              q_1 = s_0_1;
              q_2 = s_0_2;
              q_3 = s_0_3;
              s_0_0 = _mm_add_pd(s_0_0, _mm_or_pd(v_4, mask_BLP));
              s_0_1 = _mm_add_pd(s_0_1, _mm_or_pd(v_5, mask_BLP));
              s_0_2 = _mm_add_pd(s_0_2, _mm_or_pd(v_6, mask_BLP));
              s_0_3 = _mm_add_pd(s_0_3, _mm_or_pd(v_7, mask_BLP));
              q_0 = _mm_sub_pd(q_0, s_0_0);
              q_1 = _mm_sub_pd(q_1, s_0_1);
              q_2 = _mm_sub_pd(q_2, s_0_2);
              q_3 = _mm_sub_pd(q_3, s_0_3);
              v_4 = _mm_add_pd(v_4, q_0);
              v_5 = _mm_add_pd(v_5, q_1);
              v_6 = _mm_add_pd(v_6, q_2);
              v_7 = _mm_add_pd(v_7, q_3);
              q_0 = s_1_0;
              q_1 = s_1_1;
              q_2 = s_1_2;
              q_3 = s_1_3;
              s_1_0 = _mm_add_pd(s_1_0, _mm_or_pd(v_4, mask_BLP));
              s_1_1 = _mm_add_pd(s_1_1, _mm_or_pd(v_5, mask_BLP));
              s_1_2 = _mm_add_pd(s_1_2, _mm_or_pd(v_6, mask_BLP));
              s_1_3 = _mm_add_pd(s_1_3, _mm_or_pd(v_7, mask_BLP));
              q_0 = _mm_sub_pd(q_0, s_1_0);
              q_1 = _mm_sub_pd(q_1, s_1_1);
              q_2 = _mm_sub_pd(q_2, s_1_2);
              q_3 = _mm_sub_pd(q_3, s_1_3);
              v_4 = _mm_add_pd(v_4, q_0);
              v_5 = _mm_add_pd(v_5, q_1);
              v_6 = _mm_add_pd(v_6, q_2);
              v_7 = _mm_add_pd(v_7, q_3);
              s_2_0 = _mm_add_pd(s_2_0, _mm_or_pd(v_4, mask_BLP));
              s_2_1 = _mm_add_pd(s_2_1, _mm_or_pd(v_5, mask_BLP));
              s_2_2 = _mm_add_pd(s_2_2, _mm_or_pd(v_6, mask_BLP));
              s_2_3 = _mm_add_pd(s_2_3, _mm_or_pd(v_7, mask_BLP));
            }
            if(i + 2 <= n){
              v_0 = _mm_loadu_pd(v_base);
              v_1 = _mm_loadu_pd(v_base + (incv * 2));
              y_0 = _mm_loadu_pd(y_base);
              y_1 = _mm_loadu_pd(y_base + 2);
              v_2 = _mm_xor_pd(_mm_mul_pd(_mm_shuffle_pd(v_0, v_0, 0b01), _mm_shuffle_pd(y_0, y_0, 0b11)), mask_NCONJ);
              v_3 = _mm_xor_pd(_mm_mul_pd(_mm_shuffle_pd(v_1, v_1, 0b01), _mm_shuffle_pd(y_1, y_1, 0b11)), mask_NCONJ);
              v_0 = _mm_mul_pd(v_0, _mm_shuffle_pd(y_0, y_0, 0b00));
              v_1 = _mm_mul_pd(v_1, _mm_shuffle_pd(y_1, y_1, 0b00));
              q_0 = s_0_0;
              q_1 = s_0_1;
              q_2 = s_0_2;
              q_3 = s_0_3;
              s_0_0 = _mm_add_pd(s_0_0, _mm_or_pd(v_0, mask_BLP));
              s_0_1 = _mm_add_pd(s_0_1, _mm_or_pd(v_1, mask_BLP));
              s_0_2 = _mm_add_pd(s_0_2, _mm_or_pd(v_2, mask_BLP));
              s_0_3 = _mm_add_pd(s_0_3, _mm_or_pd(v_3, mask_BLP));
              q_0 = _mm_sub_pd(q_0, s_0_0);
              q_1 = _mm_sub_pd(q_1, s_0_1);
              q_2 = _mm_sub_pd(q_2, s_0_2);
              q_3 = _mm_sub_pd(q_3, s_0_3);
              v_0 = _mm_add_pd(v_0, q_0);
              v_1 = _mm_add_pd(v_1, q_1);
              v_2 = _mm_add_pd(v_2, q_2);
              v_3 = _mm_add_pd(v_3, q_3);
              q_0 = s_1_0;
              q_1 = s_1_1;
              q_2 = s_1_2;
              q_3 = s_1_3;
              s_1_0 = _mm_add_pd(s_1_0, _mm_or_pd(v_0, mask_BLP));
              s_1_1 = _mm_add_pd(s_1_1, _mm_or_pd(v_1, mask_BLP));
              s_1_2 = _mm_add_pd(s_1_2, _mm_or_pd(v_2, mask_BLP));
              s_1_3 = _mm_add_pd(s_1_3, _mm_or_pd(v_3, mask_BLP));
              q_0 = _mm_sub_pd(q_0, s_1_0);
              q_1 = _mm_sub_pd(q_1, s_1_1);
              q_2 = _mm_sub_pd(q_2, s_1_2);
              q_3 = _mm_sub_pd(q_3, s_1_3);
              v_0 = _mm_add_pd(v_0, q_0);
              v_1 = _mm_add_pd(v_1, q_1);
              v_2 = _mm_add_pd(v_2, q_2);
              v_3 = _mm_add_pd(v_3, q_3);
              s_2_0 = _mm_add_pd(s_2_0, _mm_or_pd(v_0, mask_BLP));
              s_2_1 = _mm_add_pd(s_2_1, _mm_or_pd(v_1, mask_BLP));
              s_2_2 = _mm_add_pd(s_2_2, _mm_or_pd(v_2, mask_BLP));
              s_2_3 = _mm_add_pd(s_2_3, _mm_or_pd(v_3, mask_BLP));
              i += 2, v_base += (incv * 4), y_base += 4;
            }
            if(i + 1 <= n){
              v_0 = _mm_loadu_pd(v_base);
              y_0 = _mm_loadu_pd(y_base);
              v_1 = _mm_xor_pd(_mm_mul_pd(_mm_shuffle_pd(v_0, v_0, 0b01), _mm_shuffle_pd(y_0, y_0, 0b11)), mask_NCONJ);
              v_0 = _mm_mul_pd(v_0, _mm_shuffle_pd(y_0, y_0, 0b00));
              q_0 = s_0_0;
              q_1 = s_0_1;
              s_0_0 = _mm_add_pd(s_0_0, _mm_or_pd(v_0, mask_BLP));
              s_0_1 = _mm_add_pd(s_0_1, _mm_or_pd(v_1, mask_BLP));
              q_0 = _mm_sub_pd(q_0, s_0_0);
              q_1 = _mm_sub_pd(q_1, s_0_1);
              v_0 = _mm_add_pd(v_0, q_0);
              v_1 = _mm_add_pd(v_1, q_1);
              q_0 = s_1_0;
              q_1 = s_1_1;
              s_1_0 = _mm_add_pd(s_1_0, _mm_or_pd(v_0, mask_BLP));
              s_1_1 = _mm_add_pd(s_1_1, _mm_or_pd(v_1, mask_BLP));
              q_0 = _mm_sub_pd(q_0, s_1_0);
              q_1 = _mm_sub_pd(q_1, s_1_1);
              v_0 = _mm_add_pd(v_0, q_0);
              v_1 = _mm_add_pd(v_1, q_1);
              s_2_0 = _mm_add_pd(s_2_0, _mm_or_pd(v_0, mask_BLP));
              s_2_1 = _mm_add_pd(s_2_1, _mm_or_pd(v_1, mask_BLP));
              i += 1, v_base += (incv * 2), y_base += 2;
            }
          }else{

            for(i = 0; i + 4 <= n; i += 4, v_base += (incv * 8), y_base += (incy * 8)){
              v_0 = _mm_loadu_pd(v_base);
              v_1 = _mm_loadu_pd(v_base + (incv * 2));
              v_2 = _mm_loadu_pd(v_base + (incv * 4));
              v_3 = _mm_loadu_pd(v_base + (incv * 6));
              y_0 = _mm_loadu_pd(y_base);
              y_1 = _mm_loadu_pd(y_base + (incy * 2));
              y_2 = _mm_loadu_pd(y_base + (incy * 4));
              y_3 = _mm_loadu_pd(y_base + (incy * 6));
              v_4 = _mm_xor_pd(_mm_mul_pd(_mm_shuffle_pd(v_0, v_0, 0b01), _mm_shuffle_pd(y_0, y_0, 0b11)), mask_NCONJ);
              v_5 = _mm_xor_pd(_mm_mul_pd(_mm_shuffle_pd(v_1, v_1, 0b01), _mm_shuffle_pd(y_1, y_1, 0b11)), mask_NCONJ);
              v_6 = _mm_xor_pd(_mm_mul_pd(_mm_shuffle_pd(v_2, v_2, 0b01), _mm_shuffle_pd(y_2, y_2, 0b11)), mask_NCONJ);
              v_7 = _mm_xor_pd(_mm_mul_pd(_mm_shuffle_pd(v_3, v_3, 0b01), _mm_shuffle_pd(y_3, y_3, 0b11)), mask_NCONJ);
              v_0 = _mm_mul_pd(v_0, _mm_shuffle_pd(y_0, y_0, 0b00));
              v_1 = _mm_mul_pd(v_1, _mm_shuffle_pd(y_1, y_1, 0b00));
              v_2 = _mm_mul_pd(v_2, _mm_shuffle_pd(y_2, y_2, 0b00));
              v_3 = _mm_mul_pd(v_3, _mm_shuffle_pd(y_3, y_3, 0b00));
              q_0 = s_0_0;
              q_1 = s_0_1;
              q_2 = s_0_2;
              q_3 = s_0_3;
              s_0_0 = _mm_add_pd(s_0_0, _mm_or_pd(v_0, mask_BLP));
              s_0_1 = _mm_add_pd(s_0_1, _mm_or_pd(v_1, mask_BLP));
              s_0_2 = _mm_add_pd(s_0_2, _mm_or_pd(v_2, mask_BLP));
              s_0_3 = _mm_add_pd(s_0_3, _mm_or_pd(v_3, mask_BLP));
              q_0 = _mm_sub_pd(q_0, s_0_0);
              q_1 = _mm_sub_pd(q_1, s_0_1);
              q_2 = _mm_sub_pd(q_2, s_0_2);
              q_3 = _mm_sub_pd(q_3, s_0_3);
              v_0 = _mm_add_pd(v_0, q_0);
              v_1 = _mm_add_pd(v_1, q_1);
              v_2 = _mm_add_pd(v_2, q_2);
              v_3 = _mm_add_pd(v_3, q_3);
              q_0 = s_1_0;
              q_1 = s_1_1;
              q_2 = s_1_2;
              q_3 = s_1_3;
              s_1_0 = _mm_add_pd(s_1_0, _mm_or_pd(v_0, mask_BLP));
              s_1_1 = _mm_add_pd(s_1_1, _mm_or_pd(v_1, mask_BLP));
              s_1_2 = _mm_add_pd(s_1_2, _mm_or_pd(v_2, mask_BLP));
              s_1_3 = _mm_add_pd(s_1_3, _mm_or_pd(v_3, mask_BLP));
              q_0 = _mm_sub_pd(q_0, s_1_0);
              q_1 = _mm_sub_pd(q_1, s_1_1);
              q_2 = _mm_sub_pd(q_2, s_1_2);
              q_3 = _mm_sub_pd(q_3, s_1_3);
              v_0 = _mm_add_pd(v_0, q_0);
              v_1 = _mm_add_pd(v_1, q_1);
              v_2 = _mm_add_pd(v_2, q_2);
              v_3 = _mm_add_pd(v_3, q_3);
              s_2_0 = _mm_add_pd(s_2_0, _mm_or_pd(v_0, mask_BLP));
              s_2_1 = _mm_add_pd(s_2_1, _mm_or_pd(v_1, mask_BLP));
              s_2_2 = _mm_add_pd(s_2_2, _mm_or_pd(v_2, mask_BLP));
              s_2_3 = _mm_add_pd(s_2_3, _mm_or_pd(v_3, mask_BLP));
              q_0 = s_0_0;
              q_1 = s_0_1;
              q_2 = s_0_2;
              q_3 = s_0_3;
              s_0_0 = _mm_add_pd(s_0_0, _mm_or_pd(v_4, mask_BLP));
              s_0_1 = _mm_add_pd(s_0_1, _mm_or_pd(v_5, mask_BLP));
              s_0_2 = _mm_add_pd(s_0_2, _mm_or_pd(v_6, mask_BLP));
              s_0_3 = _mm_add_pd(s_0_3, _mm_or_pd(v_7, mask_BLP));
              q_0 = _mm_sub_pd(q_0, s_0_0);
              q_1 = _mm_sub_pd(q_1, s_0_1);
              q_2 = _mm_sub_pd(q_2, s_0_2);
              q_3 = _mm_sub_pd(q_3, s_0_3);
              v_4 = _mm_add_pd(v_4, q_0);
              v_5 = _mm_add_pd(v_5, q_1);
              v_6 = _mm_add_pd(v_6, q_2);
              v_7 = _mm_add_pd(v_7, q_3);
              q_0 = s_1_0;
              q_1 = s_1_1;
              q_2 = s_1_2;
              q_3 = s_1_3;
              s_1_0 = _mm_add_pd(s_1_0, _mm_or_pd(v_4, mask_BLP));
              s_1_1 = _mm_add_pd(s_1_1, _mm_or_pd(v_5, mask_BLP));
              s_1_2 = _mm_add_pd(s_1_2, _mm_or_pd(v_6, mask_BLP));
              s_1_3 = _mm_add_pd(s_1_3, _mm_or_pd(v_7, mask_BLP));
              q_0 = _mm_sub_pd(q_0, s_1_0);
              q_1 = _mm_sub_pd(q_1, s_1_1);
              q_2 = _mm_sub_pd(q_2, s_1_2);
              q_3 = _mm_sub_pd(q_3, s_1_3);
              v_4 = _mm_add_pd(v_4, q_0);
              v_5 = _mm_add_pd(v_5, q_1);
              v_6 = _mm_add_pd(v_6, q_2);
              v_7 = _mm_add_pd(v_7, q_3);
              s_2_0 = _mm_add_pd(s_2_0, _mm_or_pd(v_4, mask_BLP));
              s_2_1 = _mm_add_pd(s_2_1, _mm_or_pd(v_5, mask_BLP));
              s_2_2 = _mm_add_pd(s_2_2, _mm_or_pd(v_6, mask_BLP));
              s_2_3 = _mm_add_pd(s_2_3, _mm_or_pd(v_7, mask_BLP));
            }
            if(i + 2 <= n){
              v_0 = _mm_loadu_pd(v_base);
              v_1 = _mm_loadu_pd(v_base + (incv * 2));
              y_0 = _mm_loadu_pd(y_base);
              y_1 = _mm_loadu_pd(y_base + (incy * 2));
              v_2 = _mm_xor_pd(_mm_mul_pd(_mm_shuffle_pd(v_0, v_0, 0b01), _mm_shuffle_pd(y_0, y_0, 0b11)), mask_NCONJ);
              v_3 = _mm_xor_pd(_mm_mul_pd(_mm_shuffle_pd(v_1, v_1, 0b01), _mm_shuffle_pd(y_1, y_1, 0b11)), mask_NCONJ);
              v_0 = _mm_mul_pd(v_0, _mm_shuffle_pd(y_0, y_0, 0b00));
              v_1 = _mm_mul_pd(v_1, _mm_shuffle_pd(y_1, y_1, 0b00));
              q_0 = s_0_0;
              q_1 = s_0_1;
              q_2 = s_0_2;
              q_3 = s_0_3;
              s_0_0 = _mm_add_pd(s_0_0, _mm_or_pd(v_0, mask_BLP));
              s_0_1 = _mm_add_pd(s_0_1, _mm_or_pd(v_1, mask_BLP));
              s_0_2 = _mm_add_pd(s_0_2, _mm_or_pd(v_2, mask_BLP));
              s_0_3 = _mm_add_pd(s_0_3, _mm_or_pd(v_3, mask_BLP));
              q_0 = _mm_sub_pd(q_0, s_0_0);
              q_1 = _mm_sub_pd(q_1, s_0_1);
              q_2 = _mm_sub_pd(q_2, s_0_2);
              q_3 = _mm_sub_pd(q_3, s_0_3);
              v_0 = _mm_add_pd(v_0, q_0);
              v_1 = _mm_add_pd(v_1, q_1);
              v_2 = _mm_add_pd(v_2, q_2);
              v_3 = _mm_add_pd(v_3, q_3);
              q_0 = s_1_0;
              q_1 = s_1_1;
              q_2 = s_1_2;
              q_3 = s_1_3;
              s_1_0 = _mm_add_pd(s_1_0, _mm_or_pd(v_0, mask_BLP));
              s_1_1 = _mm_add_pd(s_1_1, _mm_or_pd(v_1, mask_BLP));
              s_1_2 = _mm_add_pd(s_1_2, _mm_or_pd(v_2, mask_BLP));
              s_1_3 = _mm_add_pd(s_1_3, _mm_or_pd(v_3, mask_BLP));
              q_0 = _mm_sub_pd(q_0, s_1_0);
              q_1 = _mm_sub_pd(q_1, s_1_1);
              q_2 = _mm_sub_pd(q_2, s_1_2);
              q_3 = _mm_sub_pd(q_3, s_1_3);
              v_0 = _mm_add_pd(v_0, q_0);
              v_1 = _mm_add_pd(v_1, q_1);
              v_2 = _mm_add_pd(v_2, q_2);
              v_3 = _mm_add_pd(v_3, q_3);
              s_2_0 = _mm_add_pd(s_2_0, _mm_or_pd(v_0, mask_BLP));
              s_2_1 = _mm_add_pd(s_2_1, _mm_or_pd(v_1, mask_BLP));
              s_2_2 = _mm_add_pd(s_2_2, _mm_or_pd(v_2, mask_BLP));
              s_2_3 = _mm_add_pd(s_2_3, _mm_or_pd(v_3, mask_BLP));
              i += 2, v_base += (incv * 4), y_base += (incy * 4);
            }
            if(i + 1 <= n){
              v_0 = _mm_loadu_pd(v_base);
              y_0 = _mm_loadu_pd(y_base);
              v_1 = _mm_xor_pd(_mm_mul_pd(_mm_shuffle_pd(v_0, v_0, 0b01), _mm_shuffle_pd(y_0, y_0, 0b11)), mask_NCONJ);
              v_0 = _mm_mul_pd(v_0, _mm_shuffle_pd(y_0, y_0, 0b00));
              q_0 = s_0_0;
              q_1 = s_0_1;
              s_0_0 = _mm_add_pd(s_0_0, _mm_or_pd(v_0, mask_BLP));
              s_0_1 = _mm_add_pd(s_0_1, _mm_or_pd(v_1, mask_BLP));
              q_0 = _mm_sub_pd(q_0, s_0_0);
              q_1 = _mm_sub_pd(q_1, s_0_1);
              v_0 = _mm_add_pd(v_0, q_0);
              v_1 = _mm_add_pd(v_1, q_1);
              q_0 = s_1_0;
              q_1 = s_1_1;
              s_1_0 = _mm_add_pd(s_1_0, _mm_or_pd(v_0, mask_BLP));
              s_1_1 = _mm_add_pd(s_1_1, _mm_or_pd(v_1, mask_BLP));
              q_0 = _mm_sub_pd(q_0, s_1_0);
              q_1 = _mm_sub_pd(q_1, s_1_1);
              v_0 = _mm_add_pd(v_0, q_0);
              v_1 = _mm_add_pd(v_1, q_1);
              s_2_0 = _mm_add_pd(s_2_0, _mm_or_pd(v_0, mask_BLP));
              s_2_1 = _mm_add_pd(s_2_1, _mm_or_pd(v_1, mask_BLP));
              i += 1, v_base += (incv * 2), y_base += (incy * 2);
            }
          }
        }
        q_0 = _mm_loadu_pd(sum_base);
        s_0_0 = _mm_add_pd(s_0_0, _mm_sub_pd(s_0_1, q_0));
        s_0_0 = _mm_add_pd(s_0_0, _mm_sub_pd(s_0_2, q_0));
        s_0_0 = _mm_add_pd(s_0_0, _mm_sub_pd(s_0_3, q_0));
        _mm_store_pd((double*)sum, s_0_0);
        q_0 = _mm_loadu_pd(sum_base + 2);
        s_1_0 = _mm_add_pd(s_1_0, _mm_sub_pd(s_1_1, q_0));
        s_1_0 = _mm_add_pd(s_1_0, _mm_sub_pd(s_1_2, q_0));
        s_1_0 = _mm_add_pd(s_1_0, _mm_sub_pd(s_1_3, q_0));
        _mm_store_pd((double*)sum + 2, s_1_0);
        q_0 = _mm_loadu_pd(sum_base + 4);
        s_2_0 = _mm_add_pd(s_2_0, _mm_sub_pd(s_2_1, q_0));
        s_2_0 = _mm_add_pd(s_2_0, _mm_sub_pd(s_2_2, q_0));
        s_2_0 = _mm_add_pd(s_2_0, _mm_sub_pd(s_2_3, q_0));
        _mm_store_pd((double*)sum + 4, s_2_0);
        RESET_DAZ_FLAG
        return;
      }
      default:{
        int i, j;

        double* sum_base = (double*) sum;
        double* v_base = (double*) v;
        double* y_base = (double*) y;
        __m128d v_0, v_1, v_2, v_3, v_4, v_5, v_6, v_7;
        __m128d y_0, y_1, y_2, y_3;
        __m128d q_0, q_1, q_2, q_3;
        __m128d s_0, s_1, s_2, s_3;
        __m128d s_buffer[(MAX_FOLD * 4)];

        for(j = 0; j < fold; j += 1){
          s_buffer[(j * 4)] = s_buffer[((j * 4) + 1)] = s_buffer[((j * 4) + 2)] = s_buffer[((j * 4) + 3)] = _mm_loadu_pd(sum_base + (j * 2));
        }
        if(incv == 1){
          if(incy == 1){

            for(i = 0; i + 4 <= n; i += 4, v_base += 8, y_base += 8){
              v_0 = _mm_loadu_pd(v_base);
              v_1 = _mm_loadu_pd(v_base + 2);
              v_2 = _mm_loadu_pd(v_base + 4);
              v_3 = _mm_loadu_pd(v_base + 6);
              y_0 = _mm_loadu_pd(y_base);
              y_1 = _mm_loadu_pd(y_base + 2);
              y_2 = _mm_loadu_pd(y_base + 4);
              y_3 = _mm_loadu_pd(y_base + 6);
              v_4 = _mm_xor_pd(_mm_mul_pd(_mm_shuffle_pd(v_0, v_0, 0b01), _mm_shuffle_pd(y_0, y_0, 0b11)), mask_NCONJ);
              v_5 = _mm_xor_pd(_mm_mul_pd(_mm_shuffle_pd(v_1, v_1, 0b01), _mm_shuffle_pd(y_1, y_1, 0b11)), mask_NCONJ);
              v_6 = _mm_xor_pd(_mm_mul_pd(_mm_shuffle_pd(v_2, v_2, 0b01), _mm_shuffle_pd(y_2, y_2, 0b11)), mask_NCONJ);
              v_7 = _mm_xor_pd(_mm_mul_pd(_mm_shuffle_pd(v_3, v_3, 0b01), _mm_shuffle_pd(y_3, y_3, 0b11)), mask_NCONJ);
              v_0 = _mm_mul_pd(v_0, _mm_shuffle_pd(y_0, y_0, 0b00));
              v_1 = _mm_mul_pd(v_1, _mm_shuffle_pd(y_1, y_1, 0b00));
              v_2 = _mm_mul_pd(v_2, _mm_shuffle_pd(y_2, y_2, 0b00));
              v_3 = _mm_mul_pd(v_3, _mm_shuffle_pd(y_3, y_3, 0b00));
              for(j = 0; j < fold - 1; j++){
                s_0 = s_buffer[(j * 4)];
                s_1 = s_buffer[((j * 4) + 1)];
                s_2 = s_buffer[((j * 4) + 2)];
                s_3 = s_buffer[((j * 4) + 3)];
                q_0 = _mm_add_pd(s_0, _mm_or_pd(v_0, mask_BLP));
                q_1 = _mm_add_pd(s_1, _mm_or_pd(v_1, mask_BLP));
                q_2 = _mm_add_pd(s_2, _mm_or_pd(v_2, mask_BLP));
                q_3 = _mm_add_pd(s_3, _mm_or_pd(v_3, mask_BLP));
                s_buffer[(j * 4)] = q_0;
                s_buffer[((j * 4) + 1)] = q_1;
                s_buffer[((j * 4) + 2)] = q_2;
                s_buffer[((j * 4) + 3)] = q_3;
                q_0 = _mm_sub_pd(s_0, q_0);
                q_1 = _mm_sub_pd(s_1, q_1);
                q_2 = _mm_sub_pd(s_2, q_2);
                q_3 = _mm_sub_pd(s_3, q_3);
                v_0 = _mm_add_pd(v_0, q_0);
                v_1 = _mm_add_pd(v_1, q_1);
                v_2 = _mm_add_pd(v_2, q_2);
                v_3 = _mm_add_pd(v_3, q_3);
                s_0 = s_buffer[(j * 4)];
                s_1 = s_buffer[((j * 4) + 1)];
                s_2 = s_buffer[((j * 4) + 2)];
                s_3 = s_buffer[((j * 4) + 3)];
                q_0 = _mm_add_pd(s_0, _mm_or_pd(v_4, mask_BLP));
                q_1 = _mm_add_pd(s_1, _mm_or_pd(v_5, mask_BLP));
                q_2 = _mm_add_pd(s_2, _mm_or_pd(v_6, mask_BLP));
                q_3 = _mm_add_pd(s_3, _mm_or_pd(v_7, mask_BLP));
                s_buffer[(j * 4)] = q_0;
                s_buffer[((j * 4) + 1)] = q_1;
                s_buffer[((j * 4) + 2)] = q_2;
                s_buffer[((j * 4) + 3)] = q_3;
                q_0 = _mm_sub_pd(s_0, q_0);
                q_1 = _mm_sub_pd(s_1, q_1);
                q_2 = _mm_sub_pd(s_2, q_2);
                q_3 = _mm_sub_pd(s_3, q_3);
                v_4 = _mm_add_pd(v_4, q_0);
                v_5 = _mm_add_pd(v_5, q_1);
                v_6 = _mm_add_pd(v_6, q_2);
                v_7 = _mm_add_pd(v_7, q_3);
              }
              s_buffer[(j * 4)] = _mm_add_pd(s_buffer[(j * 4)], _mm_or_pd(v_4, mask_BLP));
              s_buffer[((j * 4) + 1)] = _mm_add_pd(s_buffer[((j * 4) + 1)], _mm_or_pd(v_5, mask_BLP));
              s_buffer[((j * 4) + 2)] = _mm_add_pd(s_buffer[((j * 4) + 2)], _mm_or_pd(v_6, mask_BLP));
              s_buffer[((j * 4) + 3)] = _mm_add_pd(s_buffer[((j * 4) + 3)], _mm_or_pd(v_7, mask_BLP));
            }
            if(i + 2 <= n){
              v_0 = _mm_loadu_pd(v_base);
              v_1 = _mm_loadu_pd(v_base + 2);
              y_0 = _mm_loadu_pd(y_base);
              y_1 = _mm_loadu_pd(y_base + 2);
              v_2 = _mm_xor_pd(_mm_mul_pd(_mm_shuffle_pd(v_0, v_0, 0b01), _mm_shuffle_pd(y_0, y_0, 0b11)), mask_NCONJ);
              v_3 = _mm_xor_pd(_mm_mul_pd(_mm_shuffle_pd(v_1, v_1, 0b01), _mm_shuffle_pd(y_1, y_1, 0b11)), mask_NCONJ);
              v_0 = _mm_mul_pd(v_0, _mm_shuffle_pd(y_0, y_0, 0b00));
              v_1 = _mm_mul_pd(v_1, _mm_shuffle_pd(y_1, y_1, 0b00));
              for(j = 0; j < fold - 1; j++){
                s_0 = s_buffer[(j * 4)];
                s_1 = s_buffer[((j * 4) + 1)];
                s_2 = s_buffer[((j * 4) + 2)];
                s_3 = s_buffer[((j * 4) + 3)];
                q_0 = _mm_add_pd(s_0, _mm_or_pd(v_0, mask_BLP));
                q_1 = _mm_add_pd(s_1, _mm_or_pd(v_1, mask_BLP));
                q_2 = _mm_add_pd(s_2, _mm_or_pd(v_2, mask_BLP));
                q_3 = _mm_add_pd(s_3, _mm_or_pd(v_3, mask_BLP));
                s_buffer[(j * 4)] = q_0;
                s_buffer[((j * 4) + 1)] = q_1;
                s_buffer[((j * 4) + 2)] = q_2;
                s_buffer[((j * 4) + 3)] = q_3;
                q_0 = _mm_sub_pd(s_0, q_0);
                q_1 = _mm_sub_pd(s_1, q_1);
                q_2 = _mm_sub_pd(s_2, q_2);
                q_3 = _mm_sub_pd(s_3, q_3);
                v_0 = _mm_add_pd(v_0, q_0);
                v_1 = _mm_add_pd(v_1, q_1);
                v_2 = _mm_add_pd(v_2, q_2);
                v_3 = _mm_add_pd(v_3, q_3);
              }
              s_buffer[(j * 4)] = _mm_add_pd(s_buffer[(j * 4)], _mm_or_pd(v_0, mask_BLP));
              s_buffer[((j * 4) + 1)] = _mm_add_pd(s_buffer[((j * 4) + 1)], _mm_or_pd(v_1, mask_BLP));
              s_buffer[((j * 4) + 2)] = _mm_add_pd(s_buffer[((j * 4) + 2)], _mm_or_pd(v_2, mask_BLP));
              s_buffer[((j * 4) + 3)] = _mm_add_pd(s_buffer[((j * 4) + 3)], _mm_or_pd(v_3, mask_BLP));
              i += 2, v_base += 4, y_base += 4;
            }
            if(i + 1 <= n){
              v_0 = _mm_loadu_pd(v_base);
              y_0 = _mm_loadu_pd(y_base);
              v_1 = _mm_xor_pd(_mm_mul_pd(_mm_shuffle_pd(v_0, v_0, 0b01), _mm_shuffle_pd(y_0, y_0, 0b11)), mask_NCONJ);
              v_0 = _mm_mul_pd(v_0, _mm_shuffle_pd(y_0, y_0, 0b00));
              for(j = 0; j < fold - 1; j++){
                s_0 = s_buffer[(j * 4)];
                s_1 = s_buffer[((j * 4) + 1)];
                q_0 = _mm_add_pd(s_0, _mm_or_pd(v_0, mask_BLP));
                q_1 = _mm_add_pd(s_1, _mm_or_pd(v_1, mask_BLP));
                s_buffer[(j * 4)] = q_0;
                s_buffer[((j * 4) + 1)] = q_1;
                q_0 = _mm_sub_pd(s_0, q_0);
                q_1 = _mm_sub_pd(s_1, q_1);
                v_0 = _mm_add_pd(v_0, q_0);
                v_1 = _mm_add_pd(v_1, q_1);
              }
              s_buffer[(j * 4)] = _mm_add_pd(s_buffer[(j * 4)], _mm_or_pd(v_0, mask_BLP));
              s_buffer[((j * 4) + 1)] = _mm_add_pd(s_buffer[((j * 4) + 1)], _mm_or_pd(v_1, mask_BLP));
              i += 1, v_base += 2, y_base += 2;
            }
          }else{

            for(i = 0; i + 4 <= n; i += 4, v_base += 8, y_base += (incy * 8)){
              v_0 = _mm_loadu_pd(v_base);
              v_1 = _mm_loadu_pd(v_base + 2);
              v_2 = _mm_loadu_pd(v_base + 4);
              v_3 = _mm_loadu_pd(v_base + 6);
              y_0 = _mm_loadu_pd(y_base);
              y_1 = _mm_loadu_pd(y_base + (incy * 2));
              y_2 = _mm_loadu_pd(y_base + (incy * 4));
              y_3 = _mm_loadu_pd(y_base + (incy * 6));
              v_4 = _mm_xor_pd(_mm_mul_pd(_mm_shuffle_pd(v_0, v_0, 0b01), _mm_shuffle_pd(y_0, y_0, 0b11)), mask_NCONJ);
              v_5 = _mm_xor_pd(_mm_mul_pd(_mm_shuffle_pd(v_1, v_1, 0b01), _mm_shuffle_pd(y_1, y_1, 0b11)), mask_NCONJ);
              v_6 = _mm_xor_pd(_mm_mul_pd(_mm_shuffle_pd(v_2, v_2, 0b01), _mm_shuffle_pd(y_2, y_2, 0b11)), mask_NCONJ);
              v_7 = _mm_xor_pd(_mm_mul_pd(_mm_shuffle_pd(v_3, v_3, 0b01), _mm_shuffle_pd(y_3, y_3, 0b11)), mask_NCONJ);
              v_0 = _mm_mul_pd(v_0, _mm_shuffle_pd(y_0, y_0, 0b00));
              v_1 = _mm_mul_pd(v_1, _mm_shuffle_pd(y_1, y_1, 0b00));
              v_2 = _mm_mul_pd(v_2, _mm_shuffle_pd(y_2, y_2, 0b00));
              v_3 = _mm_mul_pd(v_3, _mm_shuffle_pd(y_3, y_3, 0b00));
              for(j = 0; j < fold - 1; j++){
                s_0 = s_buffer[(j * 4)];
                s_1 = s_buffer[((j * 4) + 1)];
                s_2 = s_buffer[((j * 4) + 2)];
                s_3 = s_buffer[((j * 4) + 3)];
                q_0 = _mm_add_pd(s_0, _mm_or_pd(v_0, mask_BLP));
                q_1 = _mm_add_pd(s_1, _mm_or_pd(v_1, mask_BLP));
                q_2 = _mm_add_pd(s_2, _mm_or_pd(v_2, mask_BLP));
                q_3 = _mm_add_pd(s_3, _mm_or_pd(v_3, mask_BLP));
                s_buffer[(j * 4)] = q_0;
                s_buffer[((j * 4) + 1)] = q_1;
                s_buffer[((j * 4) + 2)] = q_2;
                s_buffer[((j * 4) + 3)] = q_3;
                q_0 = _mm_sub_pd(s_0, q_0);
                q_1 = _mm_sub_pd(s_1, q_1);
                q_2 = _mm_sub_pd(s_2, q_2);
                q_3 = _mm_sub_pd(s_3, q_3);
                v_0 = _mm_add_pd(v_0, q_0);
                v_1 = _mm_add_pd(v_1, q_1);
                v_2 = _mm_add_pd(v_2, q_2);
                v_3 = _mm_add_pd(v_3, q_3);
                s_0 = s_buffer[(j * 4)];
                s_1 = s_buffer[((j * 4) + 1)];
                s_2 = s_buffer[((j * 4) + 2)];
                s_3 = s_buffer[((j * 4) + 3)];
                q_0 = _mm_add_pd(s_0, _mm_or_pd(v_4, mask_BLP));
                q_1 = _mm_add_pd(s_1, _mm_or_pd(v_5, mask_BLP));
                q_2 = _mm_add_pd(s_2, _mm_or_pd(v_6, mask_BLP));
                q_3 = _mm_add_pd(s_3, _mm_or_pd(v_7, mask_BLP));
                s_buffer[(j * 4)] = q_0;
                s_buffer[((j * 4) + 1)] = q_1;
                s_buffer[((j * 4) + 2)] = q_2;
                s_buffer[((j * 4) + 3)] = q_3;
                q_0 = _mm_sub_pd(s_0, q_0);
                q_1 = _mm_sub_pd(s_1, q_1);
                q_2 = _mm_sub_pd(s_2, q_2);
                q_3 = _mm_sub_pd(s_3, q_3);
                v_4 = _mm_add_pd(v_4, q_0);
                v_5 = _mm_add_pd(v_5, q_1);
                v_6 = _mm_add_pd(v_6, q_2);
                v_7 = _mm_add_pd(v_7, q_3);
              }
              s_buffer[(j * 4)] = _mm_add_pd(s_buffer[(j * 4)], _mm_or_pd(v_4, mask_BLP));
              s_buffer[((j * 4) + 1)] = _mm_add_pd(s_buffer[((j * 4) + 1)], _mm_or_pd(v_5, mask_BLP));
              s_buffer[((j * 4) + 2)] = _mm_add_pd(s_buffer[((j * 4) + 2)], _mm_or_pd(v_6, mask_BLP));
              s_buffer[((j * 4) + 3)] = _mm_add_pd(s_buffer[((j * 4) + 3)], _mm_or_pd(v_7, mask_BLP));
            }
            if(i + 2 <= n){
              v_0 = _mm_loadu_pd(v_base);
              v_1 = _mm_loadu_pd(v_base + 2);
              y_0 = _mm_loadu_pd(y_base);
              y_1 = _mm_loadu_pd(y_base + (incy * 2));
              v_2 = _mm_xor_pd(_mm_mul_pd(_mm_shuffle_pd(v_0, v_0, 0b01), _mm_shuffle_pd(y_0, y_0, 0b11)), mask_NCONJ);
              v_3 = _mm_xor_pd(_mm_mul_pd(_mm_shuffle_pd(v_1, v_1, 0b01), _mm_shuffle_pd(y_1, y_1, 0b11)), mask_NCONJ);
              v_0 = _mm_mul_pd(v_0, _mm_shuffle_pd(y_0, y_0, 0b00));
              v_1 = _mm_mul_pd(v_1, _mm_shuffle_pd(y_1, y_1, 0b00));
              for(j = 0; j < fold - 1; j++){
                s_0 = s_buffer[(j * 4)];
                s_1 = s_buffer[((j * 4) + 1)];
                s_2 = s_buffer[((j * 4) + 2)];
                s_3 = s_buffer[((j * 4) + 3)];
                q_0 = _mm_add_pd(s_0, _mm_or_pd(v_0, mask_BLP));
                q_1 = _mm_add_pd(s_1, _mm_or_pd(v_1, mask_BLP));
                q_2 = _mm_add_pd(s_2, _mm_or_pd(v_2, mask_BLP));
                q_3 = _mm_add_pd(s_3, _mm_or_pd(v_3, mask_BLP));
                s_buffer[(j * 4)] = q_0;
                s_buffer[((j * 4) + 1)] = q_1;
                s_buffer[((j * 4) + 2)] = q_2;
                s_buffer[((j * 4) + 3)] = q_3;
                q_0 = _mm_sub_pd(s_0, q_0);
                q_1 = _mm_sub_pd(s_1, q_1);
                q_2 = _mm_sub_pd(s_2, q_2);
                q_3 = _mm_sub_pd(s_3, q_3);
                v_0 = _mm_add_pd(v_0, q_0);
                v_1 = _mm_add_pd(v_1, q_1);
                v_2 = _mm_add_pd(v_2, q_2);
                v_3 = _mm_add_pd(v_3, q_3);
              }
              s_buffer[(j * 4)] = _mm_add_pd(s_buffer[(j * 4)], _mm_or_pd(v_0, mask_BLP));
              s_buffer[((j * 4) + 1)] = _mm_add_pd(s_buffer[((j * 4) + 1)], _mm_or_pd(v_1, mask_BLP));
              s_buffer[((j * 4) + 2)] = _mm_add_pd(s_buffer[((j * 4) + 2)], _mm_or_pd(v_2, mask_BLP));
              s_buffer[((j * 4) + 3)] = _mm_add_pd(s_buffer[((j * 4) + 3)], _mm_or_pd(v_3, mask_BLP));
              i += 2, v_base += 4, y_base += (incy * 4);
            }
            if(i + 1 <= n){
              v_0 = _mm_loadu_pd(v_base);
              y_0 = _mm_loadu_pd(y_base);
              v_1 = _mm_xor_pd(_mm_mul_pd(_mm_shuffle_pd(v_0, v_0, 0b01), _mm_shuffle_pd(y_0, y_0, 0b11)), mask_NCONJ);
              v_0 = _mm_mul_pd(v_0, _mm_shuffle_pd(y_0, y_0, 0b00));
              for(j = 0; j < fold - 1; j++){
                s_0 = s_buffer[(j * 4)];
                s_1 = s_buffer[((j * 4) + 1)];
                q_0 = _mm_add_pd(s_0, _mm_or_pd(v_0, mask_BLP));
                q_1 = _mm_add_pd(s_1, _mm_or_pd(v_1, mask_BLP));
                s_buffer[(j * 4)] = q_0;
                s_buffer[((j * 4) + 1)] = q_1;
                q_0 = _mm_sub_pd(s_0, q_0);
                q_1 = _mm_sub_pd(s_1, q_1);
                v_0 = _mm_add_pd(v_0, q_0);
                v_1 = _mm_add_pd(v_1, q_1);
              }
              s_buffer[(j * 4)] = _mm_add_pd(s_buffer[(j * 4)], _mm_or_pd(v_0, mask_BLP));
              s_buffer[((j * 4) + 1)] = _mm_add_pd(s_buffer[((j * 4) + 1)], _mm_or_pd(v_1, mask_BLP));
              i += 1, v_base += 2, y_base += (incy * 2);
            }
          }
        }else{
          if(incy == 1){

            for(i = 0; i + 4 <= n; i += 4, v_base += (incv * 8), y_base += 8){
              v_0 = _mm_loadu_pd(v_base);
              v_1 = _mm_loadu_pd(v_base + (incv * 2));
              v_2 = _mm_loadu_pd(v_base + (incv * 4));
              v_3 = _mm_loadu_pd(v_base + (incv * 6));
              y_0 = _mm_loadu_pd(y_base);
              y_1 = _mm_loadu_pd(y_base + 2);
              y_2 = _mm_loadu_pd(y_base + 4);
              y_3 = _mm_loadu_pd(y_base + 6);
              v_4 = _mm_xor_pd(_mm_mul_pd(_mm_shuffle_pd(v_0, v_0, 0b01), _mm_shuffle_pd(y_0, y_0, 0b11)), mask_NCONJ);
              v_5 = _mm_xor_pd(_mm_mul_pd(_mm_shuffle_pd(v_1, v_1, 0b01), _mm_shuffle_pd(y_1, y_1, 0b11)), mask_NCONJ);
              v_6 = _mm_xor_pd(_mm_mul_pd(_mm_shuffle_pd(v_2, v_2, 0b01), _mm_shuffle_pd(y_2, y_2, 0b11)), mask_NCONJ);
              v_7 = _mm_xor_pd(_mm_mul_pd(_mm_shuffle_pd(v_3, v_3, 0b01), _mm_shuffle_pd(y_3, y_3, 0b11)), mask_NCONJ);
              v_0 = _mm_mul_pd(v_0, _mm_shuffle_pd(y_0, y_0, 0b00));
              v_1 = _mm_mul_pd(v_1, _mm_shuffle_pd(y_1, y_1, 0b00));
              v_2 = _mm_mul_pd(v_2, _mm_shuffle_pd(y_2, y_2, 0b00));
              v_3 = _mm_mul_pd(v_3, _mm_shuffle_pd(y_3, y_3, 0b00));
              for(j = 0; j < fold - 1; j++){
                s_0 = s_buffer[(j * 4)];
                s_1 = s_buffer[((j * 4) + 1)];
                s_2 = s_buffer[((j * 4) + 2)];
                s_3 = s_buffer[((j * 4) + 3)];
                q_0 = _mm_add_pd(s_0, _mm_or_pd(v_0, mask_BLP));
                q_1 = _mm_add_pd(s_1, _mm_or_pd(v_1, mask_BLP));
                q_2 = _mm_add_pd(s_2, _mm_or_pd(v_2, mask_BLP));
                q_3 = _mm_add_pd(s_3, _mm_or_pd(v_3, mask_BLP));
                s_buffer[(j * 4)] = q_0;
                s_buffer[((j * 4) + 1)] = q_1;
                s_buffer[((j * 4) + 2)] = q_2;
                s_buffer[((j * 4) + 3)] = q_3;
                q_0 = _mm_sub_pd(s_0, q_0);
                q_1 = _mm_sub_pd(s_1, q_1);
                q_2 = _mm_sub_pd(s_2, q_2);
                q_3 = _mm_sub_pd(s_3, q_3);
                v_0 = _mm_add_pd(v_0, q_0);
                v_1 = _mm_add_pd(v_1, q_1);
                v_2 = _mm_add_pd(v_2, q_2);
                v_3 = _mm_add_pd(v_3, q_3);
                s_0 = s_buffer[(j * 4)];
                s_1 = s_buffer[((j * 4) + 1)];
                s_2 = s_buffer[((j * 4) + 2)];
                s_3 = s_buffer[((j * 4) + 3)];
                q_0 = _mm_add_pd(s_0, _mm_or_pd(v_4, mask_BLP));
                q_1 = _mm_add_pd(s_1, _mm_or_pd(v_5, mask_BLP));
                q_2 = _mm_add_pd(s_2, _mm_or_pd(v_6, mask_BLP));
                q_3 = _mm_add_pd(s_3, _mm_or_pd(v_7, mask_BLP));
                s_buffer[(j * 4)] = q_0;
                s_buffer[((j * 4) + 1)] = q_1;
                s_buffer[((j * 4) + 2)] = q_2;
                s_buffer[((j * 4) + 3)] = q_3;
                q_0 = _mm_sub_pd(s_0, q_0);
                q_1 = _mm_sub_pd(s_1, q_1);
                q_2 = _mm_sub_pd(s_2, q_2);
                q_3 = _mm_sub_pd(s_3, q_3);
                v_4 = _mm_add_pd(v_4, q_0);
                v_5 = _mm_add_pd(v_5, q_1);
                v_6 = _mm_add_pd(v_6, q_2);
                v_7 = _mm_add_pd(v_7, q_3);
              }
              s_buffer[(j * 4)] = _mm_add_pd(s_buffer[(j * 4)], _mm_or_pd(v_4, mask_BLP));
              s_buffer[((j * 4) + 1)] = _mm_add_pd(s_buffer[((j * 4) + 1)], _mm_or_pd(v_5, mask_BLP));
              s_buffer[((j * 4) + 2)] = _mm_add_pd(s_buffer[((j * 4) + 2)], _mm_or_pd(v_6, mask_BLP));
              s_buffer[((j * 4) + 3)] = _mm_add_pd(s_buffer[((j * 4) + 3)], _mm_or_pd(v_7, mask_BLP));
            }
            if(i + 2 <= n){
              v_0 = _mm_loadu_pd(v_base);
              v_1 = _mm_loadu_pd(v_base + (incv * 2));
              y_0 = _mm_loadu_pd(y_base);
              y_1 = _mm_loadu_pd(y_base + 2);
              v_2 = _mm_xor_pd(_mm_mul_pd(_mm_shuffle_pd(v_0, v_0, 0b01), _mm_shuffle_pd(y_0, y_0, 0b11)), mask_NCONJ);
              v_3 = _mm_xor_pd(_mm_mul_pd(_mm_shuffle_pd(v_1, v_1, 0b01), _mm_shuffle_pd(y_1, y_1, 0b11)), mask_NCONJ);
              v_0 = _mm_mul_pd(v_0, _mm_shuffle_pd(y_0, y_0, 0b00));
              v_1 = _mm_mul_pd(v_1, _mm_shuffle_pd(y_1, y_1, 0b00));
              for(j = 0; j < fold - 1; j++){
                s_0 = s_buffer[(j * 4)];
                s_1 = s_buffer[((j * 4) + 1)];
                s_2 = s_buffer[((j * 4) + 2)];
                s_3 = s_buffer[((j * 4) + 3)];
                q_0 = _mm_add_pd(s_0, _mm_or_pd(v_0, mask_BLP));
                q_1 = _mm_add_pd(s_1, _mm_or_pd(v_1, mask_BLP));
                q_2 = _mm_add_pd(s_2, _mm_or_pd(v_2, mask_BLP));
                q_3 = _mm_add_pd(s_3, _mm_or_pd(v_3, mask_BLP));
                s_buffer[(j * 4)] = q_0;
                s_buffer[((j * 4) + 1)] = q_1;
                s_buffer[((j * 4) + 2)] = q_2;
                s_buffer[((j * 4) + 3)] = q_3;
                q_0 = _mm_sub_pd(s_0, q_0);
                q_1 = _mm_sub_pd(s_1, q_1);
                q_2 = _mm_sub_pd(s_2, q_2);
                q_3 = _mm_sub_pd(s_3, q_3);
                v_0 = _mm_add_pd(v_0, q_0);
                v_1 = _mm_add_pd(v_1, q_1);
                v_2 = _mm_add_pd(v_2, q_2);
                v_3 = _mm_add_pd(v_3, q_3);
              }
              s_buffer[(j * 4)] = _mm_add_pd(s_buffer[(j * 4)], _mm_or_pd(v_0, mask_BLP));
              s_buffer[((j * 4) + 1)] = _mm_add_pd(s_buffer[((j * 4) + 1)], _mm_or_pd(v_1, mask_BLP));
              s_buffer[((j * 4) + 2)] = _mm_add_pd(s_buffer[((j * 4) + 2)], _mm_or_pd(v_2, mask_BLP));
              s_buffer[((j * 4) + 3)] = _mm_add_pd(s_buffer[((j * 4) + 3)], _mm_or_pd(v_3, mask_BLP));
              i += 2, v_base += (incv * 4), y_base += 4;
            }
            if(i + 1 <= n){
              v_0 = _mm_loadu_pd(v_base);
              y_0 = _mm_loadu_pd(y_base);
              v_1 = _mm_xor_pd(_mm_mul_pd(_mm_shuffle_pd(v_0, v_0, 0b01), _mm_shuffle_pd(y_0, y_0, 0b11)), mask_NCONJ);
              v_0 = _mm_mul_pd(v_0, _mm_shuffle_pd(y_0, y_0, 0b00));
              for(j = 0; j < fold - 1; j++){
                s_0 = s_buffer[(j * 4)];
                s_1 = s_buffer[((j * 4) + 1)];
                q_0 = _mm_add_pd(s_0, _mm_or_pd(v_0, mask_BLP));
                q_1 = _mm_add_pd(s_1, _mm_or_pd(v_1, mask_BLP));
                s_buffer[(j * 4)] = q_0;
                s_buffer[((j * 4) + 1)] = q_1;
                q_0 = _mm_sub_pd(s_0, q_0);
                q_1 = _mm_sub_pd(s_1, q_1);
                v_0 = _mm_add_pd(v_0, q_0);
                v_1 = _mm_add_pd(v_1, q_1);
              }
              s_buffer[(j * 4)] = _mm_add_pd(s_buffer[(j * 4)], _mm_or_pd(v_0, mask_BLP));
              s_buffer[((j * 4) + 1)] = _mm_add_pd(s_buffer[((j * 4) + 1)], _mm_or_pd(v_1, mask_BLP));
              i += 1, v_base += (incv * 2), y_base += 2;
            }
          }else{

            for(i = 0; i + 4 <= n; i += 4, v_base += (incv * 8), y_base += (incy * 8)){
              v_0 = _mm_loadu_pd(v_base);
              v_1 = _mm_loadu_pd(v_base + (incv * 2));
              v_2 = _mm_loadu_pd(v_base + (incv * 4));
              v_3 = _mm_loadu_pd(v_base + (incv * 6));
              y_0 = _mm_loadu_pd(y_base);
              y_1 = _mm_loadu_pd(y_base + (incy * 2));
              y_2 = _mm_loadu_pd(y_base + (incy * 4));
              y_3 = _mm_loadu_pd(y_base + (incy * 6));
              v_4 = _mm_xor_pd(_mm_mul_pd(_mm_shuffle_pd(v_0, v_0, 0b01), _mm_shuffle_pd(y_0, y_0, 0b11)), mask_NCONJ);
              v_5 = _mm_xor_pd(_mm_mul_pd(_mm_shuffle_pd(v_1, v_1, 0b01), _mm_shuffle_pd(y_1, y_1, 0b11)), mask_NCONJ);
              v_6 = _mm_xor_pd(_mm_mul_pd(_mm_shuffle_pd(v_2, v_2, 0b01), _mm_shuffle_pd(y_2, y_2, 0b11)), mask_NCONJ);
              v_7 = _mm_xor_pd(_mm_mul_pd(_mm_shuffle_pd(v_3, v_3, 0b01), _mm_shuffle_pd(y_3, y_3, 0b11)), mask_NCONJ);
              v_0 = _mm_mul_pd(v_0, _mm_shuffle_pd(y_0, y_0, 0b00));
              v_1 = _mm_mul_pd(v_1, _mm_shuffle_pd(y_1, y_1, 0b00));
              v_2 = _mm_mul_pd(v_2, _mm_shuffle_pd(y_2, y_2, 0b00));
              v_3 = _mm_mul_pd(v_3, _mm_shuffle_pd(y_3, y_3, 0b00));
              for(j = 0; j < fold - 1; j++){
                s_0 = s_buffer[(j * 4)];
                s_1 = s_buffer[((j * 4) + 1)];
                s_2 = s_buffer[((j * 4) + 2)];
                s_3 = s_buffer[((j * 4) + 3)];
                q_0 = _mm_add_pd(s_0, _mm_or_pd(v_0, mask_BLP));
                q_1 = _mm_add_pd(s_1, _mm_or_pd(v_1, mask_BLP));
                q_2 = _mm_add_pd(s_2, _mm_or_pd(v_2, mask_BLP));
                q_3 = _mm_add_pd(s_3, _mm_or_pd(v_3, mask_BLP));
                s_buffer[(j * 4)] = q_0;
                s_buffer[((j * 4) + 1)] = q_1;
                s_buffer[((j * 4) + 2)] = q_2;
                s_buffer[((j * 4) + 3)] = q_3;
                q_0 = _mm_sub_pd(s_0, q_0);
                q_1 = _mm_sub_pd(s_1, q_1);
                q_2 = _mm_sub_pd(s_2, q_2);
                q_3 = _mm_sub_pd(s_3, q_3);
                v_0 = _mm_add_pd(v_0, q_0);
                v_1 = _mm_add_pd(v_1, q_1);
                v_2 = _mm_add_pd(v_2, q_2);
                v_3 = _mm_add_pd(v_3, q_3);
                s_0 = s_buffer[(j * 4)];
                s_1 = s_buffer[((j * 4) + 1)];
                s_2 = s_buffer[((j * 4) + 2)];
                s_3 = s_buffer[((j * 4) + 3)];
                q_0 = _mm_add_pd(s_0, _mm_or_pd(v_4, mask_BLP));
                q_1 = _mm_add_pd(s_1, _mm_or_pd(v_5, mask_BLP));
                q_2 = _mm_add_pd(s_2, _mm_or_pd(v_6, mask_BLP));
                q_3 = _mm_add_pd(s_3, _mm_or_pd(v_7, mask_BLP));
                s_buffer[(j * 4)] = q_0;
                s_buffer[((j * 4) + 1)] = q_1;
                s_buffer[((j * 4) + 2)] = q_2;
                s_buffer[((j * 4) + 3)] = q_3;
                q_0 = _mm_sub_pd(s_0, q_0);
                q_1 = _mm_sub_pd(s_1, q_1);
                q_2 = _mm_sub_pd(s_2, q_2);
                q_3 = _mm_sub_pd(s_3, q_3);
                v_4 = _mm_add_pd(v_4, q_0);
                v_5 = _mm_add_pd(v_5, q_1);
                v_6 = _mm_add_pd(v_6, q_2);
                v_7 = _mm_add_pd(v_7, q_3);
              }
              s_buffer[(j * 4)] = _mm_add_pd(s_buffer[(j * 4)], _mm_or_pd(v_4, mask_BLP));
              s_buffer[((j * 4) + 1)] = _mm_add_pd(s_buffer[((j * 4) + 1)], _mm_or_pd(v_5, mask_BLP));
              s_buffer[((j * 4) + 2)] = _mm_add_pd(s_buffer[((j * 4) + 2)], _mm_or_pd(v_6, mask_BLP));
              s_buffer[((j * 4) + 3)] = _mm_add_pd(s_buffer[((j * 4) + 3)], _mm_or_pd(v_7, mask_BLP));
            }
            if(i + 2 <= n){
              v_0 = _mm_loadu_pd(v_base);
              v_1 = _mm_loadu_pd(v_base + (incv * 2));
              y_0 = _mm_loadu_pd(y_base);
              y_1 = _mm_loadu_pd(y_base + (incy * 2));
              v_2 = _mm_xor_pd(_mm_mul_pd(_mm_shuffle_pd(v_0, v_0, 0b01), _mm_shuffle_pd(y_0, y_0, 0b11)), mask_NCONJ);
              v_3 = _mm_xor_pd(_mm_mul_pd(_mm_shuffle_pd(v_1, v_1, 0b01), _mm_shuffle_pd(y_1, y_1, 0b11)), mask_NCONJ);
              v_0 = _mm_mul_pd(v_0, _mm_shuffle_pd(y_0, y_0, 0b00));
              v_1 = _mm_mul_pd(v_1, _mm_shuffle_pd(y_1, y_1, 0b00));
              for(j = 0; j < fold - 1; j++){
                s_0 = s_buffer[(j * 4)];
                s_1 = s_buffer[((j * 4) + 1)];
                s_2 = s_buffer[((j * 4) + 2)];
                s_3 = s_buffer[((j * 4) + 3)];
                q_0 = _mm_add_pd(s_0, _mm_or_pd(v_0, mask_BLP));
                q_1 = _mm_add_pd(s_1, _mm_or_pd(v_1, mask_BLP));
                q_2 = _mm_add_pd(s_2, _mm_or_pd(v_2, mask_BLP));
                q_3 = _mm_add_pd(s_3, _mm_or_pd(v_3, mask_BLP));
                s_buffer[(j * 4)] = q_0;
                s_buffer[((j * 4) + 1)] = q_1;
                s_buffer[((j * 4) + 2)] = q_2;
                s_buffer[((j * 4) + 3)] = q_3;
                q_0 = _mm_sub_pd(s_0, q_0);
                q_1 = _mm_sub_pd(s_1, q_1);
                q_2 = _mm_sub_pd(s_2, q_2);
                q_3 = _mm_sub_pd(s_3, q_3);
                v_0 = _mm_add_pd(v_0, q_0);
                v_1 = _mm_add_pd(v_1, q_1);
                v_2 = _mm_add_pd(v_2, q_2);
                v_3 = _mm_add_pd(v_3, q_3);
              }
              s_buffer[(j * 4)] = _mm_add_pd(s_buffer[(j * 4)], _mm_or_pd(v_0, mask_BLP));
              s_buffer[((j * 4) + 1)] = _mm_add_pd(s_buffer[((j * 4) + 1)], _mm_or_pd(v_1, mask_BLP));
              s_buffer[((j * 4) + 2)] = _mm_add_pd(s_buffer[((j * 4) + 2)], _mm_or_pd(v_2, mask_BLP));
              s_buffer[((j * 4) + 3)] = _mm_add_pd(s_buffer[((j * 4) + 3)], _mm_or_pd(v_3, mask_BLP));
              i += 2, v_base += (incv * 4), y_base += (incy * 4);
            }
            if(i + 1 <= n){
              v_0 = _mm_loadu_pd(v_base);
              y_0 = _mm_loadu_pd(y_base);
              v_1 = _mm_xor_pd(_mm_mul_pd(_mm_shuffle_pd(v_0, v_0, 0b01), _mm_shuffle_pd(y_0, y_0, 0b11)), mask_NCONJ);
              v_0 = _mm_mul_pd(v_0, _mm_shuffle_pd(y_0, y_0, 0b00));
              for(j = 0; j < fold - 1; j++){
                s_0 = s_buffer[(j * 4)];
                s_1 = s_buffer[((j * 4) + 1)];
                q_0 = _mm_add_pd(s_0, _mm_or_pd(v_0, mask_BLP));
                q_1 = _mm_add_pd(s_1, _mm_or_pd(v_1, mask_BLP));
                s_buffer[(j * 4)] = q_0;
                s_buffer[((j * 4) + 1)] = q_1;
                q_0 = _mm_sub_pd(s_0, q_0);
                q_1 = _mm_sub_pd(s_1, q_1);
                v_0 = _mm_add_pd(v_0, q_0);
                v_1 = _mm_add_pd(v_1, q_1);
              }
              s_buffer[(j * 4)] = _mm_add_pd(s_buffer[(j * 4)], _mm_or_pd(v_0, mask_BLP));
              s_buffer[((j * 4) + 1)] = _mm_add_pd(s_buffer[((j * 4) + 1)], _mm_or_pd(v_1, mask_BLP));
              i += 1, v_base += (incv * 2), y_base += (incy * 2);
            }
          }
        }
        for(j = 0; j < fold; j += 1){
          q_0 = _mm_loadu_pd(sum_base + (j * 2));
          s_buffer[(j * 4)] = _mm_add_pd(s_buffer[(j * 4)], _mm_sub_pd(s_buffer[((j * 4) + 1)], q_0));
          s_buffer[(j * 4)] = _mm_add_pd(s_buffer[(j * 4)], _mm_sub_pd(s_buffer[((j * 4) + 2)], q_0));
          s_buffer[(j * 4)] = _mm_add_pd(s_buffer[(j * 4)], _mm_sub_pd(s_buffer[((j * 4) + 3)], q_0));
          _mm_store_pd((double*)sum + (j * 2), s_buffer[(j * 4)]);
        }
        RESET_DAZ_FLAG
        return;
      }
    }
    //[[[end]]]
  }
#else
  void zdotuI2(int n, double complex* v, int incv, double complex* y, int incy, int fold, double complex* sum){
    /*[[[cog
    cog.out(generate.generate(dotuI2.DotUI2(dataTypes.DoubleComplex, vectorizations.SISD), args, params))
    ]]]*/
    long_double tmp_BLP;
    SET_DAZ_FLAG;
    switch(fold){
      case 3:{
        int i;

        double* sum_base = (double*) sum;
        double* v_base = (double*) v;
        double* y_base = (double*) y;
        double v_0, v_1, v_2, v_3, v_4, v_5, v_6, v_7, v_8, v_9, v_10, v_11, v_12, v_13, v_14, v_15;
        double y_0, y_1, y_2, y_3, y_4, y_5, y_6, y_7;
        double q_0, q_1, q_2, q_3, q_4, q_5, q_6, q_7;
        double s_0_0, s_0_1, s_0_2, s_0_3, s_0_4, s_0_5, s_0_6, s_0_7;
        double s_1_0, s_1_1, s_1_2, s_1_3, s_1_4, s_1_5, s_1_6, s_1_7;
        double s_2_0, s_2_1, s_2_2, s_2_3, s_2_4, s_2_5, s_2_6, s_2_7;

        s_0_0 = s_0_2 = s_0_4 = s_0_6 = sum_base[0];
        s_0_1 = s_0_3 = s_0_5 = s_0_7 = sum_base[1];
        s_1_0 = s_1_2 = s_1_4 = s_1_6 = sum_base[2];
        s_1_1 = s_1_3 = s_1_5 = s_1_7 = sum_base[3];
        s_2_0 = s_2_2 = s_2_4 = s_2_6 = sum_base[4];
        s_2_1 = s_2_3 = s_2_5 = s_2_7 = sum_base[5];
        if(incv == 1){
          if(incy == 1){

            for(i = 0; i + 4 <= n; i += 4, v_base += 8, y_base += 8){
              v_0 = v_base[0];
              v_1 = v_base[1];
              v_2 = v_base[2];
              v_3 = v_base[3];
              v_4 = v_base[4];
              v_5 = v_base[5];
              v_6 = v_base[6];
              v_7 = v_base[7];
              y_0 = y_base[0];
              y_1 = y_base[1];
              y_2 = y_base[2];
              y_3 = y_base[3];
              y_4 = y_base[4];
              y_5 = y_base[5];
              y_6 = y_base[6];
              y_7 = y_base[7];
              v_8 = v_1 * y_1 * -1;
              v_9 = v_0 * y_1;
              v_10 = v_3 * y_3 * -1;
              v_11 = v_2 * y_3;
              v_12 = v_5 * y_5 * -1;
              v_13 = v_4 * y_5;
              v_14 = v_7 * y_7 * -1;
              v_15 = v_6 * y_7;
              v_0 = v_0 * y_0;
              v_1 = v_1 * y_0;
              v_2 = v_2 * y_2;
              v_3 = v_3 * y_2;
              v_4 = v_4 * y_4;
              v_5 = v_5 * y_4;
              v_6 = v_6 * y_6;
              v_7 = v_7 * y_6;
              q_0 = s_0_0;
              q_1 = s_0_1;
              q_2 = s_0_2;
              q_3 = s_0_3;
              q_4 = s_0_4;
              q_5 = s_0_5;
              q_6 = s_0_6;
              q_7 = s_0_7;
              tmp_BLP.d = v_0;
              tmp_BLP.l |= 1;
              s_0_0 = s_0_0 + tmp_BLP.d;
              tmp_BLP.d = v_1;
              tmp_BLP.l |= 1;
              s_0_1 = s_0_1 + tmp_BLP.d;
              tmp_BLP.d = v_2;
              tmp_BLP.l |= 1;
              s_0_2 = s_0_2 + tmp_BLP.d;
              tmp_BLP.d = v_3;
              tmp_BLP.l |= 1;
              s_0_3 = s_0_3 + tmp_BLP.d;
              tmp_BLP.d = v_4;
              tmp_BLP.l |= 1;
              s_0_4 = s_0_4 + tmp_BLP.d;
              tmp_BLP.d = v_5;
              tmp_BLP.l |= 1;
              s_0_5 = s_0_5 + tmp_BLP.d;
              tmp_BLP.d = v_6;
              tmp_BLP.l |= 1;
              s_0_6 = s_0_6 + tmp_BLP.d;
              tmp_BLP.d = v_7;
              tmp_BLP.l |= 1;
              s_0_7 = s_0_7 + tmp_BLP.d;
              q_0 = q_0 - s_0_0;
              q_1 = q_1 - s_0_1;
              q_2 = q_2 - s_0_2;
              q_3 = q_3 - s_0_3;
              q_4 = q_4 - s_0_4;
              q_5 = q_5 - s_0_5;
              q_6 = q_6 - s_0_6;
              q_7 = q_7 - s_0_7;
              v_0 = v_0 + q_0;
              v_1 = v_1 + q_1;
              v_2 = v_2 + q_2;
              v_3 = v_3 + q_3;
              v_4 = v_4 + q_4;
              v_5 = v_5 + q_5;
              v_6 = v_6 + q_6;
              v_7 = v_7 + q_7;
              q_0 = s_1_0;
              q_1 = s_1_1;
              q_2 = s_1_2;
              q_3 = s_1_3;
              q_4 = s_1_4;
              q_5 = s_1_5;
              q_6 = s_1_6;
              q_7 = s_1_7;
              tmp_BLP.d = v_0;
              tmp_BLP.l |= 1;
              s_1_0 = s_1_0 + tmp_BLP.d;
              tmp_BLP.d = v_1;
              tmp_BLP.l |= 1;
              s_1_1 = s_1_1 + tmp_BLP.d;
              tmp_BLP.d = v_2;
              tmp_BLP.l |= 1;
              s_1_2 = s_1_2 + tmp_BLP.d;
              tmp_BLP.d = v_3;
              tmp_BLP.l |= 1;
              s_1_3 = s_1_3 + tmp_BLP.d;
              tmp_BLP.d = v_4;
              tmp_BLP.l |= 1;
              s_1_4 = s_1_4 + tmp_BLP.d;
              tmp_BLP.d = v_5;
              tmp_BLP.l |= 1;
              s_1_5 = s_1_5 + tmp_BLP.d;
              tmp_BLP.d = v_6;
              tmp_BLP.l |= 1;
              s_1_6 = s_1_6 + tmp_BLP.d;
              tmp_BLP.d = v_7;
              tmp_BLP.l |= 1;
              s_1_7 = s_1_7 + tmp_BLP.d;
              q_0 = q_0 - s_1_0;
              q_1 = q_1 - s_1_1;
              q_2 = q_2 - s_1_2;
              q_3 = q_3 - s_1_3;
              q_4 = q_4 - s_1_4;
              q_5 = q_5 - s_1_5;
              q_6 = q_6 - s_1_6;
              q_7 = q_7 - s_1_7;
              v_0 = v_0 + q_0;
              v_1 = v_1 + q_1;
              v_2 = v_2 + q_2;
              v_3 = v_3 + q_3;
              v_4 = v_4 + q_4;
              v_5 = v_5 + q_5;
              v_6 = v_6 + q_6;
              v_7 = v_7 + q_7;
              tmp_BLP.d = v_0;
              tmp_BLP.l |= 1;
              s_2_0 = s_2_0 + tmp_BLP.d;
              tmp_BLP.d = v_1;
              tmp_BLP.l |= 1;
              s_2_1 = s_2_1 + tmp_BLP.d;
              tmp_BLP.d = v_2;
              tmp_BLP.l |= 1;
              s_2_2 = s_2_2 + tmp_BLP.d;
              tmp_BLP.d = v_3;
              tmp_BLP.l |= 1;
              s_2_3 = s_2_3 + tmp_BLP.d;
              tmp_BLP.d = v_4;
              tmp_BLP.l |= 1;
              s_2_4 = s_2_4 + tmp_BLP.d;
              tmp_BLP.d = v_5;
              tmp_BLP.l |= 1;
              s_2_5 = s_2_5 + tmp_BLP.d;
              tmp_BLP.d = v_6;
              tmp_BLP.l |= 1;
              s_2_6 = s_2_6 + tmp_BLP.d;
              tmp_BLP.d = v_7;
              tmp_BLP.l |= 1;
              s_2_7 = s_2_7 + tmp_BLP.d;
              q_0 = s_0_0;
              q_1 = s_0_1;
              q_2 = s_0_2;
              q_3 = s_0_3;
              q_4 = s_0_4;
              q_5 = s_0_5;
              q_6 = s_0_6;
              q_7 = s_0_7;
              tmp_BLP.d = v_8;
              tmp_BLP.l |= 1;
              s_0_0 = s_0_0 + tmp_BLP.d;
              tmp_BLP.d = v_9;
              tmp_BLP.l |= 1;
              s_0_1 = s_0_1 + tmp_BLP.d;
              tmp_BLP.d = v_10;
              tmp_BLP.l |= 1;
              s_0_2 = s_0_2 + tmp_BLP.d;
              tmp_BLP.d = v_11;
              tmp_BLP.l |= 1;
              s_0_3 = s_0_3 + tmp_BLP.d;
              tmp_BLP.d = v_12;
              tmp_BLP.l |= 1;
              s_0_4 = s_0_4 + tmp_BLP.d;
              tmp_BLP.d = v_13;
              tmp_BLP.l |= 1;
              s_0_5 = s_0_5 + tmp_BLP.d;
              tmp_BLP.d = v_14;
              tmp_BLP.l |= 1;
              s_0_6 = s_0_6 + tmp_BLP.d;
              tmp_BLP.d = v_15;
              tmp_BLP.l |= 1;
              s_0_7 = s_0_7 + tmp_BLP.d;
              q_0 = q_0 - s_0_0;
              q_1 = q_1 - s_0_1;
              q_2 = q_2 - s_0_2;
              q_3 = q_3 - s_0_3;
              q_4 = q_4 - s_0_4;
              q_5 = q_5 - s_0_5;
              q_6 = q_6 - s_0_6;
              q_7 = q_7 - s_0_7;
              v_8 = v_8 + q_0;
              v_9 = v_9 + q_1;
              v_10 = v_10 + q_2;
              v_11 = v_11 + q_3;
              v_12 = v_12 + q_4;
              v_13 = v_13 + q_5;
              v_14 = v_14 + q_6;
              v_15 = v_15 + q_7;
              q_0 = s_1_0;
              q_1 = s_1_1;
              q_2 = s_1_2;
              q_3 = s_1_3;
              q_4 = s_1_4;
              q_5 = s_1_5;
              q_6 = s_1_6;
              q_7 = s_1_7;
              tmp_BLP.d = v_8;
              tmp_BLP.l |= 1;
              s_1_0 = s_1_0 + tmp_BLP.d;
              tmp_BLP.d = v_9;
              tmp_BLP.l |= 1;
              s_1_1 = s_1_1 + tmp_BLP.d;
              tmp_BLP.d = v_10;
              tmp_BLP.l |= 1;
              s_1_2 = s_1_2 + tmp_BLP.d;
              tmp_BLP.d = v_11;
              tmp_BLP.l |= 1;
              s_1_3 = s_1_3 + tmp_BLP.d;
              tmp_BLP.d = v_12;
              tmp_BLP.l |= 1;
              s_1_4 = s_1_4 + tmp_BLP.d;
              tmp_BLP.d = v_13;
              tmp_BLP.l |= 1;
              s_1_5 = s_1_5 + tmp_BLP.d;
              tmp_BLP.d = v_14;
              tmp_BLP.l |= 1;
              s_1_6 = s_1_6 + tmp_BLP.d;
              tmp_BLP.d = v_15;
              tmp_BLP.l |= 1;
              s_1_7 = s_1_7 + tmp_BLP.d;
              q_0 = q_0 - s_1_0;
              q_1 = q_1 - s_1_1;
              q_2 = q_2 - s_1_2;
              q_3 = q_3 - s_1_3;
              q_4 = q_4 - s_1_4;
              q_5 = q_5 - s_1_5;
              q_6 = q_6 - s_1_6;
              q_7 = q_7 - s_1_7;
              v_8 = v_8 + q_0;
              v_9 = v_9 + q_1;
              v_10 = v_10 + q_2;
              v_11 = v_11 + q_3;
              v_12 = v_12 + q_4;
              v_13 = v_13 + q_5;
              v_14 = v_14 + q_6;
              v_15 = v_15 + q_7;
              tmp_BLP.d = v_8;
              tmp_BLP.l |= 1;
              s_2_0 = s_2_0 + tmp_BLP.d;
              tmp_BLP.d = v_9;
              tmp_BLP.l |= 1;
              s_2_1 = s_2_1 + tmp_BLP.d;
              tmp_BLP.d = v_10;
              tmp_BLP.l |= 1;
              s_2_2 = s_2_2 + tmp_BLP.d;
              tmp_BLP.d = v_11;
              tmp_BLP.l |= 1;
              s_2_3 = s_2_3 + tmp_BLP.d;
              tmp_BLP.d = v_12;
              tmp_BLP.l |= 1;
              s_2_4 = s_2_4 + tmp_BLP.d;
              tmp_BLP.d = v_13;
              tmp_BLP.l |= 1;
              s_2_5 = s_2_5 + tmp_BLP.d;
              tmp_BLP.d = v_14;
              tmp_BLP.l |= 1;
              s_2_6 = s_2_6 + tmp_BLP.d;
              tmp_BLP.d = v_15;
              tmp_BLP.l |= 1;
              s_2_7 = s_2_7 + tmp_BLP.d;
            }
            if(i + 2 <= n){
              v_0 = v_base[0];
              v_1 = v_base[1];
              v_2 = v_base[2];
              v_3 = v_base[3];
              y_0 = y_base[0];
              y_1 = y_base[1];
              y_2 = y_base[2];
              y_3 = y_base[3];
              v_4 = v_1 * y_1 * -1;
              v_5 = v_0 * y_1;
              v_6 = v_3 * y_3 * -1;
              v_7 = v_2 * y_3;
              v_0 = v_0 * y_0;
              v_1 = v_1 * y_0;
              v_2 = v_2 * y_2;
              v_3 = v_3 * y_2;
              q_0 = s_0_0;
              q_1 = s_0_1;
              q_2 = s_0_2;
              q_3 = s_0_3;
              q_4 = s_0_4;
              q_5 = s_0_5;
              q_6 = s_0_6;
              q_7 = s_0_7;
              tmp_BLP.d = v_0;
              tmp_BLP.l |= 1;
              s_0_0 = s_0_0 + tmp_BLP.d;
              tmp_BLP.d = v_1;
              tmp_BLP.l |= 1;
              s_0_1 = s_0_1 + tmp_BLP.d;
              tmp_BLP.d = v_2;
              tmp_BLP.l |= 1;
              s_0_2 = s_0_2 + tmp_BLP.d;
              tmp_BLP.d = v_3;
              tmp_BLP.l |= 1;
              s_0_3 = s_0_3 + tmp_BLP.d;
              tmp_BLP.d = v_4;
              tmp_BLP.l |= 1;
              s_0_4 = s_0_4 + tmp_BLP.d;
              tmp_BLP.d = v_5;
              tmp_BLP.l |= 1;
              s_0_5 = s_0_5 + tmp_BLP.d;
              tmp_BLP.d = v_6;
              tmp_BLP.l |= 1;
              s_0_6 = s_0_6 + tmp_BLP.d;
              tmp_BLP.d = v_7;
              tmp_BLP.l |= 1;
              s_0_7 = s_0_7 + tmp_BLP.d;
              q_0 = q_0 - s_0_0;
              q_1 = q_1 - s_0_1;
              q_2 = q_2 - s_0_2;
              q_3 = q_3 - s_0_3;
              q_4 = q_4 - s_0_4;
              q_5 = q_5 - s_0_5;
              q_6 = q_6 - s_0_6;
              q_7 = q_7 - s_0_7;
              v_0 = v_0 + q_0;
              v_1 = v_1 + q_1;
              v_2 = v_2 + q_2;
              v_3 = v_3 + q_3;
              v_4 = v_4 + q_4;
              v_5 = v_5 + q_5;
              v_6 = v_6 + q_6;
              v_7 = v_7 + q_7;
              q_0 = s_1_0;
              q_1 = s_1_1;
              q_2 = s_1_2;
              q_3 = s_1_3;
              q_4 = s_1_4;
              q_5 = s_1_5;
              q_6 = s_1_6;
              q_7 = s_1_7;
              tmp_BLP.d = v_0;
              tmp_BLP.l |= 1;
              s_1_0 = s_1_0 + tmp_BLP.d;
              tmp_BLP.d = v_1;
              tmp_BLP.l |= 1;
              s_1_1 = s_1_1 + tmp_BLP.d;
              tmp_BLP.d = v_2;
              tmp_BLP.l |= 1;
              s_1_2 = s_1_2 + tmp_BLP.d;
              tmp_BLP.d = v_3;
              tmp_BLP.l |= 1;
              s_1_3 = s_1_3 + tmp_BLP.d;
              tmp_BLP.d = v_4;
              tmp_BLP.l |= 1;
              s_1_4 = s_1_4 + tmp_BLP.d;
              tmp_BLP.d = v_5;
              tmp_BLP.l |= 1;
              s_1_5 = s_1_5 + tmp_BLP.d;
              tmp_BLP.d = v_6;
              tmp_BLP.l |= 1;
              s_1_6 = s_1_6 + tmp_BLP.d;
              tmp_BLP.d = v_7;
              tmp_BLP.l |= 1;
              s_1_7 = s_1_7 + tmp_BLP.d;
              q_0 = q_0 - s_1_0;
              q_1 = q_1 - s_1_1;
              q_2 = q_2 - s_1_2;
              q_3 = q_3 - s_1_3;
              q_4 = q_4 - s_1_4;
              q_5 = q_5 - s_1_5;
              q_6 = q_6 - s_1_6;
              q_7 = q_7 - s_1_7;
              v_0 = v_0 + q_0;
              v_1 = v_1 + q_1;
              v_2 = v_2 + q_2;
              v_3 = v_3 + q_3;
              v_4 = v_4 + q_4;
              v_5 = v_5 + q_5;
              v_6 = v_6 + q_6;
              v_7 = v_7 + q_7;
              tmp_BLP.d = v_0;
              tmp_BLP.l |= 1;
              s_2_0 = s_2_0 + tmp_BLP.d;
              tmp_BLP.d = v_1;
              tmp_BLP.l |= 1;
              s_2_1 = s_2_1 + tmp_BLP.d;
              tmp_BLP.d = v_2;
              tmp_BLP.l |= 1;
              s_2_2 = s_2_2 + tmp_BLP.d;
              tmp_BLP.d = v_3;
              tmp_BLP.l |= 1;
              s_2_3 = s_2_3 + tmp_BLP.d;
              tmp_BLP.d = v_4;
              tmp_BLP.l |= 1;
              s_2_4 = s_2_4 + tmp_BLP.d;
              tmp_BLP.d = v_5;
              tmp_BLP.l |= 1;
              s_2_5 = s_2_5 + tmp_BLP.d;
              tmp_BLP.d = v_6;
              tmp_BLP.l |= 1;
              s_2_6 = s_2_6 + tmp_BLP.d;
              tmp_BLP.d = v_7;
              tmp_BLP.l |= 1;
              s_2_7 = s_2_7 + tmp_BLP.d;
              i += 2, v_base += 4, y_base += 4;
            }
            if(i + 1 <= n){
              v_0 = v_base[0];
              v_1 = v_base[1];
              y_0 = y_base[0];
              y_1 = y_base[1];
              v_2 = v_1 * y_1 * -1;
              v_3 = v_0 * y_1;
              v_0 = v_0 * y_0;
              v_1 = v_1 * y_0;
              q_0 = s_0_0;
              q_1 = s_0_1;
              q_2 = s_0_2;
              q_3 = s_0_3;
              tmp_BLP.d = v_0;
              tmp_BLP.l |= 1;
              s_0_0 = s_0_0 + tmp_BLP.d;
              tmp_BLP.d = v_1;
              tmp_BLP.l |= 1;
              s_0_1 = s_0_1 + tmp_BLP.d;
              tmp_BLP.d = v_2;
              tmp_BLP.l |= 1;
              s_0_2 = s_0_2 + tmp_BLP.d;
              tmp_BLP.d = v_3;
              tmp_BLP.l |= 1;
              s_0_3 = s_0_3 + tmp_BLP.d;
              q_0 = q_0 - s_0_0;
              q_1 = q_1 - s_0_1;
              q_2 = q_2 - s_0_2;
              q_3 = q_3 - s_0_3;
              v_0 = v_0 + q_0;
              v_1 = v_1 + q_1;
              v_2 = v_2 + q_2;
              v_3 = v_3 + q_3;
              q_0 = s_1_0;
              q_1 = s_1_1;
              q_2 = s_1_2;
              q_3 = s_1_3;
              tmp_BLP.d = v_0;
              tmp_BLP.l |= 1;
              s_1_0 = s_1_0 + tmp_BLP.d;
              tmp_BLP.d = v_1;
              tmp_BLP.l |= 1;
              s_1_1 = s_1_1 + tmp_BLP.d;
              tmp_BLP.d = v_2;
              tmp_BLP.l |= 1;
              s_1_2 = s_1_2 + tmp_BLP.d;
              tmp_BLP.d = v_3;
              tmp_BLP.l |= 1;
              s_1_3 = s_1_3 + tmp_BLP.d;
              q_0 = q_0 - s_1_0;
              q_1 = q_1 - s_1_1;
              q_2 = q_2 - s_1_2;
              q_3 = q_3 - s_1_3;
              v_0 = v_0 + q_0;
              v_1 = v_1 + q_1;
              v_2 = v_2 + q_2;
              v_3 = v_3 + q_3;
              tmp_BLP.d = v_0;
              tmp_BLP.l |= 1;
              s_2_0 = s_2_0 + tmp_BLP.d;
              tmp_BLP.d = v_1;
              tmp_BLP.l |= 1;
              s_2_1 = s_2_1 + tmp_BLP.d;
              tmp_BLP.d = v_2;
              tmp_BLP.l |= 1;
              s_2_2 = s_2_2 + tmp_BLP.d;
              tmp_BLP.d = v_3;
              tmp_BLP.l |= 1;
              s_2_3 = s_2_3 + tmp_BLP.d;
              i += 1, v_base += 2, y_base += 2;
            }
          }else{

            for(i = 0; i + 4 <= n; i += 4, v_base += 8, y_base += (incy * 8)){
              v_0 = v_base[0];
              v_1 = v_base[1];
              v_2 = v_base[2];
              v_3 = v_base[3];
              v_4 = v_base[4];
              v_5 = v_base[5];
              v_6 = v_base[6];
              v_7 = v_base[7];
              y_0 = y_base[0];
              y_1 = y_base[1];
              y_2 = y_base[(incy * 2)];
              y_3 = y_base[((incy * 2) + 1)];
              y_4 = y_base[(incy * 4)];
              y_5 = y_base[((incy * 4) + 1)];
              y_6 = y_base[(incy * 6)];
              y_7 = y_base[((incy * 6) + 1)];
              v_8 = v_1 * y_1 * -1;
              v_9 = v_0 * y_1;
              v_10 = v_3 * y_3 * -1;
              v_11 = v_2 * y_3;
              v_12 = v_5 * y_5 * -1;
              v_13 = v_4 * y_5;
              v_14 = v_7 * y_7 * -1;
              v_15 = v_6 * y_7;
              v_0 = v_0 * y_0;
              v_1 = v_1 * y_0;
              v_2 = v_2 * y_2;
              v_3 = v_3 * y_2;
              v_4 = v_4 * y_4;
              v_5 = v_5 * y_4;
              v_6 = v_6 * y_6;
              v_7 = v_7 * y_6;
              q_0 = s_0_0;
              q_1 = s_0_1;
              q_2 = s_0_2;
              q_3 = s_0_3;
              q_4 = s_0_4;
              q_5 = s_0_5;
              q_6 = s_0_6;
              q_7 = s_0_7;
              tmp_BLP.d = v_0;
              tmp_BLP.l |= 1;
              s_0_0 = s_0_0 + tmp_BLP.d;
              tmp_BLP.d = v_1;
              tmp_BLP.l |= 1;
              s_0_1 = s_0_1 + tmp_BLP.d;
              tmp_BLP.d = v_2;
              tmp_BLP.l |= 1;
              s_0_2 = s_0_2 + tmp_BLP.d;
              tmp_BLP.d = v_3;
              tmp_BLP.l |= 1;
              s_0_3 = s_0_3 + tmp_BLP.d;
              tmp_BLP.d = v_4;
              tmp_BLP.l |= 1;
              s_0_4 = s_0_4 + tmp_BLP.d;
              tmp_BLP.d = v_5;
              tmp_BLP.l |= 1;
              s_0_5 = s_0_5 + tmp_BLP.d;
              tmp_BLP.d = v_6;
              tmp_BLP.l |= 1;
              s_0_6 = s_0_6 + tmp_BLP.d;
              tmp_BLP.d = v_7;
              tmp_BLP.l |= 1;
              s_0_7 = s_0_7 + tmp_BLP.d;
              q_0 = q_0 - s_0_0;
              q_1 = q_1 - s_0_1;
              q_2 = q_2 - s_0_2;
              q_3 = q_3 - s_0_3;
              q_4 = q_4 - s_0_4;
              q_5 = q_5 - s_0_5;
              q_6 = q_6 - s_0_6;
              q_7 = q_7 - s_0_7;
              v_0 = v_0 + q_0;
              v_1 = v_1 + q_1;
              v_2 = v_2 + q_2;
              v_3 = v_3 + q_3;
              v_4 = v_4 + q_4;
              v_5 = v_5 + q_5;
              v_6 = v_6 + q_6;
              v_7 = v_7 + q_7;
              q_0 = s_1_0;
              q_1 = s_1_1;
              q_2 = s_1_2;
              q_3 = s_1_3;
              q_4 = s_1_4;
              q_5 = s_1_5;
              q_6 = s_1_6;
              q_7 = s_1_7;
              tmp_BLP.d = v_0;
              tmp_BLP.l |= 1;
              s_1_0 = s_1_0 + tmp_BLP.d;
              tmp_BLP.d = v_1;
              tmp_BLP.l |= 1;
              s_1_1 = s_1_1 + tmp_BLP.d;
              tmp_BLP.d = v_2;
              tmp_BLP.l |= 1;
              s_1_2 = s_1_2 + tmp_BLP.d;
              tmp_BLP.d = v_3;
              tmp_BLP.l |= 1;
              s_1_3 = s_1_3 + tmp_BLP.d;
              tmp_BLP.d = v_4;
              tmp_BLP.l |= 1;
              s_1_4 = s_1_4 + tmp_BLP.d;
              tmp_BLP.d = v_5;
              tmp_BLP.l |= 1;
              s_1_5 = s_1_5 + tmp_BLP.d;
              tmp_BLP.d = v_6;
              tmp_BLP.l |= 1;
              s_1_6 = s_1_6 + tmp_BLP.d;
              tmp_BLP.d = v_7;
              tmp_BLP.l |= 1;
              s_1_7 = s_1_7 + tmp_BLP.d;
              q_0 = q_0 - s_1_0;
              q_1 = q_1 - s_1_1;
              q_2 = q_2 - s_1_2;
              q_3 = q_3 - s_1_3;
              q_4 = q_4 - s_1_4;
              q_5 = q_5 - s_1_5;
              q_6 = q_6 - s_1_6;
              q_7 = q_7 - s_1_7;
              v_0 = v_0 + q_0;
              v_1 = v_1 + q_1;
              v_2 = v_2 + q_2;
              v_3 = v_3 + q_3;
              v_4 = v_4 + q_4;
              v_5 = v_5 + q_5;
              v_6 = v_6 + q_6;
              v_7 = v_7 + q_7;
              tmp_BLP.d = v_0;
              tmp_BLP.l |= 1;
              s_2_0 = s_2_0 + tmp_BLP.d;
              tmp_BLP.d = v_1;
              tmp_BLP.l |= 1;
              s_2_1 = s_2_1 + tmp_BLP.d;
              tmp_BLP.d = v_2;
              tmp_BLP.l |= 1;
              s_2_2 = s_2_2 + tmp_BLP.d;
              tmp_BLP.d = v_3;
              tmp_BLP.l |= 1;
              s_2_3 = s_2_3 + tmp_BLP.d;
              tmp_BLP.d = v_4;
              tmp_BLP.l |= 1;
              s_2_4 = s_2_4 + tmp_BLP.d;
              tmp_BLP.d = v_5;
              tmp_BLP.l |= 1;
              s_2_5 = s_2_5 + tmp_BLP.d;
              tmp_BLP.d = v_6;
              tmp_BLP.l |= 1;
              s_2_6 = s_2_6 + tmp_BLP.d;
              tmp_BLP.d = v_7;
              tmp_BLP.l |= 1;
              s_2_7 = s_2_7 + tmp_BLP.d;
              q_0 = s_0_0;
              q_1 = s_0_1;
              q_2 = s_0_2;
              q_3 = s_0_3;
              q_4 = s_0_4;
              q_5 = s_0_5;
              q_6 = s_0_6;
              q_7 = s_0_7;
              tmp_BLP.d = v_8;
              tmp_BLP.l |= 1;
              s_0_0 = s_0_0 + tmp_BLP.d;
              tmp_BLP.d = v_9;
              tmp_BLP.l |= 1;
              s_0_1 = s_0_1 + tmp_BLP.d;
              tmp_BLP.d = v_10;
              tmp_BLP.l |= 1;
              s_0_2 = s_0_2 + tmp_BLP.d;
              tmp_BLP.d = v_11;
              tmp_BLP.l |= 1;
              s_0_3 = s_0_3 + tmp_BLP.d;
              tmp_BLP.d = v_12;
              tmp_BLP.l |= 1;
              s_0_4 = s_0_4 + tmp_BLP.d;
              tmp_BLP.d = v_13;
              tmp_BLP.l |= 1;
              s_0_5 = s_0_5 + tmp_BLP.d;
              tmp_BLP.d = v_14;
              tmp_BLP.l |= 1;
              s_0_6 = s_0_6 + tmp_BLP.d;
              tmp_BLP.d = v_15;
              tmp_BLP.l |= 1;
              s_0_7 = s_0_7 + tmp_BLP.d;
              q_0 = q_0 - s_0_0;
              q_1 = q_1 - s_0_1;
              q_2 = q_2 - s_0_2;
              q_3 = q_3 - s_0_3;
              q_4 = q_4 - s_0_4;
              q_5 = q_5 - s_0_5;
              q_6 = q_6 - s_0_6;
              q_7 = q_7 - s_0_7;
              v_8 = v_8 + q_0;
              v_9 = v_9 + q_1;
              v_10 = v_10 + q_2;
              v_11 = v_11 + q_3;
              v_12 = v_12 + q_4;
              v_13 = v_13 + q_5;
              v_14 = v_14 + q_6;
              v_15 = v_15 + q_7;
              q_0 = s_1_0;
              q_1 = s_1_1;
              q_2 = s_1_2;
              q_3 = s_1_3;
              q_4 = s_1_4;
              q_5 = s_1_5;
              q_6 = s_1_6;
              q_7 = s_1_7;
              tmp_BLP.d = v_8;
              tmp_BLP.l |= 1;
              s_1_0 = s_1_0 + tmp_BLP.d;
              tmp_BLP.d = v_9;
              tmp_BLP.l |= 1;
              s_1_1 = s_1_1 + tmp_BLP.d;
              tmp_BLP.d = v_10;
              tmp_BLP.l |= 1;
              s_1_2 = s_1_2 + tmp_BLP.d;
              tmp_BLP.d = v_11;
              tmp_BLP.l |= 1;
              s_1_3 = s_1_3 + tmp_BLP.d;
              tmp_BLP.d = v_12;
              tmp_BLP.l |= 1;
              s_1_4 = s_1_4 + tmp_BLP.d;
              tmp_BLP.d = v_13;
              tmp_BLP.l |= 1;
              s_1_5 = s_1_5 + tmp_BLP.d;
              tmp_BLP.d = v_14;
              tmp_BLP.l |= 1;
              s_1_6 = s_1_6 + tmp_BLP.d;
              tmp_BLP.d = v_15;
              tmp_BLP.l |= 1;
              s_1_7 = s_1_7 + tmp_BLP.d;
              q_0 = q_0 - s_1_0;
              q_1 = q_1 - s_1_1;
              q_2 = q_2 - s_1_2;
              q_3 = q_3 - s_1_3;
              q_4 = q_4 - s_1_4;
              q_5 = q_5 - s_1_5;
              q_6 = q_6 - s_1_6;
              q_7 = q_7 - s_1_7;
              v_8 = v_8 + q_0;
              v_9 = v_9 + q_1;
              v_10 = v_10 + q_2;
              v_11 = v_11 + q_3;
              v_12 = v_12 + q_4;
              v_13 = v_13 + q_5;
              v_14 = v_14 + q_6;
              v_15 = v_15 + q_7;
              tmp_BLP.d = v_8;
              tmp_BLP.l |= 1;
              s_2_0 = s_2_0 + tmp_BLP.d;
              tmp_BLP.d = v_9;
              tmp_BLP.l |= 1;
              s_2_1 = s_2_1 + tmp_BLP.d;
              tmp_BLP.d = v_10;
              tmp_BLP.l |= 1;
              s_2_2 = s_2_2 + tmp_BLP.d;
              tmp_BLP.d = v_11;
              tmp_BLP.l |= 1;
              s_2_3 = s_2_3 + tmp_BLP.d;
              tmp_BLP.d = v_12;
              tmp_BLP.l |= 1;
              s_2_4 = s_2_4 + tmp_BLP.d;
              tmp_BLP.d = v_13;
              tmp_BLP.l |= 1;
              s_2_5 = s_2_5 + tmp_BLP.d;
              tmp_BLP.d = v_14;
              tmp_BLP.l |= 1;
              s_2_6 = s_2_6 + tmp_BLP.d;
              tmp_BLP.d = v_15;
              tmp_BLP.l |= 1;
              s_2_7 = s_2_7 + tmp_BLP.d;
            }
            if(i + 2 <= n){
              v_0 = v_base[0];
              v_1 = v_base[1];
              v_2 = v_base[2];
              v_3 = v_base[3];
              y_0 = y_base[0];
              y_1 = y_base[1];
              y_2 = y_base[(incy * 2)];
              y_3 = y_base[((incy * 2) + 1)];
              v_4 = v_1 * y_1 * -1;
              v_5 = v_0 * y_1;
              v_6 = v_3 * y_3 * -1;
              v_7 = v_2 * y_3;
              v_0 = v_0 * y_0;
              v_1 = v_1 * y_0;
              v_2 = v_2 * y_2;
              v_3 = v_3 * y_2;
              q_0 = s_0_0;
              q_1 = s_0_1;
              q_2 = s_0_2;
              q_3 = s_0_3;
              q_4 = s_0_4;
              q_5 = s_0_5;
              q_6 = s_0_6;
              q_7 = s_0_7;
              tmp_BLP.d = v_0;
              tmp_BLP.l |= 1;
              s_0_0 = s_0_0 + tmp_BLP.d;
              tmp_BLP.d = v_1;
              tmp_BLP.l |= 1;
              s_0_1 = s_0_1 + tmp_BLP.d;
              tmp_BLP.d = v_2;
              tmp_BLP.l |= 1;
              s_0_2 = s_0_2 + tmp_BLP.d;
              tmp_BLP.d = v_3;
              tmp_BLP.l |= 1;
              s_0_3 = s_0_3 + tmp_BLP.d;
              tmp_BLP.d = v_4;
              tmp_BLP.l |= 1;
              s_0_4 = s_0_4 + tmp_BLP.d;
              tmp_BLP.d = v_5;
              tmp_BLP.l |= 1;
              s_0_5 = s_0_5 + tmp_BLP.d;
              tmp_BLP.d = v_6;
              tmp_BLP.l |= 1;
              s_0_6 = s_0_6 + tmp_BLP.d;
              tmp_BLP.d = v_7;
              tmp_BLP.l |= 1;
              s_0_7 = s_0_7 + tmp_BLP.d;
              q_0 = q_0 - s_0_0;
              q_1 = q_1 - s_0_1;
              q_2 = q_2 - s_0_2;
              q_3 = q_3 - s_0_3;
              q_4 = q_4 - s_0_4;
              q_5 = q_5 - s_0_5;
              q_6 = q_6 - s_0_6;
              q_7 = q_7 - s_0_7;
              v_0 = v_0 + q_0;
              v_1 = v_1 + q_1;
              v_2 = v_2 + q_2;
              v_3 = v_3 + q_3;
              v_4 = v_4 + q_4;
              v_5 = v_5 + q_5;
              v_6 = v_6 + q_6;
              v_7 = v_7 + q_7;
              q_0 = s_1_0;
              q_1 = s_1_1;
              q_2 = s_1_2;
              q_3 = s_1_3;
              q_4 = s_1_4;
              q_5 = s_1_5;
              q_6 = s_1_6;
              q_7 = s_1_7;
              tmp_BLP.d = v_0;
              tmp_BLP.l |= 1;
              s_1_0 = s_1_0 + tmp_BLP.d;
              tmp_BLP.d = v_1;
              tmp_BLP.l |= 1;
              s_1_1 = s_1_1 + tmp_BLP.d;
              tmp_BLP.d = v_2;
              tmp_BLP.l |= 1;
              s_1_2 = s_1_2 + tmp_BLP.d;
              tmp_BLP.d = v_3;
              tmp_BLP.l |= 1;
              s_1_3 = s_1_3 + tmp_BLP.d;
              tmp_BLP.d = v_4;
              tmp_BLP.l |= 1;
              s_1_4 = s_1_4 + tmp_BLP.d;
              tmp_BLP.d = v_5;
              tmp_BLP.l |= 1;
              s_1_5 = s_1_5 + tmp_BLP.d;
              tmp_BLP.d = v_6;
              tmp_BLP.l |= 1;
              s_1_6 = s_1_6 + tmp_BLP.d;
              tmp_BLP.d = v_7;
              tmp_BLP.l |= 1;
              s_1_7 = s_1_7 + tmp_BLP.d;
              q_0 = q_0 - s_1_0;
              q_1 = q_1 - s_1_1;
              q_2 = q_2 - s_1_2;
              q_3 = q_3 - s_1_3;
              q_4 = q_4 - s_1_4;
              q_5 = q_5 - s_1_5;
              q_6 = q_6 - s_1_6;
              q_7 = q_7 - s_1_7;
              v_0 = v_0 + q_0;
              v_1 = v_1 + q_1;
              v_2 = v_2 + q_2;
              v_3 = v_3 + q_3;
              v_4 = v_4 + q_4;
              v_5 = v_5 + q_5;
              v_6 = v_6 + q_6;
              v_7 = v_7 + q_7;
              tmp_BLP.d = v_0;
              tmp_BLP.l |= 1;
              s_2_0 = s_2_0 + tmp_BLP.d;
              tmp_BLP.d = v_1;
              tmp_BLP.l |= 1;
              s_2_1 = s_2_1 + tmp_BLP.d;
              tmp_BLP.d = v_2;
              tmp_BLP.l |= 1;
              s_2_2 = s_2_2 + tmp_BLP.d;
              tmp_BLP.d = v_3;
              tmp_BLP.l |= 1;
              s_2_3 = s_2_3 + tmp_BLP.d;
              tmp_BLP.d = v_4;
              tmp_BLP.l |= 1;
              s_2_4 = s_2_4 + tmp_BLP.d;
              tmp_BLP.d = v_5;
              tmp_BLP.l |= 1;
              s_2_5 = s_2_5 + tmp_BLP.d;
              tmp_BLP.d = v_6;
              tmp_BLP.l |= 1;
              s_2_6 = s_2_6 + tmp_BLP.d;
              tmp_BLP.d = v_7;
              tmp_BLP.l |= 1;
              s_2_7 = s_2_7 + tmp_BLP.d;
              i += 2, v_base += 4, y_base += (incy * 4);
            }
            if(i + 1 <= n){
              v_0 = v_base[0];
              v_1 = v_base[1];
              y_0 = y_base[0];
              y_1 = y_base[1];
              v_2 = v_1 * y_1 * -1;
              v_3 = v_0 * y_1;
              v_0 = v_0 * y_0;
              v_1 = v_1 * y_0;
              q_0 = s_0_0;
              q_1 = s_0_1;
              q_2 = s_0_2;
              q_3 = s_0_3;
              tmp_BLP.d = v_0;
              tmp_BLP.l |= 1;
              s_0_0 = s_0_0 + tmp_BLP.d;
              tmp_BLP.d = v_1;
              tmp_BLP.l |= 1;
              s_0_1 = s_0_1 + tmp_BLP.d;
              tmp_BLP.d = v_2;
              tmp_BLP.l |= 1;
              s_0_2 = s_0_2 + tmp_BLP.d;
              tmp_BLP.d = v_3;
              tmp_BLP.l |= 1;
              s_0_3 = s_0_3 + tmp_BLP.d;
              q_0 = q_0 - s_0_0;
              q_1 = q_1 - s_0_1;
              q_2 = q_2 - s_0_2;
              q_3 = q_3 - s_0_3;
              v_0 = v_0 + q_0;
              v_1 = v_1 + q_1;
              v_2 = v_2 + q_2;
              v_3 = v_3 + q_3;
              q_0 = s_1_0;
              q_1 = s_1_1;
              q_2 = s_1_2;
              q_3 = s_1_3;
              tmp_BLP.d = v_0;
              tmp_BLP.l |= 1;
              s_1_0 = s_1_0 + tmp_BLP.d;
              tmp_BLP.d = v_1;
              tmp_BLP.l |= 1;
              s_1_1 = s_1_1 + tmp_BLP.d;
              tmp_BLP.d = v_2;
              tmp_BLP.l |= 1;
              s_1_2 = s_1_2 + tmp_BLP.d;
              tmp_BLP.d = v_3;
              tmp_BLP.l |= 1;
              s_1_3 = s_1_3 + tmp_BLP.d;
              q_0 = q_0 - s_1_0;
              q_1 = q_1 - s_1_1;
              q_2 = q_2 - s_1_2;
              q_3 = q_3 - s_1_3;
              v_0 = v_0 + q_0;
              v_1 = v_1 + q_1;
              v_2 = v_2 + q_2;
              v_3 = v_3 + q_3;
              tmp_BLP.d = v_0;
              tmp_BLP.l |= 1;
              s_2_0 = s_2_0 + tmp_BLP.d;
              tmp_BLP.d = v_1;
              tmp_BLP.l |= 1;
              s_2_1 = s_2_1 + tmp_BLP.d;
              tmp_BLP.d = v_2;
              tmp_BLP.l |= 1;
              s_2_2 = s_2_2 + tmp_BLP.d;
              tmp_BLP.d = v_3;
              tmp_BLP.l |= 1;
              s_2_3 = s_2_3 + tmp_BLP.d;
              i += 1, v_base += 2, y_base += (incy * 2);
            }
          }
        }else{
          if(incy == 1){

            for(i = 0; i + 4 <= n; i += 4, v_base += (incv * 8), y_base += 8){
              v_0 = v_base[0];
              v_1 = v_base[1];
              v_2 = v_base[(incv * 2)];
              v_3 = v_base[((incv * 2) + 1)];
              v_4 = v_base[(incv * 4)];
              v_5 = v_base[((incv * 4) + 1)];
              v_6 = v_base[(incv * 6)];
              v_7 = v_base[((incv * 6) + 1)];
              y_0 = y_base[0];
              y_1 = y_base[1];
              y_2 = y_base[2];
              y_3 = y_base[3];
              y_4 = y_base[4];
              y_5 = y_base[5];
              y_6 = y_base[6];
              y_7 = y_base[7];
              v_8 = v_1 * y_1 * -1;
              v_9 = v_0 * y_1;
              v_10 = v_3 * y_3 * -1;
              v_11 = v_2 * y_3;
              v_12 = v_5 * y_5 * -1;
              v_13 = v_4 * y_5;
              v_14 = v_7 * y_7 * -1;
              v_15 = v_6 * y_7;
              v_0 = v_0 * y_0;
              v_1 = v_1 * y_0;
              v_2 = v_2 * y_2;
              v_3 = v_3 * y_2;
              v_4 = v_4 * y_4;
              v_5 = v_5 * y_4;
              v_6 = v_6 * y_6;
              v_7 = v_7 * y_6;
              q_0 = s_0_0;
              q_1 = s_0_1;
              q_2 = s_0_2;
              q_3 = s_0_3;
              q_4 = s_0_4;
              q_5 = s_0_5;
              q_6 = s_0_6;
              q_7 = s_0_7;
              tmp_BLP.d = v_0;
              tmp_BLP.l |= 1;
              s_0_0 = s_0_0 + tmp_BLP.d;
              tmp_BLP.d = v_1;
              tmp_BLP.l |= 1;
              s_0_1 = s_0_1 + tmp_BLP.d;
              tmp_BLP.d = v_2;
              tmp_BLP.l |= 1;
              s_0_2 = s_0_2 + tmp_BLP.d;
              tmp_BLP.d = v_3;
              tmp_BLP.l |= 1;
              s_0_3 = s_0_3 + tmp_BLP.d;
              tmp_BLP.d = v_4;
              tmp_BLP.l |= 1;
              s_0_4 = s_0_4 + tmp_BLP.d;
              tmp_BLP.d = v_5;
              tmp_BLP.l |= 1;
              s_0_5 = s_0_5 + tmp_BLP.d;
              tmp_BLP.d = v_6;
              tmp_BLP.l |= 1;
              s_0_6 = s_0_6 + tmp_BLP.d;
              tmp_BLP.d = v_7;
              tmp_BLP.l |= 1;
              s_0_7 = s_0_7 + tmp_BLP.d;
              q_0 = q_0 - s_0_0;
              q_1 = q_1 - s_0_1;
              q_2 = q_2 - s_0_2;
              q_3 = q_3 - s_0_3;
              q_4 = q_4 - s_0_4;
              q_5 = q_5 - s_0_5;
              q_6 = q_6 - s_0_6;
              q_7 = q_7 - s_0_7;
              v_0 = v_0 + q_0;
              v_1 = v_1 + q_1;
              v_2 = v_2 + q_2;
              v_3 = v_3 + q_3;
              v_4 = v_4 + q_4;
              v_5 = v_5 + q_5;
              v_6 = v_6 + q_6;
              v_7 = v_7 + q_7;
              q_0 = s_1_0;
              q_1 = s_1_1;
              q_2 = s_1_2;
              q_3 = s_1_3;
              q_4 = s_1_4;
              q_5 = s_1_5;
              q_6 = s_1_6;
              q_7 = s_1_7;
              tmp_BLP.d = v_0;
              tmp_BLP.l |= 1;
              s_1_0 = s_1_0 + tmp_BLP.d;
              tmp_BLP.d = v_1;
              tmp_BLP.l |= 1;
              s_1_1 = s_1_1 + tmp_BLP.d;
              tmp_BLP.d = v_2;
              tmp_BLP.l |= 1;
              s_1_2 = s_1_2 + tmp_BLP.d;
              tmp_BLP.d = v_3;
              tmp_BLP.l |= 1;
              s_1_3 = s_1_3 + tmp_BLP.d;
              tmp_BLP.d = v_4;
              tmp_BLP.l |= 1;
              s_1_4 = s_1_4 + tmp_BLP.d;
              tmp_BLP.d = v_5;
              tmp_BLP.l |= 1;
              s_1_5 = s_1_5 + tmp_BLP.d;
              tmp_BLP.d = v_6;
              tmp_BLP.l |= 1;
              s_1_6 = s_1_6 + tmp_BLP.d;
              tmp_BLP.d = v_7;
              tmp_BLP.l |= 1;
              s_1_7 = s_1_7 + tmp_BLP.d;
              q_0 = q_0 - s_1_0;
              q_1 = q_1 - s_1_1;
              q_2 = q_2 - s_1_2;
              q_3 = q_3 - s_1_3;
              q_4 = q_4 - s_1_4;
              q_5 = q_5 - s_1_5;
              q_6 = q_6 - s_1_6;
              q_7 = q_7 - s_1_7;
              v_0 = v_0 + q_0;
              v_1 = v_1 + q_1;
              v_2 = v_2 + q_2;
              v_3 = v_3 + q_3;
              v_4 = v_4 + q_4;
              v_5 = v_5 + q_5;
              v_6 = v_6 + q_6;
              v_7 = v_7 + q_7;
              tmp_BLP.d = v_0;
              tmp_BLP.l |= 1;
              s_2_0 = s_2_0 + tmp_BLP.d;
              tmp_BLP.d = v_1;
              tmp_BLP.l |= 1;
              s_2_1 = s_2_1 + tmp_BLP.d;
              tmp_BLP.d = v_2;
              tmp_BLP.l |= 1;
              s_2_2 = s_2_2 + tmp_BLP.d;
              tmp_BLP.d = v_3;
              tmp_BLP.l |= 1;
              s_2_3 = s_2_3 + tmp_BLP.d;
              tmp_BLP.d = v_4;
              tmp_BLP.l |= 1;
              s_2_4 = s_2_4 + tmp_BLP.d;
              tmp_BLP.d = v_5;
              tmp_BLP.l |= 1;
              s_2_5 = s_2_5 + tmp_BLP.d;
              tmp_BLP.d = v_6;
              tmp_BLP.l |= 1;
              s_2_6 = s_2_6 + tmp_BLP.d;
              tmp_BLP.d = v_7;
              tmp_BLP.l |= 1;
              s_2_7 = s_2_7 + tmp_BLP.d;
              q_0 = s_0_0;
              q_1 = s_0_1;
              q_2 = s_0_2;
              q_3 = s_0_3;
              q_4 = s_0_4;
              q_5 = s_0_5;
              q_6 = s_0_6;
              q_7 = s_0_7;
              tmp_BLP.d = v_8;
              tmp_BLP.l |= 1;
              s_0_0 = s_0_0 + tmp_BLP.d;
              tmp_BLP.d = v_9;
              tmp_BLP.l |= 1;
              s_0_1 = s_0_1 + tmp_BLP.d;
              tmp_BLP.d = v_10;
              tmp_BLP.l |= 1;
              s_0_2 = s_0_2 + tmp_BLP.d;
              tmp_BLP.d = v_11;
              tmp_BLP.l |= 1;
              s_0_3 = s_0_3 + tmp_BLP.d;
              tmp_BLP.d = v_12;
              tmp_BLP.l |= 1;
              s_0_4 = s_0_4 + tmp_BLP.d;
              tmp_BLP.d = v_13;
              tmp_BLP.l |= 1;
              s_0_5 = s_0_5 + tmp_BLP.d;
              tmp_BLP.d = v_14;
              tmp_BLP.l |= 1;
              s_0_6 = s_0_6 + tmp_BLP.d;
              tmp_BLP.d = v_15;
              tmp_BLP.l |= 1;
              s_0_7 = s_0_7 + tmp_BLP.d;
              q_0 = q_0 - s_0_0;
              q_1 = q_1 - s_0_1;
              q_2 = q_2 - s_0_2;
              q_3 = q_3 - s_0_3;
              q_4 = q_4 - s_0_4;
              q_5 = q_5 - s_0_5;
              q_6 = q_6 - s_0_6;
              q_7 = q_7 - s_0_7;
              v_8 = v_8 + q_0;
              v_9 = v_9 + q_1;
              v_10 = v_10 + q_2;
              v_11 = v_11 + q_3;
              v_12 = v_12 + q_4;
              v_13 = v_13 + q_5;
              v_14 = v_14 + q_6;
              v_15 = v_15 + q_7;
              q_0 = s_1_0;
              q_1 = s_1_1;
              q_2 = s_1_2;
              q_3 = s_1_3;
              q_4 = s_1_4;
              q_5 = s_1_5;
              q_6 = s_1_6;
              q_7 = s_1_7;
              tmp_BLP.d = v_8;
              tmp_BLP.l |= 1;
              s_1_0 = s_1_0 + tmp_BLP.d;
              tmp_BLP.d = v_9;
              tmp_BLP.l |= 1;
              s_1_1 = s_1_1 + tmp_BLP.d;
              tmp_BLP.d = v_10;
              tmp_BLP.l |= 1;
              s_1_2 = s_1_2 + tmp_BLP.d;
              tmp_BLP.d = v_11;
              tmp_BLP.l |= 1;
              s_1_3 = s_1_3 + tmp_BLP.d;
              tmp_BLP.d = v_12;
              tmp_BLP.l |= 1;
              s_1_4 = s_1_4 + tmp_BLP.d;
              tmp_BLP.d = v_13;
              tmp_BLP.l |= 1;
              s_1_5 = s_1_5 + tmp_BLP.d;
              tmp_BLP.d = v_14;
              tmp_BLP.l |= 1;
              s_1_6 = s_1_6 + tmp_BLP.d;
              tmp_BLP.d = v_15;
              tmp_BLP.l |= 1;
              s_1_7 = s_1_7 + tmp_BLP.d;
              q_0 = q_0 - s_1_0;
              q_1 = q_1 - s_1_1;
              q_2 = q_2 - s_1_2;
              q_3 = q_3 - s_1_3;
              q_4 = q_4 - s_1_4;
              q_5 = q_5 - s_1_5;
              q_6 = q_6 - s_1_6;
              q_7 = q_7 - s_1_7;
              v_8 = v_8 + q_0;
              v_9 = v_9 + q_1;
              v_10 = v_10 + q_2;
              v_11 = v_11 + q_3;
              v_12 = v_12 + q_4;
              v_13 = v_13 + q_5;
              v_14 = v_14 + q_6;
              v_15 = v_15 + q_7;
              tmp_BLP.d = v_8;
              tmp_BLP.l |= 1;
              s_2_0 = s_2_0 + tmp_BLP.d;
              tmp_BLP.d = v_9;
              tmp_BLP.l |= 1;
              s_2_1 = s_2_1 + tmp_BLP.d;
              tmp_BLP.d = v_10;
              tmp_BLP.l |= 1;
              s_2_2 = s_2_2 + tmp_BLP.d;
              tmp_BLP.d = v_11;
              tmp_BLP.l |= 1;
              s_2_3 = s_2_3 + tmp_BLP.d;
              tmp_BLP.d = v_12;
              tmp_BLP.l |= 1;
              s_2_4 = s_2_4 + tmp_BLP.d;
              tmp_BLP.d = v_13;
              tmp_BLP.l |= 1;
              s_2_5 = s_2_5 + tmp_BLP.d;
              tmp_BLP.d = v_14;
              tmp_BLP.l |= 1;
              s_2_6 = s_2_6 + tmp_BLP.d;
              tmp_BLP.d = v_15;
              tmp_BLP.l |= 1;
              s_2_7 = s_2_7 + tmp_BLP.d;
            }
            if(i + 2 <= n){
              v_0 = v_base[0];
              v_1 = v_base[1];
              v_2 = v_base[(incv * 2)];
              v_3 = v_base[((incv * 2) + 1)];
              y_0 = y_base[0];
              y_1 = y_base[1];
              y_2 = y_base[2];
              y_3 = y_base[3];
              v_4 = v_1 * y_1 * -1;
              v_5 = v_0 * y_1;
              v_6 = v_3 * y_3 * -1;
              v_7 = v_2 * y_3;
              v_0 = v_0 * y_0;
              v_1 = v_1 * y_0;
              v_2 = v_2 * y_2;
              v_3 = v_3 * y_2;
              q_0 = s_0_0;
              q_1 = s_0_1;
              q_2 = s_0_2;
              q_3 = s_0_3;
              q_4 = s_0_4;
              q_5 = s_0_5;
              q_6 = s_0_6;
              q_7 = s_0_7;
              tmp_BLP.d = v_0;
              tmp_BLP.l |= 1;
              s_0_0 = s_0_0 + tmp_BLP.d;
              tmp_BLP.d = v_1;
              tmp_BLP.l |= 1;
              s_0_1 = s_0_1 + tmp_BLP.d;
              tmp_BLP.d = v_2;
              tmp_BLP.l |= 1;
              s_0_2 = s_0_2 + tmp_BLP.d;
              tmp_BLP.d = v_3;
              tmp_BLP.l |= 1;
              s_0_3 = s_0_3 + tmp_BLP.d;
              tmp_BLP.d = v_4;
              tmp_BLP.l |= 1;
              s_0_4 = s_0_4 + tmp_BLP.d;
              tmp_BLP.d = v_5;
              tmp_BLP.l |= 1;
              s_0_5 = s_0_5 + tmp_BLP.d;
              tmp_BLP.d = v_6;
              tmp_BLP.l |= 1;
              s_0_6 = s_0_6 + tmp_BLP.d;
              tmp_BLP.d = v_7;
              tmp_BLP.l |= 1;
              s_0_7 = s_0_7 + tmp_BLP.d;
              q_0 = q_0 - s_0_0;
              q_1 = q_1 - s_0_1;
              q_2 = q_2 - s_0_2;
              q_3 = q_3 - s_0_3;
              q_4 = q_4 - s_0_4;
              q_5 = q_5 - s_0_5;
              q_6 = q_6 - s_0_6;
              q_7 = q_7 - s_0_7;
              v_0 = v_0 + q_0;
              v_1 = v_1 + q_1;
              v_2 = v_2 + q_2;
              v_3 = v_3 + q_3;
              v_4 = v_4 + q_4;
              v_5 = v_5 + q_5;
              v_6 = v_6 + q_6;
              v_7 = v_7 + q_7;
              q_0 = s_1_0;
              q_1 = s_1_1;
              q_2 = s_1_2;
              q_3 = s_1_3;
              q_4 = s_1_4;
              q_5 = s_1_5;
              q_6 = s_1_6;
              q_7 = s_1_7;
              tmp_BLP.d = v_0;
              tmp_BLP.l |= 1;
              s_1_0 = s_1_0 + tmp_BLP.d;
              tmp_BLP.d = v_1;
              tmp_BLP.l |= 1;
              s_1_1 = s_1_1 + tmp_BLP.d;
              tmp_BLP.d = v_2;
              tmp_BLP.l |= 1;
              s_1_2 = s_1_2 + tmp_BLP.d;
              tmp_BLP.d = v_3;
              tmp_BLP.l |= 1;
              s_1_3 = s_1_3 + tmp_BLP.d;
              tmp_BLP.d = v_4;
              tmp_BLP.l |= 1;
              s_1_4 = s_1_4 + tmp_BLP.d;
              tmp_BLP.d = v_5;
              tmp_BLP.l |= 1;
              s_1_5 = s_1_5 + tmp_BLP.d;
              tmp_BLP.d = v_6;
              tmp_BLP.l |= 1;
              s_1_6 = s_1_6 + tmp_BLP.d;
              tmp_BLP.d = v_7;
              tmp_BLP.l |= 1;
              s_1_7 = s_1_7 + tmp_BLP.d;
              q_0 = q_0 - s_1_0;
              q_1 = q_1 - s_1_1;
              q_2 = q_2 - s_1_2;
              q_3 = q_3 - s_1_3;
              q_4 = q_4 - s_1_4;
              q_5 = q_5 - s_1_5;
              q_6 = q_6 - s_1_6;
              q_7 = q_7 - s_1_7;
              v_0 = v_0 + q_0;
              v_1 = v_1 + q_1;
              v_2 = v_2 + q_2;
              v_3 = v_3 + q_3;
              v_4 = v_4 + q_4;
              v_5 = v_5 + q_5;
              v_6 = v_6 + q_6;
              v_7 = v_7 + q_7;
              tmp_BLP.d = v_0;
              tmp_BLP.l |= 1;
              s_2_0 = s_2_0 + tmp_BLP.d;
              tmp_BLP.d = v_1;
              tmp_BLP.l |= 1;
              s_2_1 = s_2_1 + tmp_BLP.d;
              tmp_BLP.d = v_2;
              tmp_BLP.l |= 1;
              s_2_2 = s_2_2 + tmp_BLP.d;
              tmp_BLP.d = v_3;
              tmp_BLP.l |= 1;
              s_2_3 = s_2_3 + tmp_BLP.d;
              tmp_BLP.d = v_4;
              tmp_BLP.l |= 1;
              s_2_4 = s_2_4 + tmp_BLP.d;
              tmp_BLP.d = v_5;
              tmp_BLP.l |= 1;
              s_2_5 = s_2_5 + tmp_BLP.d;
              tmp_BLP.d = v_6;
              tmp_BLP.l |= 1;
              s_2_6 = s_2_6 + tmp_BLP.d;
              tmp_BLP.d = v_7;
              tmp_BLP.l |= 1;
              s_2_7 = s_2_7 + tmp_BLP.d;
              i += 2, v_base += (incv * 4), y_base += 4;
            }
            if(i + 1 <= n){
              v_0 = v_base[0];
              v_1 = v_base[1];
              y_0 = y_base[0];
              y_1 = y_base[1];
              v_2 = v_1 * y_1 * -1;
              v_3 = v_0 * y_1;
              v_0 = v_0 * y_0;
              v_1 = v_1 * y_0;
              q_0 = s_0_0;
              q_1 = s_0_1;
              q_2 = s_0_2;
              q_3 = s_0_3;
              tmp_BLP.d = v_0;
              tmp_BLP.l |= 1;
              s_0_0 = s_0_0 + tmp_BLP.d;
              tmp_BLP.d = v_1;
              tmp_BLP.l |= 1;
              s_0_1 = s_0_1 + tmp_BLP.d;
              tmp_BLP.d = v_2;
              tmp_BLP.l |= 1;
              s_0_2 = s_0_2 + tmp_BLP.d;
              tmp_BLP.d = v_3;
              tmp_BLP.l |= 1;
              s_0_3 = s_0_3 + tmp_BLP.d;
              q_0 = q_0 - s_0_0;
              q_1 = q_1 - s_0_1;
              q_2 = q_2 - s_0_2;
              q_3 = q_3 - s_0_3;
              v_0 = v_0 + q_0;
              v_1 = v_1 + q_1;
              v_2 = v_2 + q_2;
              v_3 = v_3 + q_3;
              q_0 = s_1_0;
              q_1 = s_1_1;
              q_2 = s_1_2;
              q_3 = s_1_3;
              tmp_BLP.d = v_0;
              tmp_BLP.l |= 1;
              s_1_0 = s_1_0 + tmp_BLP.d;
              tmp_BLP.d = v_1;
              tmp_BLP.l |= 1;
              s_1_1 = s_1_1 + tmp_BLP.d;
              tmp_BLP.d = v_2;
              tmp_BLP.l |= 1;
              s_1_2 = s_1_2 + tmp_BLP.d;
              tmp_BLP.d = v_3;
              tmp_BLP.l |= 1;
              s_1_3 = s_1_3 + tmp_BLP.d;
              q_0 = q_0 - s_1_0;
              q_1 = q_1 - s_1_1;
              q_2 = q_2 - s_1_2;
              q_3 = q_3 - s_1_3;
              v_0 = v_0 + q_0;
              v_1 = v_1 + q_1;
              v_2 = v_2 + q_2;
              v_3 = v_3 + q_3;
              tmp_BLP.d = v_0;
              tmp_BLP.l |= 1;
              s_2_0 = s_2_0 + tmp_BLP.d;
              tmp_BLP.d = v_1;
              tmp_BLP.l |= 1;
              s_2_1 = s_2_1 + tmp_BLP.d;
              tmp_BLP.d = v_2;
              tmp_BLP.l |= 1;
              s_2_2 = s_2_2 + tmp_BLP.d;
              tmp_BLP.d = v_3;
              tmp_BLP.l |= 1;
              s_2_3 = s_2_3 + tmp_BLP.d;
              i += 1, v_base += (incv * 2), y_base += 2;
            }
          }else{

            for(i = 0; i + 4 <= n; i += 4, v_base += (incv * 8), y_base += (incy * 8)){
              v_0 = v_base[0];
              v_1 = v_base[1];
              v_2 = v_base[(incv * 2)];
              v_3 = v_base[((incv * 2) + 1)];
              v_4 = v_base[(incv * 4)];
              v_5 = v_base[((incv * 4) + 1)];
              v_6 = v_base[(incv * 6)];
              v_7 = v_base[((incv * 6) + 1)];
              y_0 = y_base[0];
              y_1 = y_base[1];
              y_2 = y_base[(incy * 2)];
              y_3 = y_base[((incy * 2) + 1)];
              y_4 = y_base[(incy * 4)];
              y_5 = y_base[((incy * 4) + 1)];
              y_6 = y_base[(incy * 6)];
              y_7 = y_base[((incy * 6) + 1)];
              v_8 = v_1 * y_1 * -1;
              v_9 = v_0 * y_1;
              v_10 = v_3 * y_3 * -1;
              v_11 = v_2 * y_3;
              v_12 = v_5 * y_5 * -1;
              v_13 = v_4 * y_5;
              v_14 = v_7 * y_7 * -1;
              v_15 = v_6 * y_7;
              v_0 = v_0 * y_0;
              v_1 = v_1 * y_0;
              v_2 = v_2 * y_2;
              v_3 = v_3 * y_2;
              v_4 = v_4 * y_4;
              v_5 = v_5 * y_4;
              v_6 = v_6 * y_6;
              v_7 = v_7 * y_6;
              q_0 = s_0_0;
              q_1 = s_0_1;
              q_2 = s_0_2;
              q_3 = s_0_3;
              q_4 = s_0_4;
              q_5 = s_0_5;
              q_6 = s_0_6;
              q_7 = s_0_7;
              tmp_BLP.d = v_0;
              tmp_BLP.l |= 1;
              s_0_0 = s_0_0 + tmp_BLP.d;
              tmp_BLP.d = v_1;
              tmp_BLP.l |= 1;
              s_0_1 = s_0_1 + tmp_BLP.d;
              tmp_BLP.d = v_2;
              tmp_BLP.l |= 1;
              s_0_2 = s_0_2 + tmp_BLP.d;
              tmp_BLP.d = v_3;
              tmp_BLP.l |= 1;
              s_0_3 = s_0_3 + tmp_BLP.d;
              tmp_BLP.d = v_4;
              tmp_BLP.l |= 1;
              s_0_4 = s_0_4 + tmp_BLP.d;
              tmp_BLP.d = v_5;
              tmp_BLP.l |= 1;
              s_0_5 = s_0_5 + tmp_BLP.d;
              tmp_BLP.d = v_6;
              tmp_BLP.l |= 1;
              s_0_6 = s_0_6 + tmp_BLP.d;
              tmp_BLP.d = v_7;
              tmp_BLP.l |= 1;
              s_0_7 = s_0_7 + tmp_BLP.d;
              q_0 = q_0 - s_0_0;
              q_1 = q_1 - s_0_1;
              q_2 = q_2 - s_0_2;
              q_3 = q_3 - s_0_3;
              q_4 = q_4 - s_0_4;
              q_5 = q_5 - s_0_5;
              q_6 = q_6 - s_0_6;
              q_7 = q_7 - s_0_7;
              v_0 = v_0 + q_0;
              v_1 = v_1 + q_1;
              v_2 = v_2 + q_2;
              v_3 = v_3 + q_3;
              v_4 = v_4 + q_4;
              v_5 = v_5 + q_5;
              v_6 = v_6 + q_6;
              v_7 = v_7 + q_7;
              q_0 = s_1_0;
              q_1 = s_1_1;
              q_2 = s_1_2;
              q_3 = s_1_3;
              q_4 = s_1_4;
              q_5 = s_1_5;
              q_6 = s_1_6;
              q_7 = s_1_7;
              tmp_BLP.d = v_0;
              tmp_BLP.l |= 1;
              s_1_0 = s_1_0 + tmp_BLP.d;
              tmp_BLP.d = v_1;
              tmp_BLP.l |= 1;
              s_1_1 = s_1_1 + tmp_BLP.d;
              tmp_BLP.d = v_2;
              tmp_BLP.l |= 1;
              s_1_2 = s_1_2 + tmp_BLP.d;
              tmp_BLP.d = v_3;
              tmp_BLP.l |= 1;
              s_1_3 = s_1_3 + tmp_BLP.d;
              tmp_BLP.d = v_4;
              tmp_BLP.l |= 1;
              s_1_4 = s_1_4 + tmp_BLP.d;
              tmp_BLP.d = v_5;
              tmp_BLP.l |= 1;
              s_1_5 = s_1_5 + tmp_BLP.d;
              tmp_BLP.d = v_6;
              tmp_BLP.l |= 1;
              s_1_6 = s_1_6 + tmp_BLP.d;
              tmp_BLP.d = v_7;
              tmp_BLP.l |= 1;
              s_1_7 = s_1_7 + tmp_BLP.d;
              q_0 = q_0 - s_1_0;
              q_1 = q_1 - s_1_1;
              q_2 = q_2 - s_1_2;
              q_3 = q_3 - s_1_3;
              q_4 = q_4 - s_1_4;
              q_5 = q_5 - s_1_5;
              q_6 = q_6 - s_1_6;
              q_7 = q_7 - s_1_7;
              v_0 = v_0 + q_0;
              v_1 = v_1 + q_1;
              v_2 = v_2 + q_2;
              v_3 = v_3 + q_3;
              v_4 = v_4 + q_4;
              v_5 = v_5 + q_5;
              v_6 = v_6 + q_6;
              v_7 = v_7 + q_7;
              tmp_BLP.d = v_0;
              tmp_BLP.l |= 1;
              s_2_0 = s_2_0 + tmp_BLP.d;
              tmp_BLP.d = v_1;
              tmp_BLP.l |= 1;
              s_2_1 = s_2_1 + tmp_BLP.d;
              tmp_BLP.d = v_2;
              tmp_BLP.l |= 1;
              s_2_2 = s_2_2 + tmp_BLP.d;
              tmp_BLP.d = v_3;
              tmp_BLP.l |= 1;
              s_2_3 = s_2_3 + tmp_BLP.d;
              tmp_BLP.d = v_4;
              tmp_BLP.l |= 1;
              s_2_4 = s_2_4 + tmp_BLP.d;
              tmp_BLP.d = v_5;
              tmp_BLP.l |= 1;
              s_2_5 = s_2_5 + tmp_BLP.d;
              tmp_BLP.d = v_6;
              tmp_BLP.l |= 1;
              s_2_6 = s_2_6 + tmp_BLP.d;
              tmp_BLP.d = v_7;
              tmp_BLP.l |= 1;
              s_2_7 = s_2_7 + tmp_BLP.d;
              q_0 = s_0_0;
              q_1 = s_0_1;
              q_2 = s_0_2;
              q_3 = s_0_3;
              q_4 = s_0_4;
              q_5 = s_0_5;
              q_6 = s_0_6;
              q_7 = s_0_7;
              tmp_BLP.d = v_8;
              tmp_BLP.l |= 1;
              s_0_0 = s_0_0 + tmp_BLP.d;
              tmp_BLP.d = v_9;
              tmp_BLP.l |= 1;
              s_0_1 = s_0_1 + tmp_BLP.d;
              tmp_BLP.d = v_10;
              tmp_BLP.l |= 1;
              s_0_2 = s_0_2 + tmp_BLP.d;
              tmp_BLP.d = v_11;
              tmp_BLP.l |= 1;
              s_0_3 = s_0_3 + tmp_BLP.d;
              tmp_BLP.d = v_12;
              tmp_BLP.l |= 1;
              s_0_4 = s_0_4 + tmp_BLP.d;
              tmp_BLP.d = v_13;
              tmp_BLP.l |= 1;
              s_0_5 = s_0_5 + tmp_BLP.d;
              tmp_BLP.d = v_14;
              tmp_BLP.l |= 1;
              s_0_6 = s_0_6 + tmp_BLP.d;
              tmp_BLP.d = v_15;
              tmp_BLP.l |= 1;
              s_0_7 = s_0_7 + tmp_BLP.d;
              q_0 = q_0 - s_0_0;
              q_1 = q_1 - s_0_1;
              q_2 = q_2 - s_0_2;
              q_3 = q_3 - s_0_3;
              q_4 = q_4 - s_0_4;
              q_5 = q_5 - s_0_5;
              q_6 = q_6 - s_0_6;
              q_7 = q_7 - s_0_7;
              v_8 = v_8 + q_0;
              v_9 = v_9 + q_1;
              v_10 = v_10 + q_2;
              v_11 = v_11 + q_3;
              v_12 = v_12 + q_4;
              v_13 = v_13 + q_5;
              v_14 = v_14 + q_6;
              v_15 = v_15 + q_7;
              q_0 = s_1_0;
              q_1 = s_1_1;
              q_2 = s_1_2;
              q_3 = s_1_3;
              q_4 = s_1_4;
              q_5 = s_1_5;
              q_6 = s_1_6;
              q_7 = s_1_7;
              tmp_BLP.d = v_8;
              tmp_BLP.l |= 1;
              s_1_0 = s_1_0 + tmp_BLP.d;
              tmp_BLP.d = v_9;
              tmp_BLP.l |= 1;
              s_1_1 = s_1_1 + tmp_BLP.d;
              tmp_BLP.d = v_10;
              tmp_BLP.l |= 1;
              s_1_2 = s_1_2 + tmp_BLP.d;
              tmp_BLP.d = v_11;
              tmp_BLP.l |= 1;
              s_1_3 = s_1_3 + tmp_BLP.d;
              tmp_BLP.d = v_12;
              tmp_BLP.l |= 1;
              s_1_4 = s_1_4 + tmp_BLP.d;
              tmp_BLP.d = v_13;
              tmp_BLP.l |= 1;
              s_1_5 = s_1_5 + tmp_BLP.d;
              tmp_BLP.d = v_14;
              tmp_BLP.l |= 1;
              s_1_6 = s_1_6 + tmp_BLP.d;
              tmp_BLP.d = v_15;
              tmp_BLP.l |= 1;
              s_1_7 = s_1_7 + tmp_BLP.d;
              q_0 = q_0 - s_1_0;
              q_1 = q_1 - s_1_1;
              q_2 = q_2 - s_1_2;
              q_3 = q_3 - s_1_3;
              q_4 = q_4 - s_1_4;
              q_5 = q_5 - s_1_5;
              q_6 = q_6 - s_1_6;
              q_7 = q_7 - s_1_7;
              v_8 = v_8 + q_0;
              v_9 = v_9 + q_1;
              v_10 = v_10 + q_2;
              v_11 = v_11 + q_3;
              v_12 = v_12 + q_4;
              v_13 = v_13 + q_5;
              v_14 = v_14 + q_6;
              v_15 = v_15 + q_7;
              tmp_BLP.d = v_8;
              tmp_BLP.l |= 1;
              s_2_0 = s_2_0 + tmp_BLP.d;
              tmp_BLP.d = v_9;
              tmp_BLP.l |= 1;
              s_2_1 = s_2_1 + tmp_BLP.d;
              tmp_BLP.d = v_10;
              tmp_BLP.l |= 1;
              s_2_2 = s_2_2 + tmp_BLP.d;
              tmp_BLP.d = v_11;
              tmp_BLP.l |= 1;
              s_2_3 = s_2_3 + tmp_BLP.d;
              tmp_BLP.d = v_12;
              tmp_BLP.l |= 1;
              s_2_4 = s_2_4 + tmp_BLP.d;
              tmp_BLP.d = v_13;
              tmp_BLP.l |= 1;
              s_2_5 = s_2_5 + tmp_BLP.d;
              tmp_BLP.d = v_14;
              tmp_BLP.l |= 1;
              s_2_6 = s_2_6 + tmp_BLP.d;
              tmp_BLP.d = v_15;
              tmp_BLP.l |= 1;
              s_2_7 = s_2_7 + tmp_BLP.d;
            }
            if(i + 2 <= n){
              v_0 = v_base[0];
              v_1 = v_base[1];
              v_2 = v_base[(incv * 2)];
              v_3 = v_base[((incv * 2) + 1)];
              y_0 = y_base[0];
              y_1 = y_base[1];
              y_2 = y_base[(incy * 2)];
              y_3 = y_base[((incy * 2) + 1)];
              v_4 = v_1 * y_1 * -1;
              v_5 = v_0 * y_1;
              v_6 = v_3 * y_3 * -1;
              v_7 = v_2 * y_3;
              v_0 = v_0 * y_0;
              v_1 = v_1 * y_0;
              v_2 = v_2 * y_2;
              v_3 = v_3 * y_2;
              q_0 = s_0_0;
              q_1 = s_0_1;
              q_2 = s_0_2;
              q_3 = s_0_3;
              q_4 = s_0_4;
              q_5 = s_0_5;
              q_6 = s_0_6;
              q_7 = s_0_7;
              tmp_BLP.d = v_0;
              tmp_BLP.l |= 1;
              s_0_0 = s_0_0 + tmp_BLP.d;
              tmp_BLP.d = v_1;
              tmp_BLP.l |= 1;
              s_0_1 = s_0_1 + tmp_BLP.d;
              tmp_BLP.d = v_2;
              tmp_BLP.l |= 1;
              s_0_2 = s_0_2 + tmp_BLP.d;
              tmp_BLP.d = v_3;
              tmp_BLP.l |= 1;
              s_0_3 = s_0_3 + tmp_BLP.d;
              tmp_BLP.d = v_4;
              tmp_BLP.l |= 1;
              s_0_4 = s_0_4 + tmp_BLP.d;
              tmp_BLP.d = v_5;
              tmp_BLP.l |= 1;
              s_0_5 = s_0_5 + tmp_BLP.d;
              tmp_BLP.d = v_6;
              tmp_BLP.l |= 1;
              s_0_6 = s_0_6 + tmp_BLP.d;
              tmp_BLP.d = v_7;
              tmp_BLP.l |= 1;
              s_0_7 = s_0_7 + tmp_BLP.d;
              q_0 = q_0 - s_0_0;
              q_1 = q_1 - s_0_1;
              q_2 = q_2 - s_0_2;
              q_3 = q_3 - s_0_3;
              q_4 = q_4 - s_0_4;
              q_5 = q_5 - s_0_5;
              q_6 = q_6 - s_0_6;
              q_7 = q_7 - s_0_7;
              v_0 = v_0 + q_0;
              v_1 = v_1 + q_1;
              v_2 = v_2 + q_2;
              v_3 = v_3 + q_3;
              v_4 = v_4 + q_4;
              v_5 = v_5 + q_5;
              v_6 = v_6 + q_6;
              v_7 = v_7 + q_7;
              q_0 = s_1_0;
              q_1 = s_1_1;
              q_2 = s_1_2;
              q_3 = s_1_3;
              q_4 = s_1_4;
              q_5 = s_1_5;
              q_6 = s_1_6;
              q_7 = s_1_7;
              tmp_BLP.d = v_0;
              tmp_BLP.l |= 1;
              s_1_0 = s_1_0 + tmp_BLP.d;
              tmp_BLP.d = v_1;
              tmp_BLP.l |= 1;
              s_1_1 = s_1_1 + tmp_BLP.d;
              tmp_BLP.d = v_2;
              tmp_BLP.l |= 1;
              s_1_2 = s_1_2 + tmp_BLP.d;
              tmp_BLP.d = v_3;
              tmp_BLP.l |= 1;
              s_1_3 = s_1_3 + tmp_BLP.d;
              tmp_BLP.d = v_4;
              tmp_BLP.l |= 1;
              s_1_4 = s_1_4 + tmp_BLP.d;
              tmp_BLP.d = v_5;
              tmp_BLP.l |= 1;
              s_1_5 = s_1_5 + tmp_BLP.d;
              tmp_BLP.d = v_6;
              tmp_BLP.l |= 1;
              s_1_6 = s_1_6 + tmp_BLP.d;
              tmp_BLP.d = v_7;
              tmp_BLP.l |= 1;
              s_1_7 = s_1_7 + tmp_BLP.d;
              q_0 = q_0 - s_1_0;
              q_1 = q_1 - s_1_1;
              q_2 = q_2 - s_1_2;
              q_3 = q_3 - s_1_3;
              q_4 = q_4 - s_1_4;
              q_5 = q_5 - s_1_5;
              q_6 = q_6 - s_1_6;
              q_7 = q_7 - s_1_7;
              v_0 = v_0 + q_0;
              v_1 = v_1 + q_1;
              v_2 = v_2 + q_2;
              v_3 = v_3 + q_3;
              v_4 = v_4 + q_4;
              v_5 = v_5 + q_5;
              v_6 = v_6 + q_6;
              v_7 = v_7 + q_7;
              tmp_BLP.d = v_0;
              tmp_BLP.l |= 1;
              s_2_0 = s_2_0 + tmp_BLP.d;
              tmp_BLP.d = v_1;
              tmp_BLP.l |= 1;
              s_2_1 = s_2_1 + tmp_BLP.d;
              tmp_BLP.d = v_2;
              tmp_BLP.l |= 1;
              s_2_2 = s_2_2 + tmp_BLP.d;
              tmp_BLP.d = v_3;
              tmp_BLP.l |= 1;
              s_2_3 = s_2_3 + tmp_BLP.d;
              tmp_BLP.d = v_4;
              tmp_BLP.l |= 1;
              s_2_4 = s_2_4 + tmp_BLP.d;
              tmp_BLP.d = v_5;
              tmp_BLP.l |= 1;
              s_2_5 = s_2_5 + tmp_BLP.d;
              tmp_BLP.d = v_6;
              tmp_BLP.l |= 1;
              s_2_6 = s_2_6 + tmp_BLP.d;
              tmp_BLP.d = v_7;
              tmp_BLP.l |= 1;
              s_2_7 = s_2_7 + tmp_BLP.d;
              i += 2, v_base += (incv * 4), y_base += (incy * 4);
            }
            if(i + 1 <= n){
              v_0 = v_base[0];
              v_1 = v_base[1];
              y_0 = y_base[0];
              y_1 = y_base[1];
              v_2 = v_1 * y_1 * -1;
              v_3 = v_0 * y_1;
              v_0 = v_0 * y_0;
              v_1 = v_1 * y_0;
              q_0 = s_0_0;
              q_1 = s_0_1;
              q_2 = s_0_2;
              q_3 = s_0_3;
              tmp_BLP.d = v_0;
              tmp_BLP.l |= 1;
              s_0_0 = s_0_0 + tmp_BLP.d;
              tmp_BLP.d = v_1;
              tmp_BLP.l |= 1;
              s_0_1 = s_0_1 + tmp_BLP.d;
              tmp_BLP.d = v_2;
              tmp_BLP.l |= 1;
              s_0_2 = s_0_2 + tmp_BLP.d;
              tmp_BLP.d = v_3;
              tmp_BLP.l |= 1;
              s_0_3 = s_0_3 + tmp_BLP.d;
              q_0 = q_0 - s_0_0;
              q_1 = q_1 - s_0_1;
              q_2 = q_2 - s_0_2;
              q_3 = q_3 - s_0_3;
              v_0 = v_0 + q_0;
              v_1 = v_1 + q_1;
              v_2 = v_2 + q_2;
              v_3 = v_3 + q_3;
              q_0 = s_1_0;
              q_1 = s_1_1;
              q_2 = s_1_2;
              q_3 = s_1_3;
              tmp_BLP.d = v_0;
              tmp_BLP.l |= 1;
              s_1_0 = s_1_0 + tmp_BLP.d;
              tmp_BLP.d = v_1;
              tmp_BLP.l |= 1;
              s_1_1 = s_1_1 + tmp_BLP.d;
              tmp_BLP.d = v_2;
              tmp_BLP.l |= 1;
              s_1_2 = s_1_2 + tmp_BLP.d;
              tmp_BLP.d = v_3;
              tmp_BLP.l |= 1;
              s_1_3 = s_1_3 + tmp_BLP.d;
              q_0 = q_0 - s_1_0;
              q_1 = q_1 - s_1_1;
              q_2 = q_2 - s_1_2;
              q_3 = q_3 - s_1_3;
              v_0 = v_0 + q_0;
              v_1 = v_1 + q_1;
              v_2 = v_2 + q_2;
              v_3 = v_3 + q_3;
              tmp_BLP.d = v_0;
              tmp_BLP.l |= 1;
              s_2_0 = s_2_0 + tmp_BLP.d;
              tmp_BLP.d = v_1;
              tmp_BLP.l |= 1;
              s_2_1 = s_2_1 + tmp_BLP.d;
              tmp_BLP.d = v_2;
              tmp_BLP.l |= 1;
              s_2_2 = s_2_2 + tmp_BLP.d;
              tmp_BLP.d = v_3;
              tmp_BLP.l |= 1;
              s_2_3 = s_2_3 + tmp_BLP.d;
              i += 1, v_base += (incv * 2), y_base += (incy * 2);
            }
          }
        }
        q_0 = ((double*)sum)[0];
        s_0_0 = s_0_0 + (s_0_2 - q_0);
        s_0_0 = s_0_0 + (s_0_4 - q_0);
        s_0_0 = s_0_0 + (s_0_6 - q_0);
        q_0 = ((double*)sum)[1];
        s_0_1 = s_0_1 + (s_0_3 - q_0);
        s_0_1 = s_0_1 + (s_0_5 - q_0);
        s_0_1 = s_0_1 + (s_0_7 - q_0);
        ((double*)sum)[0] = s_0_0;
        ((double*)sum)[1] = s_0_1;
        q_0 = ((double*)sum)[2];
        s_1_0 = s_1_0 + (s_1_2 - q_0);
        s_1_0 = s_1_0 + (s_1_4 - q_0);
        s_1_0 = s_1_0 + (s_1_6 - q_0);
        q_0 = ((double*)sum)[3];
        s_1_1 = s_1_1 + (s_1_3 - q_0);
        s_1_1 = s_1_1 + (s_1_5 - q_0);
        s_1_1 = s_1_1 + (s_1_7 - q_0);
        ((double*)sum)[2] = s_1_0;
        ((double*)sum)[3] = s_1_1;
        q_0 = ((double*)sum)[4];
        s_2_0 = s_2_0 + (s_2_2 - q_0);
        s_2_0 = s_2_0 + (s_2_4 - q_0);
        s_2_0 = s_2_0 + (s_2_6 - q_0);
        q_0 = ((double*)sum)[5];
        s_2_1 = s_2_1 + (s_2_3 - q_0);
        s_2_1 = s_2_1 + (s_2_5 - q_0);
        s_2_1 = s_2_1 + (s_2_7 - q_0);
        ((double*)sum)[4] = s_2_0;
        ((double*)sum)[5] = s_2_1;
        RESET_DAZ_FLAG
        return;
      }
      default:{
        int i, j;

        double* sum_base = (double*) sum;
        double* v_base = (double*) v;
        double* y_base = (double*) y;
        double v_0, v_1, v_2, v_3, v_4, v_5, v_6, v_7, v_8, v_9, v_10, v_11, v_12, v_13, v_14, v_15;
        double y_0, y_1, y_2, y_3, y_4, y_5, y_6, y_7;
        double q_0, q_1, q_2, q_3, q_4, q_5, q_6, q_7;
        double s_0, s_1, s_2, s_3, s_4, s_5, s_6, s_7;
        double s_buffer[(MAX_FOLD * 8)];

        for(j = 0; j < fold; j += 1){
          s_buffer[(j * 8)] = s_buffer[((j * 8) + 2)] = s_buffer[((j * 8) + 4)] = s_buffer[((j * 8) + 6)] = sum_base[(j * 2)];
          s_buffer[((j * 8) + 1)] = s_buffer[((j * 8) + 3)] = s_buffer[((j * 8) + 5)] = s_buffer[((j * 8) + 7)] = sum_base[((j * 2) + 1)];
        }
        if(incv == 1){
          if(incy == 1){

            for(i = 0; i + 4 <= n; i += 4, v_base += 8, y_base += 8){
              v_0 = v_base[0];
              v_1 = v_base[1];
              v_2 = v_base[2];
              v_3 = v_base[3];
              v_4 = v_base[4];
              v_5 = v_base[5];
              v_6 = v_base[6];
              v_7 = v_base[7];
              y_0 = y_base[0];
              y_1 = y_base[1];
              y_2 = y_base[2];
              y_3 = y_base[3];
              y_4 = y_base[4];
              y_5 = y_base[5];
              y_6 = y_base[6];
              y_7 = y_base[7];
              v_8 = v_1 * y_1 * -1;
              v_9 = v_0 * y_1;
              v_10 = v_3 * y_3 * -1;
              v_11 = v_2 * y_3;
              v_12 = v_5 * y_5 * -1;
              v_13 = v_4 * y_5;
              v_14 = v_7 * y_7 * -1;
              v_15 = v_6 * y_7;
              v_0 = v_0 * y_0;
              v_1 = v_1 * y_0;
              v_2 = v_2 * y_2;
              v_3 = v_3 * y_2;
              v_4 = v_4 * y_4;
              v_5 = v_5 * y_4;
              v_6 = v_6 * y_6;
              v_7 = v_7 * y_6;
              for(j = 0; j < fold - 1; j++){
                s_0 = s_buffer[(j * 8)];
                s_1 = s_buffer[((j * 8) + 1)];
                s_2 = s_buffer[((j * 8) + 2)];
                s_3 = s_buffer[((j * 8) + 3)];
                s_4 = s_buffer[((j * 8) + 4)];
                s_5 = s_buffer[((j * 8) + 5)];
                s_6 = s_buffer[((j * 8) + 6)];
                s_7 = s_buffer[((j * 8) + 7)];
                tmp_BLP.d = v_0;
                tmp_BLP.l |= 1;
                q_0 = s_0 + tmp_BLP.d;
                tmp_BLP.d = v_1;
                tmp_BLP.l |= 1;
                q_1 = s_1 + tmp_BLP.d;
                tmp_BLP.d = v_2;
                tmp_BLP.l |= 1;
                q_2 = s_2 + tmp_BLP.d;
                tmp_BLP.d = v_3;
                tmp_BLP.l |= 1;
                q_3 = s_3 + tmp_BLP.d;
                tmp_BLP.d = v_4;
                tmp_BLP.l |= 1;
                q_4 = s_4 + tmp_BLP.d;
                tmp_BLP.d = v_5;
                tmp_BLP.l |= 1;
                q_5 = s_5 + tmp_BLP.d;
                tmp_BLP.d = v_6;
                tmp_BLP.l |= 1;
                q_6 = s_6 + tmp_BLP.d;
                tmp_BLP.d = v_7;
                tmp_BLP.l |= 1;
                q_7 = s_7 + tmp_BLP.d;
                s_buffer[(j * 8)] = q_0;
                s_buffer[((j * 8) + 1)] = q_1;
                s_buffer[((j * 8) + 2)] = q_2;
                s_buffer[((j * 8) + 3)] = q_3;
                s_buffer[((j * 8) + 4)] = q_4;
                s_buffer[((j * 8) + 5)] = q_5;
                s_buffer[((j * 8) + 6)] = q_6;
                s_buffer[((j * 8) + 7)] = q_7;
                q_0 = s_0 - q_0;
                q_1 = s_1 - q_1;
                q_2 = s_2 - q_2;
                q_3 = s_3 - q_3;
                q_4 = s_4 - q_4;
                q_5 = s_5 - q_5;
                q_6 = s_6 - q_6;
                q_7 = s_7 - q_7;
                v_0 = v_0 + q_0;
                v_1 = v_1 + q_1;
                v_2 = v_2 + q_2;
                v_3 = v_3 + q_3;
                v_4 = v_4 + q_4;
                v_5 = v_5 + q_5;
                v_6 = v_6 + q_6;
                v_7 = v_7 + q_7;
                s_0 = s_buffer[(j * 8)];
                s_1 = s_buffer[((j * 8) + 1)];
                s_2 = s_buffer[((j * 8) + 2)];
                s_3 = s_buffer[((j * 8) + 3)];
                s_4 = s_buffer[((j * 8) + 4)];
                s_5 = s_buffer[((j * 8) + 5)];
                s_6 = s_buffer[((j * 8) + 6)];
                s_7 = s_buffer[((j * 8) + 7)];
                tmp_BLP.d = v_8;
                tmp_BLP.l |= 1;
                q_0 = s_0 + tmp_BLP.d;
                tmp_BLP.d = v_9;
                tmp_BLP.l |= 1;
                q_1 = s_1 + tmp_BLP.d;
                tmp_BLP.d = v_10;
                tmp_BLP.l |= 1;
                q_2 = s_2 + tmp_BLP.d;
                tmp_BLP.d = v_11;
                tmp_BLP.l |= 1;
                q_3 = s_3 + tmp_BLP.d;
                tmp_BLP.d = v_12;
                tmp_BLP.l |= 1;
                q_4 = s_4 + tmp_BLP.d;
                tmp_BLP.d = v_13;
                tmp_BLP.l |= 1;
                q_5 = s_5 + tmp_BLP.d;
                tmp_BLP.d = v_14;
                tmp_BLP.l |= 1;
                q_6 = s_6 + tmp_BLP.d;
                tmp_BLP.d = v_15;
                tmp_BLP.l |= 1;
                q_7 = s_7 + tmp_BLP.d;
                s_buffer[(j * 8)] = q_0;
                s_buffer[((j * 8) + 1)] = q_1;
                s_buffer[((j * 8) + 2)] = q_2;
                s_buffer[((j * 8) + 3)] = q_3;
                s_buffer[((j * 8) + 4)] = q_4;
                s_buffer[((j * 8) + 5)] = q_5;
                s_buffer[((j * 8) + 6)] = q_6;
                s_buffer[((j * 8) + 7)] = q_7;
                q_0 = s_0 - q_0;
                q_1 = s_1 - q_1;
                q_2 = s_2 - q_2;
                q_3 = s_3 - q_3;
                q_4 = s_4 - q_4;
                q_5 = s_5 - q_5;
                q_6 = s_6 - q_6;
                q_7 = s_7 - q_7;
                v_8 = v_8 + q_0;
                v_9 = v_9 + q_1;
                v_10 = v_10 + q_2;
                v_11 = v_11 + q_3;
                v_12 = v_12 + q_4;
                v_13 = v_13 + q_5;
                v_14 = v_14 + q_6;
                v_15 = v_15 + q_7;
              }
              tmp_BLP.d = v_8;
              tmp_BLP.l |= 1;
              s_buffer[(j * 8)] = s_buffer[(j * 8)] + tmp_BLP.d;
              tmp_BLP.d = v_9;
              tmp_BLP.l |= 1;
              s_buffer[((j * 8) + 1)] = s_buffer[((j * 8) + 1)] + tmp_BLP.d;
              tmp_BLP.d = v_10;
              tmp_BLP.l |= 1;
              s_buffer[((j * 8) + 2)] = s_buffer[((j * 8) + 2)] + tmp_BLP.d;
              tmp_BLP.d = v_11;
              tmp_BLP.l |= 1;
              s_buffer[((j * 8) + 3)] = s_buffer[((j * 8) + 3)] + tmp_BLP.d;
              tmp_BLP.d = v_12;
              tmp_BLP.l |= 1;
              s_buffer[((j * 8) + 4)] = s_buffer[((j * 8) + 4)] + tmp_BLP.d;
              tmp_BLP.d = v_13;
              tmp_BLP.l |= 1;
              s_buffer[((j * 8) + 5)] = s_buffer[((j * 8) + 5)] + tmp_BLP.d;
              tmp_BLP.d = v_14;
              tmp_BLP.l |= 1;
              s_buffer[((j * 8) + 6)] = s_buffer[((j * 8) + 6)] + tmp_BLP.d;
              tmp_BLP.d = v_15;
              tmp_BLP.l |= 1;
              s_buffer[((j * 8) + 7)] = s_buffer[((j * 8) + 7)] + tmp_BLP.d;
            }
            if(i + 2 <= n){
              v_0 = v_base[0];
              v_1 = v_base[1];
              v_2 = v_base[2];
              v_3 = v_base[3];
              y_0 = y_base[0];
              y_1 = y_base[1];
              y_2 = y_base[2];
              y_3 = y_base[3];
              v_4 = v_1 * y_1 * -1;
              v_5 = v_0 * y_1;
              v_6 = v_3 * y_3 * -1;
              v_7 = v_2 * y_3;
              v_0 = v_0 * y_0;
              v_1 = v_1 * y_0;
              v_2 = v_2 * y_2;
              v_3 = v_3 * y_2;
              for(j = 0; j < fold - 1; j++){
                s_0 = s_buffer[(j * 8)];
                s_1 = s_buffer[((j * 8) + 1)];
                s_2 = s_buffer[((j * 8) + 2)];
                s_3 = s_buffer[((j * 8) + 3)];
                s_4 = s_buffer[((j * 8) + 4)];
                s_5 = s_buffer[((j * 8) + 5)];
                s_6 = s_buffer[((j * 8) + 6)];
                s_7 = s_buffer[((j * 8) + 7)];
                tmp_BLP.d = v_0;
                tmp_BLP.l |= 1;
                q_0 = s_0 + tmp_BLP.d;
                tmp_BLP.d = v_1;
                tmp_BLP.l |= 1;
                q_1 = s_1 + tmp_BLP.d;
                tmp_BLP.d = v_2;
                tmp_BLP.l |= 1;
                q_2 = s_2 + tmp_BLP.d;
                tmp_BLP.d = v_3;
                tmp_BLP.l |= 1;
                q_3 = s_3 + tmp_BLP.d;
                tmp_BLP.d = v_4;
                tmp_BLP.l |= 1;
                q_4 = s_4 + tmp_BLP.d;
                tmp_BLP.d = v_5;
                tmp_BLP.l |= 1;
                q_5 = s_5 + tmp_BLP.d;
                tmp_BLP.d = v_6;
                tmp_BLP.l |= 1;
                q_6 = s_6 + tmp_BLP.d;
                tmp_BLP.d = v_7;
                tmp_BLP.l |= 1;
                q_7 = s_7 + tmp_BLP.d;
                s_buffer[(j * 8)] = q_0;
                s_buffer[((j * 8) + 1)] = q_1;
                s_buffer[((j * 8) + 2)] = q_2;
                s_buffer[((j * 8) + 3)] = q_3;
                s_buffer[((j * 8) + 4)] = q_4;
                s_buffer[((j * 8) + 5)] = q_5;
                s_buffer[((j * 8) + 6)] = q_6;
                s_buffer[((j * 8) + 7)] = q_7;
                q_0 = s_0 - q_0;
                q_1 = s_1 - q_1;
                q_2 = s_2 - q_2;
                q_3 = s_3 - q_3;
                q_4 = s_4 - q_4;
                q_5 = s_5 - q_5;
                q_6 = s_6 - q_6;
                q_7 = s_7 - q_7;
                v_0 = v_0 + q_0;
                v_1 = v_1 + q_1;
                v_2 = v_2 + q_2;
                v_3 = v_3 + q_3;
                v_4 = v_4 + q_4;
                v_5 = v_5 + q_5;
                v_6 = v_6 + q_6;
                v_7 = v_7 + q_7;
              }
              tmp_BLP.d = v_0;
              tmp_BLP.l |= 1;
              s_buffer[(j * 8)] = s_buffer[(j * 8)] + tmp_BLP.d;
              tmp_BLP.d = v_1;
              tmp_BLP.l |= 1;
              s_buffer[((j * 8) + 1)] = s_buffer[((j * 8) + 1)] + tmp_BLP.d;
              tmp_BLP.d = v_2;
              tmp_BLP.l |= 1;
              s_buffer[((j * 8) + 2)] = s_buffer[((j * 8) + 2)] + tmp_BLP.d;
              tmp_BLP.d = v_3;
              tmp_BLP.l |= 1;
              s_buffer[((j * 8) + 3)] = s_buffer[((j * 8) + 3)] + tmp_BLP.d;
              tmp_BLP.d = v_4;
              tmp_BLP.l |= 1;
              s_buffer[((j * 8) + 4)] = s_buffer[((j * 8) + 4)] + tmp_BLP.d;
              tmp_BLP.d = v_5;
              tmp_BLP.l |= 1;
              s_buffer[((j * 8) + 5)] = s_buffer[((j * 8) + 5)] + tmp_BLP.d;
              tmp_BLP.d = v_6;
              tmp_BLP.l |= 1;
              s_buffer[((j * 8) + 6)] = s_buffer[((j * 8) + 6)] + tmp_BLP.d;
              tmp_BLP.d = v_7;
              tmp_BLP.l |= 1;
              s_buffer[((j * 8) + 7)] = s_buffer[((j * 8) + 7)] + tmp_BLP.d;
              i += 2, v_base += 4, y_base += 4;
            }
            if(i + 1 <= n){
              v_0 = v_base[0];
              v_1 = v_base[1];
              y_0 = y_base[0];
              y_1 = y_base[1];
              v_2 = v_1 * y_1 * -1;
              v_3 = v_0 * y_1;
              v_0 = v_0 * y_0;
              v_1 = v_1 * y_0;
              for(j = 0; j < fold - 1; j++){
                s_0 = s_buffer[(j * 8)];
                s_1 = s_buffer[((j * 8) + 1)];
                s_2 = s_buffer[((j * 8) + 2)];
                s_3 = s_buffer[((j * 8) + 3)];
                tmp_BLP.d = v_0;
                tmp_BLP.l |= 1;
                q_0 = s_0 + tmp_BLP.d;
                tmp_BLP.d = v_1;
                tmp_BLP.l |= 1;
                q_1 = s_1 + tmp_BLP.d;
                tmp_BLP.d = v_2;
                tmp_BLP.l |= 1;
                q_2 = s_2 + tmp_BLP.d;
                tmp_BLP.d = v_3;
                tmp_BLP.l |= 1;
                q_3 = s_3 + tmp_BLP.d;
                s_buffer[(j * 8)] = q_0;
                s_buffer[((j * 8) + 1)] = q_1;
                s_buffer[((j * 8) + 2)] = q_2;
                s_buffer[((j * 8) + 3)] = q_3;
                q_0 = s_0 - q_0;
                q_1 = s_1 - q_1;
                q_2 = s_2 - q_2;
                q_3 = s_3 - q_3;
                v_0 = v_0 + q_0;
                v_1 = v_1 + q_1;
                v_2 = v_2 + q_2;
                v_3 = v_3 + q_3;
              }
              tmp_BLP.d = v_0;
              tmp_BLP.l |= 1;
              s_buffer[(j * 8)] = s_buffer[(j * 8)] + tmp_BLP.d;
              tmp_BLP.d = v_1;
              tmp_BLP.l |= 1;
              s_buffer[((j * 8) + 1)] = s_buffer[((j * 8) + 1)] + tmp_BLP.d;
              tmp_BLP.d = v_2;
              tmp_BLP.l |= 1;
              s_buffer[((j * 8) + 2)] = s_buffer[((j * 8) + 2)] + tmp_BLP.d;
              tmp_BLP.d = v_3;
              tmp_BLP.l |= 1;
              s_buffer[((j * 8) + 3)] = s_buffer[((j * 8) + 3)] + tmp_BLP.d;
              i += 1, v_base += 2, y_base += 2;
            }
          }else{

            for(i = 0; i + 4 <= n; i += 4, v_base += 8, y_base += (incy * 8)){
              v_0 = v_base[0];
              v_1 = v_base[1];
              v_2 = v_base[2];
              v_3 = v_base[3];
              v_4 = v_base[4];
              v_5 = v_base[5];
              v_6 = v_base[6];
              v_7 = v_base[7];
              y_0 = y_base[0];
              y_1 = y_base[1];
              y_2 = y_base[(incy * 2)];
              y_3 = y_base[((incy * 2) + 1)];
              y_4 = y_base[(incy * 4)];
              y_5 = y_base[((incy * 4) + 1)];
              y_6 = y_base[(incy * 6)];
              y_7 = y_base[((incy * 6) + 1)];
              v_8 = v_1 * y_1 * -1;
              v_9 = v_0 * y_1;
              v_10 = v_3 * y_3 * -1;
              v_11 = v_2 * y_3;
              v_12 = v_5 * y_5 * -1;
              v_13 = v_4 * y_5;
              v_14 = v_7 * y_7 * -1;
              v_15 = v_6 * y_7;
              v_0 = v_0 * y_0;
              v_1 = v_1 * y_0;
              v_2 = v_2 * y_2;
              v_3 = v_3 * y_2;
              v_4 = v_4 * y_4;
              v_5 = v_5 * y_4;
              v_6 = v_6 * y_6;
              v_7 = v_7 * y_6;
              for(j = 0; j < fold - 1; j++){
                s_0 = s_buffer[(j * 8)];
                s_1 = s_buffer[((j * 8) + 1)];
                s_2 = s_buffer[((j * 8) + 2)];
                s_3 = s_buffer[((j * 8) + 3)];
                s_4 = s_buffer[((j * 8) + 4)];
                s_5 = s_buffer[((j * 8) + 5)];
                s_6 = s_buffer[((j * 8) + 6)];
                s_7 = s_buffer[((j * 8) + 7)];
                tmp_BLP.d = v_0;
                tmp_BLP.l |= 1;
                q_0 = s_0 + tmp_BLP.d;
                tmp_BLP.d = v_1;
                tmp_BLP.l |= 1;
                q_1 = s_1 + tmp_BLP.d;
                tmp_BLP.d = v_2;
                tmp_BLP.l |= 1;
                q_2 = s_2 + tmp_BLP.d;
                tmp_BLP.d = v_3;
                tmp_BLP.l |= 1;
                q_3 = s_3 + tmp_BLP.d;
                tmp_BLP.d = v_4;
                tmp_BLP.l |= 1;
                q_4 = s_4 + tmp_BLP.d;
                tmp_BLP.d = v_5;
                tmp_BLP.l |= 1;
                q_5 = s_5 + tmp_BLP.d;
                tmp_BLP.d = v_6;
                tmp_BLP.l |= 1;
                q_6 = s_6 + tmp_BLP.d;
                tmp_BLP.d = v_7;
                tmp_BLP.l |= 1;
                q_7 = s_7 + tmp_BLP.d;
                s_buffer[(j * 8)] = q_0;
                s_buffer[((j * 8) + 1)] = q_1;
                s_buffer[((j * 8) + 2)] = q_2;
                s_buffer[((j * 8) + 3)] = q_3;
                s_buffer[((j * 8) + 4)] = q_4;
                s_buffer[((j * 8) + 5)] = q_5;
                s_buffer[((j * 8) + 6)] = q_6;
                s_buffer[((j * 8) + 7)] = q_7;
                q_0 = s_0 - q_0;
                q_1 = s_1 - q_1;
                q_2 = s_2 - q_2;
                q_3 = s_3 - q_3;
                q_4 = s_4 - q_4;
                q_5 = s_5 - q_5;
                q_6 = s_6 - q_6;
                q_7 = s_7 - q_7;
                v_0 = v_0 + q_0;
                v_1 = v_1 + q_1;
                v_2 = v_2 + q_2;
                v_3 = v_3 + q_3;
                v_4 = v_4 + q_4;
                v_5 = v_5 + q_5;
                v_6 = v_6 + q_6;
                v_7 = v_7 + q_7;
                s_0 = s_buffer[(j * 8)];
                s_1 = s_buffer[((j * 8) + 1)];
                s_2 = s_buffer[((j * 8) + 2)];
                s_3 = s_buffer[((j * 8) + 3)];
                s_4 = s_buffer[((j * 8) + 4)];
                s_5 = s_buffer[((j * 8) + 5)];
                s_6 = s_buffer[((j * 8) + 6)];
                s_7 = s_buffer[((j * 8) + 7)];
                tmp_BLP.d = v_8;
                tmp_BLP.l |= 1;
                q_0 = s_0 + tmp_BLP.d;
                tmp_BLP.d = v_9;
                tmp_BLP.l |= 1;
                q_1 = s_1 + tmp_BLP.d;
                tmp_BLP.d = v_10;
                tmp_BLP.l |= 1;
                q_2 = s_2 + tmp_BLP.d;
                tmp_BLP.d = v_11;
                tmp_BLP.l |= 1;
                q_3 = s_3 + tmp_BLP.d;
                tmp_BLP.d = v_12;
                tmp_BLP.l |= 1;
                q_4 = s_4 + tmp_BLP.d;
                tmp_BLP.d = v_13;
                tmp_BLP.l |= 1;
                q_5 = s_5 + tmp_BLP.d;
                tmp_BLP.d = v_14;
                tmp_BLP.l |= 1;
                q_6 = s_6 + tmp_BLP.d;
                tmp_BLP.d = v_15;
                tmp_BLP.l |= 1;
                q_7 = s_7 + tmp_BLP.d;
                s_buffer[(j * 8)] = q_0;
                s_buffer[((j * 8) + 1)] = q_1;
                s_buffer[((j * 8) + 2)] = q_2;
                s_buffer[((j * 8) + 3)] = q_3;
                s_buffer[((j * 8) + 4)] = q_4;
                s_buffer[((j * 8) + 5)] = q_5;
                s_buffer[((j * 8) + 6)] = q_6;
                s_buffer[((j * 8) + 7)] = q_7;
                q_0 = s_0 - q_0;
                q_1 = s_1 - q_1;
                q_2 = s_2 - q_2;
                q_3 = s_3 - q_3;
                q_4 = s_4 - q_4;
                q_5 = s_5 - q_5;
                q_6 = s_6 - q_6;
                q_7 = s_7 - q_7;
                v_8 = v_8 + q_0;
                v_9 = v_9 + q_1;
                v_10 = v_10 + q_2;
                v_11 = v_11 + q_3;
                v_12 = v_12 + q_4;
                v_13 = v_13 + q_5;
                v_14 = v_14 + q_6;
                v_15 = v_15 + q_7;
              }
              tmp_BLP.d = v_8;
              tmp_BLP.l |= 1;
              s_buffer[(j * 8)] = s_buffer[(j * 8)] + tmp_BLP.d;
              tmp_BLP.d = v_9;
              tmp_BLP.l |= 1;
              s_buffer[((j * 8) + 1)] = s_buffer[((j * 8) + 1)] + tmp_BLP.d;
              tmp_BLP.d = v_10;
              tmp_BLP.l |= 1;
              s_buffer[((j * 8) + 2)] = s_buffer[((j * 8) + 2)] + tmp_BLP.d;
              tmp_BLP.d = v_11;
              tmp_BLP.l |= 1;
              s_buffer[((j * 8) + 3)] = s_buffer[((j * 8) + 3)] + tmp_BLP.d;
              tmp_BLP.d = v_12;
              tmp_BLP.l |= 1;
              s_buffer[((j * 8) + 4)] = s_buffer[((j * 8) + 4)] + tmp_BLP.d;
              tmp_BLP.d = v_13;
              tmp_BLP.l |= 1;
              s_buffer[((j * 8) + 5)] = s_buffer[((j * 8) + 5)] + tmp_BLP.d;
              tmp_BLP.d = v_14;
              tmp_BLP.l |= 1;
              s_buffer[((j * 8) + 6)] = s_buffer[((j * 8) + 6)] + tmp_BLP.d;
              tmp_BLP.d = v_15;
              tmp_BLP.l |= 1;
              s_buffer[((j * 8) + 7)] = s_buffer[((j * 8) + 7)] + tmp_BLP.d;
            }
            if(i + 2 <= n){
              v_0 = v_base[0];
              v_1 = v_base[1];
              v_2 = v_base[2];
              v_3 = v_base[3];
              y_0 = y_base[0];
              y_1 = y_base[1];
              y_2 = y_base[(incy * 2)];
              y_3 = y_base[((incy * 2) + 1)];
              v_4 = v_1 * y_1 * -1;
              v_5 = v_0 * y_1;
              v_6 = v_3 * y_3 * -1;
              v_7 = v_2 * y_3;
              v_0 = v_0 * y_0;
              v_1 = v_1 * y_0;
              v_2 = v_2 * y_2;
              v_3 = v_3 * y_2;
              for(j = 0; j < fold - 1; j++){
                s_0 = s_buffer[(j * 8)];
                s_1 = s_buffer[((j * 8) + 1)];
                s_2 = s_buffer[((j * 8) + 2)];
                s_3 = s_buffer[((j * 8) + 3)];
                s_4 = s_buffer[((j * 8) + 4)];
                s_5 = s_buffer[((j * 8) + 5)];
                s_6 = s_buffer[((j * 8) + 6)];
                s_7 = s_buffer[((j * 8) + 7)];
                tmp_BLP.d = v_0;
                tmp_BLP.l |= 1;
                q_0 = s_0 + tmp_BLP.d;
                tmp_BLP.d = v_1;
                tmp_BLP.l |= 1;
                q_1 = s_1 + tmp_BLP.d;
                tmp_BLP.d = v_2;
                tmp_BLP.l |= 1;
                q_2 = s_2 + tmp_BLP.d;
                tmp_BLP.d = v_3;
                tmp_BLP.l |= 1;
                q_3 = s_3 + tmp_BLP.d;
                tmp_BLP.d = v_4;
                tmp_BLP.l |= 1;
                q_4 = s_4 + tmp_BLP.d;
                tmp_BLP.d = v_5;
                tmp_BLP.l |= 1;
                q_5 = s_5 + tmp_BLP.d;
                tmp_BLP.d = v_6;
                tmp_BLP.l |= 1;
                q_6 = s_6 + tmp_BLP.d;
                tmp_BLP.d = v_7;
                tmp_BLP.l |= 1;
                q_7 = s_7 + tmp_BLP.d;
                s_buffer[(j * 8)] = q_0;
                s_buffer[((j * 8) + 1)] = q_1;
                s_buffer[((j * 8) + 2)] = q_2;
                s_buffer[((j * 8) + 3)] = q_3;
                s_buffer[((j * 8) + 4)] = q_4;
                s_buffer[((j * 8) + 5)] = q_5;
                s_buffer[((j * 8) + 6)] = q_6;
                s_buffer[((j * 8) + 7)] = q_7;
                q_0 = s_0 - q_0;
                q_1 = s_1 - q_1;
                q_2 = s_2 - q_2;
                q_3 = s_3 - q_3;
                q_4 = s_4 - q_4;
                q_5 = s_5 - q_5;
                q_6 = s_6 - q_6;
                q_7 = s_7 - q_7;
                v_0 = v_0 + q_0;
                v_1 = v_1 + q_1;
                v_2 = v_2 + q_2;
                v_3 = v_3 + q_3;
                v_4 = v_4 + q_4;
                v_5 = v_5 + q_5;
                v_6 = v_6 + q_6;
                v_7 = v_7 + q_7;
              }
              tmp_BLP.d = v_0;
              tmp_BLP.l |= 1;
              s_buffer[(j * 8)] = s_buffer[(j * 8)] + tmp_BLP.d;
              tmp_BLP.d = v_1;
              tmp_BLP.l |= 1;
              s_buffer[((j * 8) + 1)] = s_buffer[((j * 8) + 1)] + tmp_BLP.d;
              tmp_BLP.d = v_2;
              tmp_BLP.l |= 1;
              s_buffer[((j * 8) + 2)] = s_buffer[((j * 8) + 2)] + tmp_BLP.d;
              tmp_BLP.d = v_3;
              tmp_BLP.l |= 1;
              s_buffer[((j * 8) + 3)] = s_buffer[((j * 8) + 3)] + tmp_BLP.d;
              tmp_BLP.d = v_4;
              tmp_BLP.l |= 1;
              s_buffer[((j * 8) + 4)] = s_buffer[((j * 8) + 4)] + tmp_BLP.d;
              tmp_BLP.d = v_5;
              tmp_BLP.l |= 1;
              s_buffer[((j * 8) + 5)] = s_buffer[((j * 8) + 5)] + tmp_BLP.d;
              tmp_BLP.d = v_6;
              tmp_BLP.l |= 1;
              s_buffer[((j * 8) + 6)] = s_buffer[((j * 8) + 6)] + tmp_BLP.d;
              tmp_BLP.d = v_7;
              tmp_BLP.l |= 1;
              s_buffer[((j * 8) + 7)] = s_buffer[((j * 8) + 7)] + tmp_BLP.d;
              i += 2, v_base += 4, y_base += (incy * 4);
            }
            if(i + 1 <= n){
              v_0 = v_base[0];
              v_1 = v_base[1];
              y_0 = y_base[0];
              y_1 = y_base[1];
              v_2 = v_1 * y_1 * -1;
              v_3 = v_0 * y_1;
              v_0 = v_0 * y_0;
              v_1 = v_1 * y_0;
              for(j = 0; j < fold - 1; j++){
                s_0 = s_buffer[(j * 8)];
                s_1 = s_buffer[((j * 8) + 1)];
                s_2 = s_buffer[((j * 8) + 2)];
                s_3 = s_buffer[((j * 8) + 3)];
                tmp_BLP.d = v_0;
                tmp_BLP.l |= 1;
                q_0 = s_0 + tmp_BLP.d;
                tmp_BLP.d = v_1;
                tmp_BLP.l |= 1;
                q_1 = s_1 + tmp_BLP.d;
                tmp_BLP.d = v_2;
                tmp_BLP.l |= 1;
                q_2 = s_2 + tmp_BLP.d;
                tmp_BLP.d = v_3;
                tmp_BLP.l |= 1;
                q_3 = s_3 + tmp_BLP.d;
                s_buffer[(j * 8)] = q_0;
                s_buffer[((j * 8) + 1)] = q_1;
                s_buffer[((j * 8) + 2)] = q_2;
                s_buffer[((j * 8) + 3)] = q_3;
                q_0 = s_0 - q_0;
                q_1 = s_1 - q_1;
                q_2 = s_2 - q_2;
                q_3 = s_3 - q_3;
                v_0 = v_0 + q_0;
                v_1 = v_1 + q_1;
                v_2 = v_2 + q_2;
                v_3 = v_3 + q_3;
              }
              tmp_BLP.d = v_0;
              tmp_BLP.l |= 1;
              s_buffer[(j * 8)] = s_buffer[(j * 8)] + tmp_BLP.d;
              tmp_BLP.d = v_1;
              tmp_BLP.l |= 1;
              s_buffer[((j * 8) + 1)] = s_buffer[((j * 8) + 1)] + tmp_BLP.d;
              tmp_BLP.d = v_2;
              tmp_BLP.l |= 1;
              s_buffer[((j * 8) + 2)] = s_buffer[((j * 8) + 2)] + tmp_BLP.d;
              tmp_BLP.d = v_3;
              tmp_BLP.l |= 1;
              s_buffer[((j * 8) + 3)] = s_buffer[((j * 8) + 3)] + tmp_BLP.d;
              i += 1, v_base += 2, y_base += (incy * 2);
            }
          }
        }else{
          if(incy == 1){

            for(i = 0; i + 4 <= n; i += 4, v_base += (incv * 8), y_base += 8){
              v_0 = v_base[0];
              v_1 = v_base[1];
              v_2 = v_base[(incv * 2)];
              v_3 = v_base[((incv * 2) + 1)];
              v_4 = v_base[(incv * 4)];
              v_5 = v_base[((incv * 4) + 1)];
              v_6 = v_base[(incv * 6)];
              v_7 = v_base[((incv * 6) + 1)];
              y_0 = y_base[0];
              y_1 = y_base[1];
              y_2 = y_base[2];
              y_3 = y_base[3];
              y_4 = y_base[4];
              y_5 = y_base[5];
              y_6 = y_base[6];
              y_7 = y_base[7];
              v_8 = v_1 * y_1 * -1;
              v_9 = v_0 * y_1;
              v_10 = v_3 * y_3 * -1;
              v_11 = v_2 * y_3;
              v_12 = v_5 * y_5 * -1;
              v_13 = v_4 * y_5;
              v_14 = v_7 * y_7 * -1;
              v_15 = v_6 * y_7;
              v_0 = v_0 * y_0;
              v_1 = v_1 * y_0;
              v_2 = v_2 * y_2;
              v_3 = v_3 * y_2;
              v_4 = v_4 * y_4;
              v_5 = v_5 * y_4;
              v_6 = v_6 * y_6;
              v_7 = v_7 * y_6;
              for(j = 0; j < fold - 1; j++){
                s_0 = s_buffer[(j * 8)];
                s_1 = s_buffer[((j * 8) + 1)];
                s_2 = s_buffer[((j * 8) + 2)];
                s_3 = s_buffer[((j * 8) + 3)];
                s_4 = s_buffer[((j * 8) + 4)];
                s_5 = s_buffer[((j * 8) + 5)];
                s_6 = s_buffer[((j * 8) + 6)];
                s_7 = s_buffer[((j * 8) + 7)];
                tmp_BLP.d = v_0;
                tmp_BLP.l |= 1;
                q_0 = s_0 + tmp_BLP.d;
                tmp_BLP.d = v_1;
                tmp_BLP.l |= 1;
                q_1 = s_1 + tmp_BLP.d;
                tmp_BLP.d = v_2;
                tmp_BLP.l |= 1;
                q_2 = s_2 + tmp_BLP.d;
                tmp_BLP.d = v_3;
                tmp_BLP.l |= 1;
                q_3 = s_3 + tmp_BLP.d;
                tmp_BLP.d = v_4;
                tmp_BLP.l |= 1;
                q_4 = s_4 + tmp_BLP.d;
                tmp_BLP.d = v_5;
                tmp_BLP.l |= 1;
                q_5 = s_5 + tmp_BLP.d;
                tmp_BLP.d = v_6;
                tmp_BLP.l |= 1;
                q_6 = s_6 + tmp_BLP.d;
                tmp_BLP.d = v_7;
                tmp_BLP.l |= 1;
                q_7 = s_7 + tmp_BLP.d;
                s_buffer[(j * 8)] = q_0;
                s_buffer[((j * 8) + 1)] = q_1;
                s_buffer[((j * 8) + 2)] = q_2;
                s_buffer[((j * 8) + 3)] = q_3;
                s_buffer[((j * 8) + 4)] = q_4;
                s_buffer[((j * 8) + 5)] = q_5;
                s_buffer[((j * 8) + 6)] = q_6;
                s_buffer[((j * 8) + 7)] = q_7;
                q_0 = s_0 - q_0;
                q_1 = s_1 - q_1;
                q_2 = s_2 - q_2;
                q_3 = s_3 - q_3;
                q_4 = s_4 - q_4;
                q_5 = s_5 - q_5;
                q_6 = s_6 - q_6;
                q_7 = s_7 - q_7;
                v_0 = v_0 + q_0;
                v_1 = v_1 + q_1;
                v_2 = v_2 + q_2;
                v_3 = v_3 + q_3;
                v_4 = v_4 + q_4;
                v_5 = v_5 + q_5;
                v_6 = v_6 + q_6;
                v_7 = v_7 + q_7;
                s_0 = s_buffer[(j * 8)];
                s_1 = s_buffer[((j * 8) + 1)];
                s_2 = s_buffer[((j * 8) + 2)];
                s_3 = s_buffer[((j * 8) + 3)];
                s_4 = s_buffer[((j * 8) + 4)];
                s_5 = s_buffer[((j * 8) + 5)];
                s_6 = s_buffer[((j * 8) + 6)];
                s_7 = s_buffer[((j * 8) + 7)];
                tmp_BLP.d = v_8;
                tmp_BLP.l |= 1;
                q_0 = s_0 + tmp_BLP.d;
                tmp_BLP.d = v_9;
                tmp_BLP.l |= 1;
                q_1 = s_1 + tmp_BLP.d;
                tmp_BLP.d = v_10;
                tmp_BLP.l |= 1;
                q_2 = s_2 + tmp_BLP.d;
                tmp_BLP.d = v_11;
                tmp_BLP.l |= 1;
                q_3 = s_3 + tmp_BLP.d;
                tmp_BLP.d = v_12;
                tmp_BLP.l |= 1;
                q_4 = s_4 + tmp_BLP.d;
                tmp_BLP.d = v_13;
                tmp_BLP.l |= 1;
                q_5 = s_5 + tmp_BLP.d;
                tmp_BLP.d = v_14;
                tmp_BLP.l |= 1;
                q_6 = s_6 + tmp_BLP.d;
                tmp_BLP.d = v_15;
                tmp_BLP.l |= 1;
                q_7 = s_7 + tmp_BLP.d;
                s_buffer[(j * 8)] = q_0;
                s_buffer[((j * 8) + 1)] = q_1;
                s_buffer[((j * 8) + 2)] = q_2;
                s_buffer[((j * 8) + 3)] = q_3;
                s_buffer[((j * 8) + 4)] = q_4;
                s_buffer[((j * 8) + 5)] = q_5;
                s_buffer[((j * 8) + 6)] = q_6;
                s_buffer[((j * 8) + 7)] = q_7;
                q_0 = s_0 - q_0;
                q_1 = s_1 - q_1;
                q_2 = s_2 - q_2;
                q_3 = s_3 - q_3;
                q_4 = s_4 - q_4;
                q_5 = s_5 - q_5;
                q_6 = s_6 - q_6;
                q_7 = s_7 - q_7;
                v_8 = v_8 + q_0;
                v_9 = v_9 + q_1;
                v_10 = v_10 + q_2;
                v_11 = v_11 + q_3;
                v_12 = v_12 + q_4;
                v_13 = v_13 + q_5;
                v_14 = v_14 + q_6;
                v_15 = v_15 + q_7;
              }
              tmp_BLP.d = v_8;
              tmp_BLP.l |= 1;
              s_buffer[(j * 8)] = s_buffer[(j * 8)] + tmp_BLP.d;
              tmp_BLP.d = v_9;
              tmp_BLP.l |= 1;
              s_buffer[((j * 8) + 1)] = s_buffer[((j * 8) + 1)] + tmp_BLP.d;
              tmp_BLP.d = v_10;
              tmp_BLP.l |= 1;
              s_buffer[((j * 8) + 2)] = s_buffer[((j * 8) + 2)] + tmp_BLP.d;
              tmp_BLP.d = v_11;
              tmp_BLP.l |= 1;
              s_buffer[((j * 8) + 3)] = s_buffer[((j * 8) + 3)] + tmp_BLP.d;
              tmp_BLP.d = v_12;
              tmp_BLP.l |= 1;
              s_buffer[((j * 8) + 4)] = s_buffer[((j * 8) + 4)] + tmp_BLP.d;
              tmp_BLP.d = v_13;
              tmp_BLP.l |= 1;
              s_buffer[((j * 8) + 5)] = s_buffer[((j * 8) + 5)] + tmp_BLP.d;
              tmp_BLP.d = v_14;
              tmp_BLP.l |= 1;
              s_buffer[((j * 8) + 6)] = s_buffer[((j * 8) + 6)] + tmp_BLP.d;
              tmp_BLP.d = v_15;
              tmp_BLP.l |= 1;
              s_buffer[((j * 8) + 7)] = s_buffer[((j * 8) + 7)] + tmp_BLP.d;
            }
            if(i + 2 <= n){
              v_0 = v_base[0];
              v_1 = v_base[1];
              v_2 = v_base[(incv * 2)];
              v_3 = v_base[((incv * 2) + 1)];
              y_0 = y_base[0];
              y_1 = y_base[1];
              y_2 = y_base[2];
              y_3 = y_base[3];
              v_4 = v_1 * y_1 * -1;
              v_5 = v_0 * y_1;
              v_6 = v_3 * y_3 * -1;
              v_7 = v_2 * y_3;
              v_0 = v_0 * y_0;
              v_1 = v_1 * y_0;
              v_2 = v_2 * y_2;
              v_3 = v_3 * y_2;
              for(j = 0; j < fold - 1; j++){
                s_0 = s_buffer[(j * 8)];
                s_1 = s_buffer[((j * 8) + 1)];
                s_2 = s_buffer[((j * 8) + 2)];
                s_3 = s_buffer[((j * 8) + 3)];
                s_4 = s_buffer[((j * 8) + 4)];
                s_5 = s_buffer[((j * 8) + 5)];
                s_6 = s_buffer[((j * 8) + 6)];
                s_7 = s_buffer[((j * 8) + 7)];
                tmp_BLP.d = v_0;
                tmp_BLP.l |= 1;
                q_0 = s_0 + tmp_BLP.d;
                tmp_BLP.d = v_1;
                tmp_BLP.l |= 1;
                q_1 = s_1 + tmp_BLP.d;
                tmp_BLP.d = v_2;
                tmp_BLP.l |= 1;
                q_2 = s_2 + tmp_BLP.d;
                tmp_BLP.d = v_3;
                tmp_BLP.l |= 1;
                q_3 = s_3 + tmp_BLP.d;
                tmp_BLP.d = v_4;
                tmp_BLP.l |= 1;
                q_4 = s_4 + tmp_BLP.d;
                tmp_BLP.d = v_5;
                tmp_BLP.l |= 1;
                q_5 = s_5 + tmp_BLP.d;
                tmp_BLP.d = v_6;
                tmp_BLP.l |= 1;
                q_6 = s_6 + tmp_BLP.d;
                tmp_BLP.d = v_7;
                tmp_BLP.l |= 1;
                q_7 = s_7 + tmp_BLP.d;
                s_buffer[(j * 8)] = q_0;
                s_buffer[((j * 8) + 1)] = q_1;
                s_buffer[((j * 8) + 2)] = q_2;
                s_buffer[((j * 8) + 3)] = q_3;
                s_buffer[((j * 8) + 4)] = q_4;
                s_buffer[((j * 8) + 5)] = q_5;
                s_buffer[((j * 8) + 6)] = q_6;
                s_buffer[((j * 8) + 7)] = q_7;
                q_0 = s_0 - q_0;
                q_1 = s_1 - q_1;
                q_2 = s_2 - q_2;
                q_3 = s_3 - q_3;
                q_4 = s_4 - q_4;
                q_5 = s_5 - q_5;
                q_6 = s_6 - q_6;
                q_7 = s_7 - q_7;
                v_0 = v_0 + q_0;
                v_1 = v_1 + q_1;
                v_2 = v_2 + q_2;
                v_3 = v_3 + q_3;
                v_4 = v_4 + q_4;
                v_5 = v_5 + q_5;
                v_6 = v_6 + q_6;
                v_7 = v_7 + q_7;
              }
              tmp_BLP.d = v_0;
              tmp_BLP.l |= 1;
              s_buffer[(j * 8)] = s_buffer[(j * 8)] + tmp_BLP.d;
              tmp_BLP.d = v_1;
              tmp_BLP.l |= 1;
              s_buffer[((j * 8) + 1)] = s_buffer[((j * 8) + 1)] + tmp_BLP.d;
              tmp_BLP.d = v_2;
              tmp_BLP.l |= 1;
              s_buffer[((j * 8) + 2)] = s_buffer[((j * 8) + 2)] + tmp_BLP.d;
              tmp_BLP.d = v_3;
              tmp_BLP.l |= 1;
              s_buffer[((j * 8) + 3)] = s_buffer[((j * 8) + 3)] + tmp_BLP.d;
              tmp_BLP.d = v_4;
              tmp_BLP.l |= 1;
              s_buffer[((j * 8) + 4)] = s_buffer[((j * 8) + 4)] + tmp_BLP.d;
              tmp_BLP.d = v_5;
              tmp_BLP.l |= 1;
              s_buffer[((j * 8) + 5)] = s_buffer[((j * 8) + 5)] + tmp_BLP.d;
              tmp_BLP.d = v_6;
              tmp_BLP.l |= 1;
              s_buffer[((j * 8) + 6)] = s_buffer[((j * 8) + 6)] + tmp_BLP.d;
              tmp_BLP.d = v_7;
              tmp_BLP.l |= 1;
              s_buffer[((j * 8) + 7)] = s_buffer[((j * 8) + 7)] + tmp_BLP.d;
              i += 2, v_base += (incv * 4), y_base += 4;
            }
            if(i + 1 <= n){
              v_0 = v_base[0];
              v_1 = v_base[1];
              y_0 = y_base[0];
              y_1 = y_base[1];
              v_2 = v_1 * y_1 * -1;
              v_3 = v_0 * y_1;
              v_0 = v_0 * y_0;
              v_1 = v_1 * y_0;
              for(j = 0; j < fold - 1; j++){
                s_0 = s_buffer[(j * 8)];
                s_1 = s_buffer[((j * 8) + 1)];
                s_2 = s_buffer[((j * 8) + 2)];
                s_3 = s_buffer[((j * 8) + 3)];
                tmp_BLP.d = v_0;
                tmp_BLP.l |= 1;
                q_0 = s_0 + tmp_BLP.d;
                tmp_BLP.d = v_1;
                tmp_BLP.l |= 1;
                q_1 = s_1 + tmp_BLP.d;
                tmp_BLP.d = v_2;
                tmp_BLP.l |= 1;
                q_2 = s_2 + tmp_BLP.d;
                tmp_BLP.d = v_3;
                tmp_BLP.l |= 1;
                q_3 = s_3 + tmp_BLP.d;
                s_buffer[(j * 8)] = q_0;
                s_buffer[((j * 8) + 1)] = q_1;
                s_buffer[((j * 8) + 2)] = q_2;
                s_buffer[((j * 8) + 3)] = q_3;
                q_0 = s_0 - q_0;
                q_1 = s_1 - q_1;
                q_2 = s_2 - q_2;
                q_3 = s_3 - q_3;
                v_0 = v_0 + q_0;
                v_1 = v_1 + q_1;
                v_2 = v_2 + q_2;
                v_3 = v_3 + q_3;
              }
              tmp_BLP.d = v_0;
              tmp_BLP.l |= 1;
              s_buffer[(j * 8)] = s_buffer[(j * 8)] + tmp_BLP.d;
              tmp_BLP.d = v_1;
              tmp_BLP.l |= 1;
              s_buffer[((j * 8) + 1)] = s_buffer[((j * 8) + 1)] + tmp_BLP.d;
              tmp_BLP.d = v_2;
              tmp_BLP.l |= 1;
              s_buffer[((j * 8) + 2)] = s_buffer[((j * 8) + 2)] + tmp_BLP.d;
              tmp_BLP.d = v_3;
              tmp_BLP.l |= 1;
              s_buffer[((j * 8) + 3)] = s_buffer[((j * 8) + 3)] + tmp_BLP.d;
              i += 1, v_base += (incv * 2), y_base += 2;
            }
          }else{

            for(i = 0; i + 4 <= n; i += 4, v_base += (incv * 8), y_base += (incy * 8)){
              v_0 = v_base[0];
              v_1 = v_base[1];
              v_2 = v_base[(incv * 2)];
              v_3 = v_base[((incv * 2) + 1)];
              v_4 = v_base[(incv * 4)];
              v_5 = v_base[((incv * 4) + 1)];
              v_6 = v_base[(incv * 6)];
              v_7 = v_base[((incv * 6) + 1)];
              y_0 = y_base[0];
              y_1 = y_base[1];
              y_2 = y_base[(incy * 2)];
              y_3 = y_base[((incy * 2) + 1)];
              y_4 = y_base[(incy * 4)];
              y_5 = y_base[((incy * 4) + 1)];
              y_6 = y_base[(incy * 6)];
              y_7 = y_base[((incy * 6) + 1)];
              v_8 = v_1 * y_1 * -1;
              v_9 = v_0 * y_1;
              v_10 = v_3 * y_3 * -1;
              v_11 = v_2 * y_3;
              v_12 = v_5 * y_5 * -1;
              v_13 = v_4 * y_5;
              v_14 = v_7 * y_7 * -1;
              v_15 = v_6 * y_7;
              v_0 = v_0 * y_0;
              v_1 = v_1 * y_0;
              v_2 = v_2 * y_2;
              v_3 = v_3 * y_2;
              v_4 = v_4 * y_4;
              v_5 = v_5 * y_4;
              v_6 = v_6 * y_6;
              v_7 = v_7 * y_6;
              for(j = 0; j < fold - 1; j++){
                s_0 = s_buffer[(j * 8)];
                s_1 = s_buffer[((j * 8) + 1)];
                s_2 = s_buffer[((j * 8) + 2)];
                s_3 = s_buffer[((j * 8) + 3)];
                s_4 = s_buffer[((j * 8) + 4)];
                s_5 = s_buffer[((j * 8) + 5)];
                s_6 = s_buffer[((j * 8) + 6)];
                s_7 = s_buffer[((j * 8) + 7)];
                tmp_BLP.d = v_0;
                tmp_BLP.l |= 1;
                q_0 = s_0 + tmp_BLP.d;
                tmp_BLP.d = v_1;
                tmp_BLP.l |= 1;
                q_1 = s_1 + tmp_BLP.d;
                tmp_BLP.d = v_2;
                tmp_BLP.l |= 1;
                q_2 = s_2 + tmp_BLP.d;
                tmp_BLP.d = v_3;
                tmp_BLP.l |= 1;
                q_3 = s_3 + tmp_BLP.d;
                tmp_BLP.d = v_4;
                tmp_BLP.l |= 1;
                q_4 = s_4 + tmp_BLP.d;
                tmp_BLP.d = v_5;
                tmp_BLP.l |= 1;
                q_5 = s_5 + tmp_BLP.d;
                tmp_BLP.d = v_6;
                tmp_BLP.l |= 1;
                q_6 = s_6 + tmp_BLP.d;
                tmp_BLP.d = v_7;
                tmp_BLP.l |= 1;
                q_7 = s_7 + tmp_BLP.d;
                s_buffer[(j * 8)] = q_0;
                s_buffer[((j * 8) + 1)] = q_1;
                s_buffer[((j * 8) + 2)] = q_2;
                s_buffer[((j * 8) + 3)] = q_3;
                s_buffer[((j * 8) + 4)] = q_4;
                s_buffer[((j * 8) + 5)] = q_5;
                s_buffer[((j * 8) + 6)] = q_6;
                s_buffer[((j * 8) + 7)] = q_7;
                q_0 = s_0 - q_0;
                q_1 = s_1 - q_1;
                q_2 = s_2 - q_2;
                q_3 = s_3 - q_3;
                q_4 = s_4 - q_4;
                q_5 = s_5 - q_5;
                q_6 = s_6 - q_6;
                q_7 = s_7 - q_7;
                v_0 = v_0 + q_0;
                v_1 = v_1 + q_1;
                v_2 = v_2 + q_2;
                v_3 = v_3 + q_3;
                v_4 = v_4 + q_4;
                v_5 = v_5 + q_5;
                v_6 = v_6 + q_6;
                v_7 = v_7 + q_7;
                s_0 = s_buffer[(j * 8)];
                s_1 = s_buffer[((j * 8) + 1)];
                s_2 = s_buffer[((j * 8) + 2)];
                s_3 = s_buffer[((j * 8) + 3)];
                s_4 = s_buffer[((j * 8) + 4)];
                s_5 = s_buffer[((j * 8) + 5)];
                s_6 = s_buffer[((j * 8) + 6)];
                s_7 = s_buffer[((j * 8) + 7)];
                tmp_BLP.d = v_8;
                tmp_BLP.l |= 1;
                q_0 = s_0 + tmp_BLP.d;
                tmp_BLP.d = v_9;
                tmp_BLP.l |= 1;
                q_1 = s_1 + tmp_BLP.d;
                tmp_BLP.d = v_10;
                tmp_BLP.l |= 1;
                q_2 = s_2 + tmp_BLP.d;
                tmp_BLP.d = v_11;
                tmp_BLP.l |= 1;
                q_3 = s_3 + tmp_BLP.d;
                tmp_BLP.d = v_12;
                tmp_BLP.l |= 1;
                q_4 = s_4 + tmp_BLP.d;
                tmp_BLP.d = v_13;
                tmp_BLP.l |= 1;
                q_5 = s_5 + tmp_BLP.d;
                tmp_BLP.d = v_14;
                tmp_BLP.l |= 1;
                q_6 = s_6 + tmp_BLP.d;
                tmp_BLP.d = v_15;
                tmp_BLP.l |= 1;
                q_7 = s_7 + tmp_BLP.d;
                s_buffer[(j * 8)] = q_0;
                s_buffer[((j * 8) + 1)] = q_1;
                s_buffer[((j * 8) + 2)] = q_2;
                s_buffer[((j * 8) + 3)] = q_3;
                s_buffer[((j * 8) + 4)] = q_4;
                s_buffer[((j * 8) + 5)] = q_5;
                s_buffer[((j * 8) + 6)] = q_6;
                s_buffer[((j * 8) + 7)] = q_7;
                q_0 = s_0 - q_0;
                q_1 = s_1 - q_1;
                q_2 = s_2 - q_2;
                q_3 = s_3 - q_3;
                q_4 = s_4 - q_4;
                q_5 = s_5 - q_5;
                q_6 = s_6 - q_6;
                q_7 = s_7 - q_7;
                v_8 = v_8 + q_0;
                v_9 = v_9 + q_1;
                v_10 = v_10 + q_2;
                v_11 = v_11 + q_3;
                v_12 = v_12 + q_4;
                v_13 = v_13 + q_5;
                v_14 = v_14 + q_6;
                v_15 = v_15 + q_7;
              }
              tmp_BLP.d = v_8;
              tmp_BLP.l |= 1;
              s_buffer[(j * 8)] = s_buffer[(j * 8)] + tmp_BLP.d;
              tmp_BLP.d = v_9;
              tmp_BLP.l |= 1;
              s_buffer[((j * 8) + 1)] = s_buffer[((j * 8) + 1)] + tmp_BLP.d;
              tmp_BLP.d = v_10;
              tmp_BLP.l |= 1;
              s_buffer[((j * 8) + 2)] = s_buffer[((j * 8) + 2)] + tmp_BLP.d;
              tmp_BLP.d = v_11;
              tmp_BLP.l |= 1;
              s_buffer[((j * 8) + 3)] = s_buffer[((j * 8) + 3)] + tmp_BLP.d;
              tmp_BLP.d = v_12;
              tmp_BLP.l |= 1;
              s_buffer[((j * 8) + 4)] = s_buffer[((j * 8) + 4)] + tmp_BLP.d;
              tmp_BLP.d = v_13;
              tmp_BLP.l |= 1;
              s_buffer[((j * 8) + 5)] = s_buffer[((j * 8) + 5)] + tmp_BLP.d;
              tmp_BLP.d = v_14;
              tmp_BLP.l |= 1;
              s_buffer[((j * 8) + 6)] = s_buffer[((j * 8) + 6)] + tmp_BLP.d;
              tmp_BLP.d = v_15;
              tmp_BLP.l |= 1;
              s_buffer[((j * 8) + 7)] = s_buffer[((j * 8) + 7)] + tmp_BLP.d;
            }
            if(i + 2 <= n){
              v_0 = v_base[0];
              v_1 = v_base[1];
              v_2 = v_base[(incv * 2)];
              v_3 = v_base[((incv * 2) + 1)];
              y_0 = y_base[0];
              y_1 = y_base[1];
              y_2 = y_base[(incy * 2)];
              y_3 = y_base[((incy * 2) + 1)];
              v_4 = v_1 * y_1 * -1;
              v_5 = v_0 * y_1;
              v_6 = v_3 * y_3 * -1;
              v_7 = v_2 * y_3;
              v_0 = v_0 * y_0;
              v_1 = v_1 * y_0;
              v_2 = v_2 * y_2;
              v_3 = v_3 * y_2;
              for(j = 0; j < fold - 1; j++){
                s_0 = s_buffer[(j * 8)];
                s_1 = s_buffer[((j * 8) + 1)];
                s_2 = s_buffer[((j * 8) + 2)];
                s_3 = s_buffer[((j * 8) + 3)];
                s_4 = s_buffer[((j * 8) + 4)];
                s_5 = s_buffer[((j * 8) + 5)];
                s_6 = s_buffer[((j * 8) + 6)];
                s_7 = s_buffer[((j * 8) + 7)];
                tmp_BLP.d = v_0;
                tmp_BLP.l |= 1;
                q_0 = s_0 + tmp_BLP.d;
                tmp_BLP.d = v_1;
                tmp_BLP.l |= 1;
                q_1 = s_1 + tmp_BLP.d;
                tmp_BLP.d = v_2;
                tmp_BLP.l |= 1;
                q_2 = s_2 + tmp_BLP.d;
                tmp_BLP.d = v_3;
                tmp_BLP.l |= 1;
                q_3 = s_3 + tmp_BLP.d;
                tmp_BLP.d = v_4;
                tmp_BLP.l |= 1;
                q_4 = s_4 + tmp_BLP.d;
                tmp_BLP.d = v_5;
                tmp_BLP.l |= 1;
                q_5 = s_5 + tmp_BLP.d;
                tmp_BLP.d = v_6;
                tmp_BLP.l |= 1;
                q_6 = s_6 + tmp_BLP.d;
                tmp_BLP.d = v_7;
                tmp_BLP.l |= 1;
                q_7 = s_7 + tmp_BLP.d;
                s_buffer[(j * 8)] = q_0;
                s_buffer[((j * 8) + 1)] = q_1;
                s_buffer[((j * 8) + 2)] = q_2;
                s_buffer[((j * 8) + 3)] = q_3;
                s_buffer[((j * 8) + 4)] = q_4;
                s_buffer[((j * 8) + 5)] = q_5;
                s_buffer[((j * 8) + 6)] = q_6;
                s_buffer[((j * 8) + 7)] = q_7;
                q_0 = s_0 - q_0;
                q_1 = s_1 - q_1;
                q_2 = s_2 - q_2;
                q_3 = s_3 - q_3;
                q_4 = s_4 - q_4;
                q_5 = s_5 - q_5;
                q_6 = s_6 - q_6;
                q_7 = s_7 - q_7;
                v_0 = v_0 + q_0;
                v_1 = v_1 + q_1;
                v_2 = v_2 + q_2;
                v_3 = v_3 + q_3;
                v_4 = v_4 + q_4;
                v_5 = v_5 + q_5;
                v_6 = v_6 + q_6;
                v_7 = v_7 + q_7;
              }
              tmp_BLP.d = v_0;
              tmp_BLP.l |= 1;
              s_buffer[(j * 8)] = s_buffer[(j * 8)] + tmp_BLP.d;
              tmp_BLP.d = v_1;
              tmp_BLP.l |= 1;
              s_buffer[((j * 8) + 1)] = s_buffer[((j * 8) + 1)] + tmp_BLP.d;
              tmp_BLP.d = v_2;
              tmp_BLP.l |= 1;
              s_buffer[((j * 8) + 2)] = s_buffer[((j * 8) + 2)] + tmp_BLP.d;
              tmp_BLP.d = v_3;
              tmp_BLP.l |= 1;
              s_buffer[((j * 8) + 3)] = s_buffer[((j * 8) + 3)] + tmp_BLP.d;
              tmp_BLP.d = v_4;
              tmp_BLP.l |= 1;
              s_buffer[((j * 8) + 4)] = s_buffer[((j * 8) + 4)] + tmp_BLP.d;
              tmp_BLP.d = v_5;
              tmp_BLP.l |= 1;
              s_buffer[((j * 8) + 5)] = s_buffer[((j * 8) + 5)] + tmp_BLP.d;
              tmp_BLP.d = v_6;
              tmp_BLP.l |= 1;
              s_buffer[((j * 8) + 6)] = s_buffer[((j * 8) + 6)] + tmp_BLP.d;
              tmp_BLP.d = v_7;
              tmp_BLP.l |= 1;
              s_buffer[((j * 8) + 7)] = s_buffer[((j * 8) + 7)] + tmp_BLP.d;
              i += 2, v_base += (incv * 4), y_base += (incy * 4);
            }
            if(i + 1 <= n){
              v_0 = v_base[0];
              v_1 = v_base[1];
              y_0 = y_base[0];
              y_1 = y_base[1];
              v_2 = v_1 * y_1 * -1;
              v_3 = v_0 * y_1;
              v_0 = v_0 * y_0;
              v_1 = v_1 * y_0;
              for(j = 0; j < fold - 1; j++){
                s_0 = s_buffer[(j * 8)];
                s_1 = s_buffer[((j * 8) + 1)];
                s_2 = s_buffer[((j * 8) + 2)];
                s_3 = s_buffer[((j * 8) + 3)];
                tmp_BLP.d = v_0;
                tmp_BLP.l |= 1;
                q_0 = s_0 + tmp_BLP.d;
                tmp_BLP.d = v_1;
                tmp_BLP.l |= 1;
                q_1 = s_1 + tmp_BLP.d;
                tmp_BLP.d = v_2;
                tmp_BLP.l |= 1;
                q_2 = s_2 + tmp_BLP.d;
                tmp_BLP.d = v_3;
                tmp_BLP.l |= 1;
                q_3 = s_3 + tmp_BLP.d;
                s_buffer[(j * 8)] = q_0;
                s_buffer[((j * 8) + 1)] = q_1;
                s_buffer[((j * 8) + 2)] = q_2;
                s_buffer[((j * 8) + 3)] = q_3;
                q_0 = s_0 - q_0;
                q_1 = s_1 - q_1;
                q_2 = s_2 - q_2;
                q_3 = s_3 - q_3;
                v_0 = v_0 + q_0;
                v_1 = v_1 + q_1;
                v_2 = v_2 + q_2;
                v_3 = v_3 + q_3;
              }
              tmp_BLP.d = v_0;
              tmp_BLP.l |= 1;
              s_buffer[(j * 8)] = s_buffer[(j * 8)] + tmp_BLP.d;
              tmp_BLP.d = v_1;
              tmp_BLP.l |= 1;
              s_buffer[((j * 8) + 1)] = s_buffer[((j * 8) + 1)] + tmp_BLP.d;
              tmp_BLP.d = v_2;
              tmp_BLP.l |= 1;
              s_buffer[((j * 8) + 2)] = s_buffer[((j * 8) + 2)] + tmp_BLP.d;
              tmp_BLP.d = v_3;
              tmp_BLP.l |= 1;
              s_buffer[((j * 8) + 3)] = s_buffer[((j * 8) + 3)] + tmp_BLP.d;
              i += 1, v_base += (incv * 2), y_base += (incy * 2);
            }
          }
        }
        for(j = 0; j < fold; j += 1){
          q_0 = ((double*)sum)[(j * 2)];
          s_buffer[(j * 8)] = s_buffer[(j * 8)] + (s_buffer[((j * 8) + 2)] - q_0);
          s_buffer[(j * 8)] = s_buffer[(j * 8)] + (s_buffer[((j * 8) + 4)] - q_0);
          s_buffer[(j * 8)] = s_buffer[(j * 8)] + (s_buffer[((j * 8) + 6)] - q_0);
          q_0 = ((double*)sum)[((j * 2) + 1)];
          s_buffer[((j * 8) + 1)] = s_buffer[((j * 8) + 1)] + (s_buffer[((j * 8) + 3)] - q_0);
          s_buffer[((j * 8) + 1)] = s_buffer[((j * 8) + 1)] + (s_buffer[((j * 8) + 5)] - q_0);
          s_buffer[((j * 8) + 1)] = s_buffer[((j * 8) + 1)] + (s_buffer[((j * 8) + 7)] - q_0);
          ((double*)sum)[(j * 2)] = s_buffer[(j * 8)];
          ((double*)sum)[((j * 2) + 1)] = s_buffer[((j * 8) + 1)];
        }
        RESET_DAZ_FLAG
        return;
      }
    }
    //[[[end]]]
  }
#endif
