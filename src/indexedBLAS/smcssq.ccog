/*
 *  Created   13/10/25   H.D. Nguyen
 */

#include <stdlib.h>
#include <stdio.h>
#include <math.h>
#include <float.h>

#include "../config.h"
#include "../common/common.h"
#include "indexedBLAS.h"

/*[[[cog
import cog
import generate
import dataTypes
import depositSSq
import vectorizations
from src.common import blockSize
from scripts import terminal

code_block = generate.CodeBlock()
vectorizations.conditionally_include_vectorizations(code_block)
cog.out(str(code_block))

cog.outl()

cog.out(generate.generate(blockSize.BlockSize("smcssq", "N_block_MAX", 32, terminal.get_siendurance()//2, terminal.get_siendurance()//2, ["bench_rscnrm2_fold_{}".format(terminal.get_sidefaultfold())]), cog.inFile, args, params, mode))
]]]*/
#ifdef __AVX__
  #include <immintrin.h>

#elif defined(__SSE2__)
  #include <emmintrin.h>

#else


#endif

#define N_block_MAX 256
//[[[end]]]

float idxdBLAS_smcssq(const int fold, const int N, const void *X, const int incX, const float scaleY, float *manY, const int incmanY, float *carY, const int inccarY){
  float amax_tmp[2];
  float amax;
  float scl = 0.0;
  float new_scl;
  int i, j;
  int N_block = N_block_MAX;
  int deposits = 0;
  float_complex_indexed *ssq = idxd_cialloc(fold);
  idxd_cisetzero(fold, ssq);

  const float *x = (const float*)X;


  for (i = 0; i < N; i += N_block) {
    N_block = MIN((N - i), N_block);

    idxdBLAS_camax_sub(N_block, x, incX, amax_tmp);
    amax = MAX(amax_tmp[0], amax_tmp[1]);

    if (isinf(amax) || isinf(manY[0])){
      for (j = 0; j < N_block; j++){
        manY[0] += fabsf(x[j * 2 * incX]);
        manY[0] += fabsf(x[j * 2 * incX + 1]);
      }
    }
    if (isnan(manY[0]) || isnan(ssq[0]) || isnan(ssq[1])){
      manY[0] += ssq[0] + ssq[1];
      free(ssq);
      return idxd_sscale(1.0);
    } else if (isinf(manY[0])){
      x += N_block * 2 * incX;
      continue;
    }

    if (deposits + N_block > idxd_SIENDURANCE) {
      idxd_cirenorm(fold, ssq);
      deposits = 0;
    }

    new_scl = idxd_sscale(amax);
    if (new_scl > scl) {
      if(scl > 0.0){
        idxd_cmsrescale(fold, new_scl, scl, ssq, 1, ssq + 2 * fold, 1);
      }
      scl = new_scl;
    }

    amax /= scl;
    amax = amax * amax;

    idxd_cisupdate(fold, amax, ssq);

    /*[[[cog
      cog.out(generate.generate(depositSSq.DepositSSq(dataTypes.FloatComplex, "fold", "N_block", "x", "incX", "ssq", 1, "scl"), cog.inFile, args, params, mode))
      ]]]*/
    {
      #ifdef __AVX__
        __m256 scale_mask = _mm256_set1_ps(scl);
        __m256 blp_mask_tmp;
        {
          __m256 tmp;
          blp_mask_tmp = _mm256_set1_ps(1.0);
          tmp = _mm256_set1_ps(1.0 + (FLT_EPSILON * 1.0001));
          blp_mask_tmp = _mm256_xor_ps(blp_mask_tmp, tmp);
        }
        __m256 cons_tmp; (void)cons_tmp;
        float cons_buffer_tmp[8] __attribute__((aligned(32))); (void)cons_buffer_tmp;
        unsigned int SIMD_daz_ftz_old_tmp = 0;
        unsigned int SIMD_daz_ftz_new_tmp = 0;


        switch(fold){
          case 3:
            {
              int i;
              __m256 x_0;
              __m256 compression_0;
              __m256 expansion_0;
              __m256 expansion_mask_0;
              __m256 q_0;
              __m256 s_0_0;
              __m256 s_1_0;
              __m256 s_2_0;

              s_0_0 = (__m256)_mm256_broadcast_sd((double *)(((float*)ssq)));
              s_1_0 = (__m256)_mm256_broadcast_sd((double *)(((float*)ssq) + 2));
              s_2_0 = (__m256)_mm256_broadcast_sd((double *)(((float*)ssq) + 4));

              if(incX == 1){
                if(idxd_smindex0(ssq) || idxd_smindex0(ssq + 1)){
                  if(idxd_smindex0(ssq)){
                    if(idxd_smindex0(ssq + 1)){
                      compression_0 = _mm256_set1_ps(idxd_SMCOMPRESSION);
                      expansion_0 = _mm256_set1_ps(idxd_SMEXPANSION * 0.5);
                      expansion_mask_0 = _mm256_set1_ps(idxd_SMEXPANSION * 0.5);
                    }else{
                      compression_0 = _mm256_set_ps(1.0, idxd_SMCOMPRESSION, 1.0, idxd_SMCOMPRESSION, 1.0, idxd_SMCOMPRESSION, 1.0, idxd_SMCOMPRESSION);
                      expansion_0 = _mm256_set_ps(1.0, idxd_SMEXPANSION * 0.5, 1.0, idxd_SMEXPANSION * 0.5, 1.0, idxd_SMEXPANSION * 0.5, 1.0, idxd_SMEXPANSION * 0.5);
                      expansion_mask_0 = _mm256_set_ps(0.0, idxd_SMEXPANSION * 0.5, 0.0, idxd_SMEXPANSION * 0.5, 0.0, idxd_SMEXPANSION * 0.5, 0.0, idxd_SMEXPANSION * 0.5);
                    }
                  }else{
                    compression_0 = _mm256_set_ps(idxd_SMCOMPRESSION, 1.0, idxd_SMCOMPRESSION, 1.0, idxd_SMCOMPRESSION, 1.0, idxd_SMCOMPRESSION, 1.0);
                    expansion_0 = _mm256_set_ps(idxd_SMEXPANSION * 0.5, 1.0, idxd_SMEXPANSION * 0.5, 1.0, idxd_SMEXPANSION * 0.5, 1.0, idxd_SMEXPANSION * 0.5, 1.0);
                    expansion_mask_0 = _mm256_set_ps(idxd_SMEXPANSION * 0.5, 0.0, idxd_SMEXPANSION * 0.5, 0.0, idxd_SMEXPANSION * 0.5, 0.0, idxd_SMEXPANSION * 0.5, 0.0);
                  }
                  for(i = 0; i + 4 <= N_block; i += 4, x += 8){
                    x_0 = _mm256_div_ps(_mm256_loadu_ps(((float*)x)), scale_mask);
                    x_0 = _mm256_mul_ps(x_0, x_0);

                    q_0 = s_0_0;
                    s_0_0 = _mm256_add_ps(s_0_0, _mm256_or_ps(_mm256_mul_ps(x_0, compression_0), blp_mask_tmp));
                    q_0 = _mm256_sub_ps(q_0, s_0_0);
                    x_0 = _mm256_add_ps(_mm256_add_ps(x_0, _mm256_mul_ps(q_0, expansion_0)), _mm256_mul_ps(q_0, expansion_mask_0));
                    q_0 = s_1_0;
                    s_1_0 = _mm256_add_ps(s_1_0, _mm256_or_ps(x_0, blp_mask_tmp));
                    q_0 = _mm256_sub_ps(q_0, s_1_0);
                    x_0 = _mm256_add_ps(x_0, q_0);
                    s_2_0 = _mm256_add_ps(s_2_0, _mm256_or_ps(x_0, blp_mask_tmp));
                  }
                  if(i < N_block){
                    x_0 = _mm256_div_ps((__m256)_mm256_set_pd(0, (N_block - i)>2?((double*)((float*)x))[2]:0, (N_block - i)>1?((double*)((float*)x))[1]:0, ((double*)((float*)x))[0]), scale_mask);
                    x_0 = _mm256_mul_ps(x_0, x_0);

                    q_0 = s_0_0;
                    s_0_0 = _mm256_add_ps(s_0_0, _mm256_or_ps(_mm256_mul_ps(x_0, compression_0), blp_mask_tmp));
                    q_0 = _mm256_sub_ps(q_0, s_0_0);
                    x_0 = _mm256_add_ps(_mm256_add_ps(x_0, _mm256_mul_ps(q_0, expansion_0)), _mm256_mul_ps(q_0, expansion_mask_0));
                    q_0 = s_1_0;
                    s_1_0 = _mm256_add_ps(s_1_0, _mm256_or_ps(x_0, blp_mask_tmp));
                    q_0 = _mm256_sub_ps(q_0, s_1_0);
                    x_0 = _mm256_add_ps(x_0, q_0);
                    s_2_0 = _mm256_add_ps(s_2_0, _mm256_or_ps(x_0, blp_mask_tmp));
                    x += ((N_block - i) * 2);
                  }
                }else{
                  for(i = 0; i + 4 <= N_block; i += 4, x += 8){
                    x_0 = _mm256_div_ps(_mm256_loadu_ps(((float*)x)), scale_mask);
                    x_0 = _mm256_mul_ps(x_0, x_0);

                    q_0 = s_0_0;
                    s_0_0 = _mm256_add_ps(s_0_0, _mm256_or_ps(x_0, blp_mask_tmp));
                    q_0 = _mm256_sub_ps(q_0, s_0_0);
                    x_0 = _mm256_add_ps(x_0, q_0);
                    q_0 = s_1_0;
                    s_1_0 = _mm256_add_ps(s_1_0, _mm256_or_ps(x_0, blp_mask_tmp));
                    q_0 = _mm256_sub_ps(q_0, s_1_0);
                    x_0 = _mm256_add_ps(x_0, q_0);
                    s_2_0 = _mm256_add_ps(s_2_0, _mm256_or_ps(x_0, blp_mask_tmp));
                  }
                  if(i < N_block){
                    x_0 = _mm256_div_ps((__m256)_mm256_set_pd(0, (N_block - i)>2?((double*)((float*)x))[2]:0, (N_block - i)>1?((double*)((float*)x))[1]:0, ((double*)((float*)x))[0]), scale_mask);
                    x_0 = _mm256_mul_ps(x_0, x_0);

                    q_0 = s_0_0;
                    s_0_0 = _mm256_add_ps(s_0_0, _mm256_or_ps(x_0, blp_mask_tmp));
                    q_0 = _mm256_sub_ps(q_0, s_0_0);
                    x_0 = _mm256_add_ps(x_0, q_0);
                    q_0 = s_1_0;
                    s_1_0 = _mm256_add_ps(s_1_0, _mm256_or_ps(x_0, blp_mask_tmp));
                    q_0 = _mm256_sub_ps(q_0, s_1_0);
                    x_0 = _mm256_add_ps(x_0, q_0);
                    s_2_0 = _mm256_add_ps(s_2_0, _mm256_or_ps(x_0, blp_mask_tmp));
                    x += ((N_block - i) * 2);
                  }
                }
              }else{
                if(idxd_smindex0(ssq) || idxd_smindex0(ssq + 1)){
                  if(idxd_smindex0(ssq)){
                    if(idxd_smindex0(ssq + 1)){
                      compression_0 = _mm256_set1_ps(idxd_SMCOMPRESSION);
                      expansion_0 = _mm256_set1_ps(idxd_SMEXPANSION * 0.5);
                      expansion_mask_0 = _mm256_set1_ps(idxd_SMEXPANSION * 0.5);
                    }else{
                      compression_0 = _mm256_set_ps(1.0, idxd_SMCOMPRESSION, 1.0, idxd_SMCOMPRESSION, 1.0, idxd_SMCOMPRESSION, 1.0, idxd_SMCOMPRESSION);
                      expansion_0 = _mm256_set_ps(1.0, idxd_SMEXPANSION * 0.5, 1.0, idxd_SMEXPANSION * 0.5, 1.0, idxd_SMEXPANSION * 0.5, 1.0, idxd_SMEXPANSION * 0.5);
                      expansion_mask_0 = _mm256_set_ps(0.0, idxd_SMEXPANSION * 0.5, 0.0, idxd_SMEXPANSION * 0.5, 0.0, idxd_SMEXPANSION * 0.5, 0.0, idxd_SMEXPANSION * 0.5);
                    }
                  }else{
                    compression_0 = _mm256_set_ps(idxd_SMCOMPRESSION, 1.0, idxd_SMCOMPRESSION, 1.0, idxd_SMCOMPRESSION, 1.0, idxd_SMCOMPRESSION, 1.0);
                    expansion_0 = _mm256_set_ps(idxd_SMEXPANSION * 0.5, 1.0, idxd_SMEXPANSION * 0.5, 1.0, idxd_SMEXPANSION * 0.5, 1.0, idxd_SMEXPANSION * 0.5, 1.0);
                    expansion_mask_0 = _mm256_set_ps(idxd_SMEXPANSION * 0.5, 0.0, idxd_SMEXPANSION * 0.5, 0.0, idxd_SMEXPANSION * 0.5, 0.0, idxd_SMEXPANSION * 0.5, 0.0);
                  }
                  for(i = 0; i + 4 <= N_block; i += 4, x += (incX * 8)){
                    x_0 = _mm256_div_ps(_mm256_set_ps(((float*)x)[((incX * 6) + 1)], ((float*)x)[(incX * 6)], ((float*)x)[((incX * 4) + 1)], ((float*)x)[(incX * 4)], ((float*)x)[((incX * 2) + 1)], ((float*)x)[(incX * 2)], ((float*)x)[1], ((float*)x)[0]), scale_mask);
                    x_0 = _mm256_mul_ps(x_0, x_0);

                    q_0 = s_0_0;
                    s_0_0 = _mm256_add_ps(s_0_0, _mm256_or_ps(_mm256_mul_ps(x_0, compression_0), blp_mask_tmp));
                    q_0 = _mm256_sub_ps(q_0, s_0_0);
                    x_0 = _mm256_add_ps(_mm256_add_ps(x_0, _mm256_mul_ps(q_0, expansion_0)), _mm256_mul_ps(q_0, expansion_mask_0));
                    q_0 = s_1_0;
                    s_1_0 = _mm256_add_ps(s_1_0, _mm256_or_ps(x_0, blp_mask_tmp));
                    q_0 = _mm256_sub_ps(q_0, s_1_0);
                    x_0 = _mm256_add_ps(x_0, q_0);
                    s_2_0 = _mm256_add_ps(s_2_0, _mm256_or_ps(x_0, blp_mask_tmp));
                  }
                  if(i < N_block){
                    x_0 = _mm256_div_ps((__m256)_mm256_set_pd(0, (N_block - i)>2?((double*)((float*)x))[(incX * 2)]:0, (N_block - i)>1?((double*)((float*)x))[incX]:0, ((double*)((float*)x))[0]), scale_mask);
                    x_0 = _mm256_mul_ps(x_0, x_0);

                    q_0 = s_0_0;
                    s_0_0 = _mm256_add_ps(s_0_0, _mm256_or_ps(_mm256_mul_ps(x_0, compression_0), blp_mask_tmp));
                    q_0 = _mm256_sub_ps(q_0, s_0_0);
                    x_0 = _mm256_add_ps(_mm256_add_ps(x_0, _mm256_mul_ps(q_0, expansion_0)), _mm256_mul_ps(q_0, expansion_mask_0));
                    q_0 = s_1_0;
                    s_1_0 = _mm256_add_ps(s_1_0, _mm256_or_ps(x_0, blp_mask_tmp));
                    q_0 = _mm256_sub_ps(q_0, s_1_0);
                    x_0 = _mm256_add_ps(x_0, q_0);
                    s_2_0 = _mm256_add_ps(s_2_0, _mm256_or_ps(x_0, blp_mask_tmp));
                    x += (incX * (N_block - i) * 2);
                  }
                }else{
                  for(i = 0; i + 4 <= N_block; i += 4, x += (incX * 8)){
                    x_0 = _mm256_div_ps(_mm256_set_ps(((float*)x)[((incX * 6) + 1)], ((float*)x)[(incX * 6)], ((float*)x)[((incX * 4) + 1)], ((float*)x)[(incX * 4)], ((float*)x)[((incX * 2) + 1)], ((float*)x)[(incX * 2)], ((float*)x)[1], ((float*)x)[0]), scale_mask);
                    x_0 = _mm256_mul_ps(x_0, x_0);

                    q_0 = s_0_0;
                    s_0_0 = _mm256_add_ps(s_0_0, _mm256_or_ps(x_0, blp_mask_tmp));
                    q_0 = _mm256_sub_ps(q_0, s_0_0);
                    x_0 = _mm256_add_ps(x_0, q_0);
                    q_0 = s_1_0;
                    s_1_0 = _mm256_add_ps(s_1_0, _mm256_or_ps(x_0, blp_mask_tmp));
                    q_0 = _mm256_sub_ps(q_0, s_1_0);
                    x_0 = _mm256_add_ps(x_0, q_0);
                    s_2_0 = _mm256_add_ps(s_2_0, _mm256_or_ps(x_0, blp_mask_tmp));
                  }
                  if(i < N_block){
                    x_0 = _mm256_div_ps((__m256)_mm256_set_pd(0, (N_block - i)>2?((double*)((float*)x))[(incX * 2)]:0, (N_block - i)>1?((double*)((float*)x))[incX]:0, ((double*)((float*)x))[0]), scale_mask);
                    x_0 = _mm256_mul_ps(x_0, x_0);

                    q_0 = s_0_0;
                    s_0_0 = _mm256_add_ps(s_0_0, _mm256_or_ps(x_0, blp_mask_tmp));
                    q_0 = _mm256_sub_ps(q_0, s_0_0);
                    x_0 = _mm256_add_ps(x_0, q_0);
                    q_0 = s_1_0;
                    s_1_0 = _mm256_add_ps(s_1_0, _mm256_or_ps(x_0, blp_mask_tmp));
                    q_0 = _mm256_sub_ps(q_0, s_1_0);
                    x_0 = _mm256_add_ps(x_0, q_0);
                    s_2_0 = _mm256_add_ps(s_2_0, _mm256_or_ps(x_0, blp_mask_tmp));
                    x += (incX * (N_block - i) * 2);
                  }
                }
              }

              s_0_0 = _mm256_sub_ps(s_0_0, _mm256_set_ps(((float*)ssq)[1], ((float*)ssq)[0], ((float*)ssq)[1], ((float*)ssq)[0], ((float*)ssq)[1], ((float*)ssq)[0], 0, 0));
              _mm256_store_ps(cons_buffer_tmp, s_0_0);
              ((float*)ssq)[0] = cons_buffer_tmp[0] + cons_buffer_tmp[2] + cons_buffer_tmp[4] + cons_buffer_tmp[6];
              ((float*)ssq)[1] = cons_buffer_tmp[1] + cons_buffer_tmp[3] + cons_buffer_tmp[5] + cons_buffer_tmp[7];
              s_1_0 = _mm256_sub_ps(s_1_0, _mm256_set_ps(((float*)ssq)[3], ((float*)ssq)[2], ((float*)ssq)[3], ((float*)ssq)[2], ((float*)ssq)[3], ((float*)ssq)[2], 0, 0));
              _mm256_store_ps(cons_buffer_tmp, s_1_0);
              ((float*)ssq)[2] = cons_buffer_tmp[0] + cons_buffer_tmp[2] + cons_buffer_tmp[4] + cons_buffer_tmp[6];
              ((float*)ssq)[3] = cons_buffer_tmp[1] + cons_buffer_tmp[3] + cons_buffer_tmp[5] + cons_buffer_tmp[7];
              s_2_0 = _mm256_sub_ps(s_2_0, _mm256_set_ps(((float*)ssq)[5], ((float*)ssq)[4], ((float*)ssq)[5], ((float*)ssq)[4], ((float*)ssq)[5], ((float*)ssq)[4], 0, 0));
              _mm256_store_ps(cons_buffer_tmp, s_2_0);
              ((float*)ssq)[4] = cons_buffer_tmp[0] + cons_buffer_tmp[2] + cons_buffer_tmp[4] + cons_buffer_tmp[6];
              ((float*)ssq)[5] = cons_buffer_tmp[1] + cons_buffer_tmp[3] + cons_buffer_tmp[5] + cons_buffer_tmp[7];

              if(SIMD_daz_ftz_new_tmp != SIMD_daz_ftz_old_tmp){
                _mm_setcsr(SIMD_daz_ftz_old_tmp);
              }
            }
            break;
          default:
            {
              int i, j;
              __m256 x_0;
              __m256 compression_0;
              __m256 expansion_0;
              __m256 expansion_mask_0;
              __m256 q_0;
              __m256 s_0;
              __m256 s_buffer[SIMAXFOLD];

              for(j = 0; j < fold; j += 1){
                s_buffer[j] = (__m256)_mm256_broadcast_sd((double *)(((float*)ssq) + (j * 2)));
              }

              if(incX == 1){
                if(idxd_smindex0(ssq) || idxd_smindex0(ssq + 1)){
                  if(idxd_smindex0(ssq)){
                    if(idxd_smindex0(ssq + 1)){
                      compression_0 = _mm256_set1_ps(idxd_SMCOMPRESSION);
                      expansion_0 = _mm256_set1_ps(idxd_SMEXPANSION * 0.5);
                      expansion_mask_0 = _mm256_set1_ps(idxd_SMEXPANSION * 0.5);
                    }else{
                      compression_0 = _mm256_set_ps(1.0, idxd_SMCOMPRESSION, 1.0, idxd_SMCOMPRESSION, 1.0, idxd_SMCOMPRESSION, 1.0, idxd_SMCOMPRESSION);
                      expansion_0 = _mm256_set_ps(1.0, idxd_SMEXPANSION * 0.5, 1.0, idxd_SMEXPANSION * 0.5, 1.0, idxd_SMEXPANSION * 0.5, 1.0, idxd_SMEXPANSION * 0.5);
                      expansion_mask_0 = _mm256_set_ps(0.0, idxd_SMEXPANSION * 0.5, 0.0, idxd_SMEXPANSION * 0.5, 0.0, idxd_SMEXPANSION * 0.5, 0.0, idxd_SMEXPANSION * 0.5);
                    }
                  }else{
                    compression_0 = _mm256_set_ps(idxd_SMCOMPRESSION, 1.0, idxd_SMCOMPRESSION, 1.0, idxd_SMCOMPRESSION, 1.0, idxd_SMCOMPRESSION, 1.0);
                    expansion_0 = _mm256_set_ps(idxd_SMEXPANSION * 0.5, 1.0, idxd_SMEXPANSION * 0.5, 1.0, idxd_SMEXPANSION * 0.5, 1.0, idxd_SMEXPANSION * 0.5, 1.0);
                    expansion_mask_0 = _mm256_set_ps(idxd_SMEXPANSION * 0.5, 0.0, idxd_SMEXPANSION * 0.5, 0.0, idxd_SMEXPANSION * 0.5, 0.0, idxd_SMEXPANSION * 0.5, 0.0);
                  }
                  for(i = 0; i + 4 <= N_block; i += 4, x += 8){
                    x_0 = _mm256_div_ps(_mm256_loadu_ps(((float*)x)), scale_mask);
                    x_0 = _mm256_mul_ps(x_0, x_0);

                    s_0 = s_buffer[0];
                    q_0 = _mm256_add_ps(s_0, _mm256_or_ps(_mm256_mul_ps(x_0, compression_0), blp_mask_tmp));
                    s_buffer[0] = q_0;
                    q_0 = _mm256_sub_ps(s_0, q_0);
                    x_0 = _mm256_add_ps(_mm256_add_ps(x_0, _mm256_mul_ps(q_0, expansion_0)), _mm256_mul_ps(q_0, expansion_mask_0));
                    for(j = 1; j < fold - 1; j++){
                      s_0 = s_buffer[j];
                      q_0 = _mm256_add_ps(s_0, _mm256_or_ps(x_0, blp_mask_tmp));
                      s_buffer[j] = q_0;
                      q_0 = _mm256_sub_ps(s_0, q_0);
                      x_0 = _mm256_add_ps(x_0, q_0);
                    }
                    s_buffer[j] = _mm256_add_ps(s_buffer[j], _mm256_or_ps(x_0, blp_mask_tmp));
                  }
                  if(i < N_block){
                    x_0 = _mm256_div_ps((__m256)_mm256_set_pd(0, (N_block - i)>2?((double*)((float*)x))[2]:0, (N_block - i)>1?((double*)((float*)x))[1]:0, ((double*)((float*)x))[0]), scale_mask);
                    x_0 = _mm256_mul_ps(x_0, x_0);

                    s_0 = s_buffer[0];
                    q_0 = _mm256_add_ps(s_0, _mm256_or_ps(_mm256_mul_ps(x_0, compression_0), blp_mask_tmp));
                    s_buffer[0] = q_0;
                    q_0 = _mm256_sub_ps(s_0, q_0);
                    x_0 = _mm256_add_ps(_mm256_add_ps(x_0, _mm256_mul_ps(q_0, expansion_0)), _mm256_mul_ps(q_0, expansion_mask_0));
                    for(j = 1; j < fold - 1; j++){
                      s_0 = s_buffer[j];
                      q_0 = _mm256_add_ps(s_0, _mm256_or_ps(x_0, blp_mask_tmp));
                      s_buffer[j] = q_0;
                      q_0 = _mm256_sub_ps(s_0, q_0);
                      x_0 = _mm256_add_ps(x_0, q_0);
                    }
                    s_buffer[j] = _mm256_add_ps(s_buffer[j], _mm256_or_ps(x_0, blp_mask_tmp));
                    x += ((N_block - i) * 2);
                  }
                }else{
                  for(i = 0; i + 4 <= N_block; i += 4, x += 8){
                    x_0 = _mm256_div_ps(_mm256_loadu_ps(((float*)x)), scale_mask);
                    x_0 = _mm256_mul_ps(x_0, x_0);

                    for(j = 0; j < fold - 1; j++){
                      s_0 = s_buffer[j];
                      q_0 = _mm256_add_ps(s_0, _mm256_or_ps(x_0, blp_mask_tmp));
                      s_buffer[j] = q_0;
                      q_0 = _mm256_sub_ps(s_0, q_0);
                      x_0 = _mm256_add_ps(x_0, q_0);
                    }
                    s_buffer[j] = _mm256_add_ps(s_buffer[j], _mm256_or_ps(x_0, blp_mask_tmp));
                  }
                  if(i < N_block){
                    x_0 = _mm256_div_ps((__m256)_mm256_set_pd(0, (N_block - i)>2?((double*)((float*)x))[2]:0, (N_block - i)>1?((double*)((float*)x))[1]:0, ((double*)((float*)x))[0]), scale_mask);
                    x_0 = _mm256_mul_ps(x_0, x_0);

                    for(j = 0; j < fold - 1; j++){
                      s_0 = s_buffer[j];
                      q_0 = _mm256_add_ps(s_0, _mm256_or_ps(x_0, blp_mask_tmp));
                      s_buffer[j] = q_0;
                      q_0 = _mm256_sub_ps(s_0, q_0);
                      x_0 = _mm256_add_ps(x_0, q_0);
                    }
                    s_buffer[j] = _mm256_add_ps(s_buffer[j], _mm256_or_ps(x_0, blp_mask_tmp));
                    x += ((N_block - i) * 2);
                  }
                }
              }else{
                if(idxd_smindex0(ssq) || idxd_smindex0(ssq + 1)){
                  if(idxd_smindex0(ssq)){
                    if(idxd_smindex0(ssq + 1)){
                      compression_0 = _mm256_set1_ps(idxd_SMCOMPRESSION);
                      expansion_0 = _mm256_set1_ps(idxd_SMEXPANSION * 0.5);
                      expansion_mask_0 = _mm256_set1_ps(idxd_SMEXPANSION * 0.5);
                    }else{
                      compression_0 = _mm256_set_ps(1.0, idxd_SMCOMPRESSION, 1.0, idxd_SMCOMPRESSION, 1.0, idxd_SMCOMPRESSION, 1.0, idxd_SMCOMPRESSION);
                      expansion_0 = _mm256_set_ps(1.0, idxd_SMEXPANSION * 0.5, 1.0, idxd_SMEXPANSION * 0.5, 1.0, idxd_SMEXPANSION * 0.5, 1.0, idxd_SMEXPANSION * 0.5);
                      expansion_mask_0 = _mm256_set_ps(0.0, idxd_SMEXPANSION * 0.5, 0.0, idxd_SMEXPANSION * 0.5, 0.0, idxd_SMEXPANSION * 0.5, 0.0, idxd_SMEXPANSION * 0.5);
                    }
                  }else{
                    compression_0 = _mm256_set_ps(idxd_SMCOMPRESSION, 1.0, idxd_SMCOMPRESSION, 1.0, idxd_SMCOMPRESSION, 1.0, idxd_SMCOMPRESSION, 1.0);
                    expansion_0 = _mm256_set_ps(idxd_SMEXPANSION * 0.5, 1.0, idxd_SMEXPANSION * 0.5, 1.0, idxd_SMEXPANSION * 0.5, 1.0, idxd_SMEXPANSION * 0.5, 1.0);
                    expansion_mask_0 = _mm256_set_ps(idxd_SMEXPANSION * 0.5, 0.0, idxd_SMEXPANSION * 0.5, 0.0, idxd_SMEXPANSION * 0.5, 0.0, idxd_SMEXPANSION * 0.5, 0.0);
                  }
                  for(i = 0; i + 4 <= N_block; i += 4, x += (incX * 8)){
                    x_0 = _mm256_div_ps(_mm256_set_ps(((float*)x)[((incX * 6) + 1)], ((float*)x)[(incX * 6)], ((float*)x)[((incX * 4) + 1)], ((float*)x)[(incX * 4)], ((float*)x)[((incX * 2) + 1)], ((float*)x)[(incX * 2)], ((float*)x)[1], ((float*)x)[0]), scale_mask);
                    x_0 = _mm256_mul_ps(x_0, x_0);

                    s_0 = s_buffer[0];
                    q_0 = _mm256_add_ps(s_0, _mm256_or_ps(_mm256_mul_ps(x_0, compression_0), blp_mask_tmp));
                    s_buffer[0] = q_0;
                    q_0 = _mm256_sub_ps(s_0, q_0);
                    x_0 = _mm256_add_ps(_mm256_add_ps(x_0, _mm256_mul_ps(q_0, expansion_0)), _mm256_mul_ps(q_0, expansion_mask_0));
                    for(j = 1; j < fold - 1; j++){
                      s_0 = s_buffer[j];
                      q_0 = _mm256_add_ps(s_0, _mm256_or_ps(x_0, blp_mask_tmp));
                      s_buffer[j] = q_0;
                      q_0 = _mm256_sub_ps(s_0, q_0);
                      x_0 = _mm256_add_ps(x_0, q_0);
                    }
                    s_buffer[j] = _mm256_add_ps(s_buffer[j], _mm256_or_ps(x_0, blp_mask_tmp));
                  }
                  if(i < N_block){
                    x_0 = _mm256_div_ps((__m256)_mm256_set_pd(0, (N_block - i)>2?((double*)((float*)x))[(incX * 2)]:0, (N_block - i)>1?((double*)((float*)x))[incX]:0, ((double*)((float*)x))[0]), scale_mask);
                    x_0 = _mm256_mul_ps(x_0, x_0);

                    s_0 = s_buffer[0];
                    q_0 = _mm256_add_ps(s_0, _mm256_or_ps(_mm256_mul_ps(x_0, compression_0), blp_mask_tmp));
                    s_buffer[0] = q_0;
                    q_0 = _mm256_sub_ps(s_0, q_0);
                    x_0 = _mm256_add_ps(_mm256_add_ps(x_0, _mm256_mul_ps(q_0, expansion_0)), _mm256_mul_ps(q_0, expansion_mask_0));
                    for(j = 1; j < fold - 1; j++){
                      s_0 = s_buffer[j];
                      q_0 = _mm256_add_ps(s_0, _mm256_or_ps(x_0, blp_mask_tmp));
                      s_buffer[j] = q_0;
                      q_0 = _mm256_sub_ps(s_0, q_0);
                      x_0 = _mm256_add_ps(x_0, q_0);
                    }
                    s_buffer[j] = _mm256_add_ps(s_buffer[j], _mm256_or_ps(x_0, blp_mask_tmp));
                    x += (incX * (N_block - i) * 2);
                  }
                }else{
                  for(i = 0; i + 4 <= N_block; i += 4, x += (incX * 8)){
                    x_0 = _mm256_div_ps(_mm256_set_ps(((float*)x)[((incX * 6) + 1)], ((float*)x)[(incX * 6)], ((float*)x)[((incX * 4) + 1)], ((float*)x)[(incX * 4)], ((float*)x)[((incX * 2) + 1)], ((float*)x)[(incX * 2)], ((float*)x)[1], ((float*)x)[0]), scale_mask);
                    x_0 = _mm256_mul_ps(x_0, x_0);

                    for(j = 0; j < fold - 1; j++){
                      s_0 = s_buffer[j];
                      q_0 = _mm256_add_ps(s_0, _mm256_or_ps(x_0, blp_mask_tmp));
                      s_buffer[j] = q_0;
                      q_0 = _mm256_sub_ps(s_0, q_0);
                      x_0 = _mm256_add_ps(x_0, q_0);
                    }
                    s_buffer[j] = _mm256_add_ps(s_buffer[j], _mm256_or_ps(x_0, blp_mask_tmp));
                  }
                  if(i < N_block){
                    x_0 = _mm256_div_ps((__m256)_mm256_set_pd(0, (N_block - i)>2?((double*)((float*)x))[(incX * 2)]:0, (N_block - i)>1?((double*)((float*)x))[incX]:0, ((double*)((float*)x))[0]), scale_mask);
                    x_0 = _mm256_mul_ps(x_0, x_0);

                    for(j = 0; j < fold - 1; j++){
                      s_0 = s_buffer[j];
                      q_0 = _mm256_add_ps(s_0, _mm256_or_ps(x_0, blp_mask_tmp));
                      s_buffer[j] = q_0;
                      q_0 = _mm256_sub_ps(s_0, q_0);
                      x_0 = _mm256_add_ps(x_0, q_0);
                    }
                    s_buffer[j] = _mm256_add_ps(s_buffer[j], _mm256_or_ps(x_0, blp_mask_tmp));
                    x += (incX * (N_block - i) * 2);
                  }
                }
              }

              for(j = 0; j < fold; j += 1){
                s_buffer[j] = _mm256_sub_ps(s_buffer[j], _mm256_set_ps(((float*)ssq)[((j * 2) + 1)], ((float*)ssq)[(j * 2)], ((float*)ssq)[((j * 2) + 1)], ((float*)ssq)[(j * 2)], ((float*)ssq)[((j * 2) + 1)], ((float*)ssq)[(j * 2)], 0, 0));
                _mm256_store_ps(cons_buffer_tmp, s_buffer[j]);
                ((float*)ssq)[(j * 2)] = cons_buffer_tmp[0] + cons_buffer_tmp[2] + cons_buffer_tmp[4] + cons_buffer_tmp[6];
                ((float*)ssq)[((j * 2) + 1)] = cons_buffer_tmp[1] + cons_buffer_tmp[3] + cons_buffer_tmp[5] + cons_buffer_tmp[7];
              }

              if(SIMD_daz_ftz_new_tmp != SIMD_daz_ftz_old_tmp){
                _mm_setcsr(SIMD_daz_ftz_old_tmp);
              }
            }
            break;
        }

      #elif defined(__SSE2__)
        __m128 scale_mask = _mm_set1_ps(scl);
        __m128 blp_mask_tmp;
        {
          __m128 tmp;
          blp_mask_tmp = _mm_set1_ps(1.0);
          tmp = _mm_set1_ps(1.0 + (FLT_EPSILON * 1.0001));
          blp_mask_tmp = _mm_xor_ps(blp_mask_tmp, tmp);
        }
        __m128 cons_tmp; (void)cons_tmp;
        float cons_buffer_tmp[4] __attribute__((aligned(16))); (void)cons_buffer_tmp;
        unsigned int SIMD_daz_ftz_old_tmp = 0;
        unsigned int SIMD_daz_ftz_new_tmp = 0;


        switch(fold){
          case 3:
            {
              int i;
              __m128 x_0;
              __m128 compression_0;
              __m128 expansion_0;
              __m128 expansion_mask_0;
              __m128 q_0;
              __m128 s_0_0;
              __m128 s_1_0;
              __m128 s_2_0;

              s_0_0 = (__m128)_mm_load1_pd((double *)(((float*)ssq)));
              s_1_0 = (__m128)_mm_load1_pd((double *)(((float*)ssq) + 2));
              s_2_0 = (__m128)_mm_load1_pd((double *)(((float*)ssq) + 4));

              if(incX == 1){
                if(idxd_smindex0(ssq) || idxd_smindex0(ssq + 1)){
                  if(idxd_smindex0(ssq)){
                    if(idxd_smindex0(ssq + 1)){
                      compression_0 = _mm_set1_ps(idxd_SMCOMPRESSION);
                      expansion_0 = _mm_set1_ps(idxd_SMEXPANSION * 0.5);
                      expansion_mask_0 = _mm_set1_ps(idxd_SMEXPANSION * 0.5);
                    }else{
                      compression_0 = _mm_set_ps(1.0, idxd_SMCOMPRESSION, 1.0, idxd_SMCOMPRESSION);
                      expansion_0 = _mm_set_ps(1.0, idxd_SMEXPANSION * 0.5, 1.0, idxd_SMEXPANSION * 0.5);
                      expansion_mask_0 = _mm_set_ps(0.0, idxd_SMEXPANSION * 0.5, 0.0, idxd_SMEXPANSION * 0.5);
                    }
                  }else{
                    compression_0 = _mm_set_ps(idxd_SMCOMPRESSION, 1.0, idxd_SMCOMPRESSION, 1.0);
                    expansion_0 = _mm_set_ps(idxd_SMEXPANSION * 0.5, 1.0, idxd_SMEXPANSION * 0.5, 1.0);
                    expansion_mask_0 = _mm_set_ps(idxd_SMEXPANSION * 0.5, 0.0, idxd_SMEXPANSION * 0.5, 0.0);
                  }
                  for(i = 0; i + 2 <= N_block; i += 2, x += 4){
                    x_0 = _mm_div_ps(_mm_loadu_ps(((float*)x)), scale_mask);
                    x_0 = _mm_mul_ps(x_0, x_0);

                    q_0 = s_0_0;
                    s_0_0 = _mm_add_ps(s_0_0, _mm_or_ps(_mm_mul_ps(x_0, compression_0), blp_mask_tmp));
                    q_0 = _mm_sub_ps(q_0, s_0_0);
                    x_0 = _mm_add_ps(_mm_add_ps(x_0, _mm_mul_ps(q_0, expansion_0)), _mm_mul_ps(q_0, expansion_mask_0));
                    q_0 = s_1_0;
                    s_1_0 = _mm_add_ps(s_1_0, _mm_or_ps(x_0, blp_mask_tmp));
                    q_0 = _mm_sub_ps(q_0, s_1_0);
                    x_0 = _mm_add_ps(x_0, q_0);
                    s_2_0 = _mm_add_ps(s_2_0, _mm_or_ps(x_0, blp_mask_tmp));
                  }
                  if(i < N_block){
                    x_0 = _mm_div_ps(_mm_set_ps(0, 0, ((float*)x)[1], ((float*)x)[0]), scale_mask);
                    x_0 = _mm_mul_ps(x_0, x_0);

                    q_0 = s_0_0;
                    s_0_0 = _mm_add_ps(s_0_0, _mm_or_ps(_mm_mul_ps(x_0, compression_0), blp_mask_tmp));
                    q_0 = _mm_sub_ps(q_0, s_0_0);
                    x_0 = _mm_add_ps(_mm_add_ps(x_0, _mm_mul_ps(q_0, expansion_0)), _mm_mul_ps(q_0, expansion_mask_0));
                    q_0 = s_1_0;
                    s_1_0 = _mm_add_ps(s_1_0, _mm_or_ps(x_0, blp_mask_tmp));
                    q_0 = _mm_sub_ps(q_0, s_1_0);
                    x_0 = _mm_add_ps(x_0, q_0);
                    s_2_0 = _mm_add_ps(s_2_0, _mm_or_ps(x_0, blp_mask_tmp));
                    x += ((N_block - i) * 2);
                  }
                }else{
                  for(i = 0; i + 2 <= N_block; i += 2, x += 4){
                    x_0 = _mm_div_ps(_mm_loadu_ps(((float*)x)), scale_mask);
                    x_0 = _mm_mul_ps(x_0, x_0);

                    q_0 = s_0_0;
                    s_0_0 = _mm_add_ps(s_0_0, _mm_or_ps(x_0, blp_mask_tmp));
                    q_0 = _mm_sub_ps(q_0, s_0_0);
                    x_0 = _mm_add_ps(x_0, q_0);
                    q_0 = s_1_0;
                    s_1_0 = _mm_add_ps(s_1_0, _mm_or_ps(x_0, blp_mask_tmp));
                    q_0 = _mm_sub_ps(q_0, s_1_0);
                    x_0 = _mm_add_ps(x_0, q_0);
                    s_2_0 = _mm_add_ps(s_2_0, _mm_or_ps(x_0, blp_mask_tmp));
                  }
                  if(i < N_block){
                    x_0 = _mm_div_ps(_mm_set_ps(0, 0, ((float*)x)[1], ((float*)x)[0]), scale_mask);
                    x_0 = _mm_mul_ps(x_0, x_0);

                    q_0 = s_0_0;
                    s_0_0 = _mm_add_ps(s_0_0, _mm_or_ps(x_0, blp_mask_tmp));
                    q_0 = _mm_sub_ps(q_0, s_0_0);
                    x_0 = _mm_add_ps(x_0, q_0);
                    q_0 = s_1_0;
                    s_1_0 = _mm_add_ps(s_1_0, _mm_or_ps(x_0, blp_mask_tmp));
                    q_0 = _mm_sub_ps(q_0, s_1_0);
                    x_0 = _mm_add_ps(x_0, q_0);
                    s_2_0 = _mm_add_ps(s_2_0, _mm_or_ps(x_0, blp_mask_tmp));
                    x += ((N_block - i) * 2);
                  }
                }
              }else{
                if(idxd_smindex0(ssq) || idxd_smindex0(ssq + 1)){
                  if(idxd_smindex0(ssq)){
                    if(idxd_smindex0(ssq + 1)){
                      compression_0 = _mm_set1_ps(idxd_SMCOMPRESSION);
                      expansion_0 = _mm_set1_ps(idxd_SMEXPANSION * 0.5);
                      expansion_mask_0 = _mm_set1_ps(idxd_SMEXPANSION * 0.5);
                    }else{
                      compression_0 = _mm_set_ps(1.0, idxd_SMCOMPRESSION, 1.0, idxd_SMCOMPRESSION);
                      expansion_0 = _mm_set_ps(1.0, idxd_SMEXPANSION * 0.5, 1.0, idxd_SMEXPANSION * 0.5);
                      expansion_mask_0 = _mm_set_ps(0.0, idxd_SMEXPANSION * 0.5, 0.0, idxd_SMEXPANSION * 0.5);
                    }
                  }else{
                    compression_0 = _mm_set_ps(idxd_SMCOMPRESSION, 1.0, idxd_SMCOMPRESSION, 1.0);
                    expansion_0 = _mm_set_ps(idxd_SMEXPANSION * 0.5, 1.0, idxd_SMEXPANSION * 0.5, 1.0);
                    expansion_mask_0 = _mm_set_ps(idxd_SMEXPANSION * 0.5, 0.0, idxd_SMEXPANSION * 0.5, 0.0);
                  }
                  for(i = 0; i + 2 <= N_block; i += 2, x += (incX * 4)){
                    x_0 = _mm_div_ps(_mm_set_ps(((float*)x)[((incX * 2) + 1)], ((float*)x)[(incX * 2)], ((float*)x)[1], ((float*)x)[0]), scale_mask);
                    x_0 = _mm_mul_ps(x_0, x_0);

                    q_0 = s_0_0;
                    s_0_0 = _mm_add_ps(s_0_0, _mm_or_ps(_mm_mul_ps(x_0, compression_0), blp_mask_tmp));
                    q_0 = _mm_sub_ps(q_0, s_0_0);
                    x_0 = _mm_add_ps(_mm_add_ps(x_0, _mm_mul_ps(q_0, expansion_0)), _mm_mul_ps(q_0, expansion_mask_0));
                    q_0 = s_1_0;
                    s_1_0 = _mm_add_ps(s_1_0, _mm_or_ps(x_0, blp_mask_tmp));
                    q_0 = _mm_sub_ps(q_0, s_1_0);
                    x_0 = _mm_add_ps(x_0, q_0);
                    s_2_0 = _mm_add_ps(s_2_0, _mm_or_ps(x_0, blp_mask_tmp));
                  }
                  if(i < N_block){
                    x_0 = _mm_div_ps(_mm_set_ps(0, 0, ((float*)x)[1], ((float*)x)[0]), scale_mask);
                    x_0 = _mm_mul_ps(x_0, x_0);

                    q_0 = s_0_0;
                    s_0_0 = _mm_add_ps(s_0_0, _mm_or_ps(_mm_mul_ps(x_0, compression_0), blp_mask_tmp));
                    q_0 = _mm_sub_ps(q_0, s_0_0);
                    x_0 = _mm_add_ps(_mm_add_ps(x_0, _mm_mul_ps(q_0, expansion_0)), _mm_mul_ps(q_0, expansion_mask_0));
                    q_0 = s_1_0;
                    s_1_0 = _mm_add_ps(s_1_0, _mm_or_ps(x_0, blp_mask_tmp));
                    q_0 = _mm_sub_ps(q_0, s_1_0);
                    x_0 = _mm_add_ps(x_0, q_0);
                    s_2_0 = _mm_add_ps(s_2_0, _mm_or_ps(x_0, blp_mask_tmp));
                    x += (incX * (N_block - i) * 2);
                  }
                }else{
                  for(i = 0; i + 2 <= N_block; i += 2, x += (incX * 4)){
                    x_0 = _mm_div_ps(_mm_set_ps(((float*)x)[((incX * 2) + 1)], ((float*)x)[(incX * 2)], ((float*)x)[1], ((float*)x)[0]), scale_mask);
                    x_0 = _mm_mul_ps(x_0, x_0);

                    q_0 = s_0_0;
                    s_0_0 = _mm_add_ps(s_0_0, _mm_or_ps(x_0, blp_mask_tmp));
                    q_0 = _mm_sub_ps(q_0, s_0_0);
                    x_0 = _mm_add_ps(x_0, q_0);
                    q_0 = s_1_0;
                    s_1_0 = _mm_add_ps(s_1_0, _mm_or_ps(x_0, blp_mask_tmp));
                    q_0 = _mm_sub_ps(q_0, s_1_0);
                    x_0 = _mm_add_ps(x_0, q_0);
                    s_2_0 = _mm_add_ps(s_2_0, _mm_or_ps(x_0, blp_mask_tmp));
                  }
                  if(i < N_block){
                    x_0 = _mm_div_ps(_mm_set_ps(0, 0, ((float*)x)[1], ((float*)x)[0]), scale_mask);
                    x_0 = _mm_mul_ps(x_0, x_0);

                    q_0 = s_0_0;
                    s_0_0 = _mm_add_ps(s_0_0, _mm_or_ps(x_0, blp_mask_tmp));
                    q_0 = _mm_sub_ps(q_0, s_0_0);
                    x_0 = _mm_add_ps(x_0, q_0);
                    q_0 = s_1_0;
                    s_1_0 = _mm_add_ps(s_1_0, _mm_or_ps(x_0, blp_mask_tmp));
                    q_0 = _mm_sub_ps(q_0, s_1_0);
                    x_0 = _mm_add_ps(x_0, q_0);
                    s_2_0 = _mm_add_ps(s_2_0, _mm_or_ps(x_0, blp_mask_tmp));
                    x += (incX * (N_block - i) * 2);
                  }
                }
              }

              s_0_0 = _mm_sub_ps(s_0_0, _mm_set_ps(((float*)ssq)[1], ((float*)ssq)[0], 0, 0));
              _mm_store_ps(cons_buffer_tmp, s_0_0);
              ((float*)ssq)[0] = cons_buffer_tmp[0] + cons_buffer_tmp[2];
              ((float*)ssq)[1] = cons_buffer_tmp[1] + cons_buffer_tmp[3];
              s_1_0 = _mm_sub_ps(s_1_0, _mm_set_ps(((float*)ssq)[3], ((float*)ssq)[2], 0, 0));
              _mm_store_ps(cons_buffer_tmp, s_1_0);
              ((float*)ssq)[2] = cons_buffer_tmp[0] + cons_buffer_tmp[2];
              ((float*)ssq)[3] = cons_buffer_tmp[1] + cons_buffer_tmp[3];
              s_2_0 = _mm_sub_ps(s_2_0, _mm_set_ps(((float*)ssq)[5], ((float*)ssq)[4], 0, 0));
              _mm_store_ps(cons_buffer_tmp, s_2_0);
              ((float*)ssq)[4] = cons_buffer_tmp[0] + cons_buffer_tmp[2];
              ((float*)ssq)[5] = cons_buffer_tmp[1] + cons_buffer_tmp[3];

              if(SIMD_daz_ftz_new_tmp != SIMD_daz_ftz_old_tmp){
                _mm_setcsr(SIMD_daz_ftz_old_tmp);
              }
            }
            break;
          default:
            {
              int i, j;
              __m128 x_0;
              __m128 compression_0;
              __m128 expansion_0;
              __m128 expansion_mask_0;
              __m128 q_0;
              __m128 s_0;
              __m128 s_buffer[SIMAXFOLD];

              for(j = 0; j < fold; j += 1){
                s_buffer[j] = (__m128)_mm_load1_pd((double *)(((float*)ssq) + (j * 2)));
              }

              if(incX == 1){
                if(idxd_smindex0(ssq) || idxd_smindex0(ssq + 1)){
                  if(idxd_smindex0(ssq)){
                    if(idxd_smindex0(ssq + 1)){
                      compression_0 = _mm_set1_ps(idxd_SMCOMPRESSION);
                      expansion_0 = _mm_set1_ps(idxd_SMEXPANSION * 0.5);
                      expansion_mask_0 = _mm_set1_ps(idxd_SMEXPANSION * 0.5);
                    }else{
                      compression_0 = _mm_set_ps(1.0, idxd_SMCOMPRESSION, 1.0, idxd_SMCOMPRESSION);
                      expansion_0 = _mm_set_ps(1.0, idxd_SMEXPANSION * 0.5, 1.0, idxd_SMEXPANSION * 0.5);
                      expansion_mask_0 = _mm_set_ps(0.0, idxd_SMEXPANSION * 0.5, 0.0, idxd_SMEXPANSION * 0.5);
                    }
                  }else{
                    compression_0 = _mm_set_ps(idxd_SMCOMPRESSION, 1.0, idxd_SMCOMPRESSION, 1.0);
                    expansion_0 = _mm_set_ps(idxd_SMEXPANSION * 0.5, 1.0, idxd_SMEXPANSION * 0.5, 1.0);
                    expansion_mask_0 = _mm_set_ps(idxd_SMEXPANSION * 0.5, 0.0, idxd_SMEXPANSION * 0.5, 0.0);
                  }
                  for(i = 0; i + 2 <= N_block; i += 2, x += 4){
                    x_0 = _mm_div_ps(_mm_loadu_ps(((float*)x)), scale_mask);
                    x_0 = _mm_mul_ps(x_0, x_0);

                    s_0 = s_buffer[0];
                    q_0 = _mm_add_ps(s_0, _mm_or_ps(_mm_mul_ps(x_0, compression_0), blp_mask_tmp));
                    s_buffer[0] = q_0;
                    q_0 = _mm_sub_ps(s_0, q_0);
                    x_0 = _mm_add_ps(_mm_add_ps(x_0, _mm_mul_ps(q_0, expansion_0)), _mm_mul_ps(q_0, expansion_mask_0));
                    for(j = 1; j < fold - 1; j++){
                      s_0 = s_buffer[j];
                      q_0 = _mm_add_ps(s_0, _mm_or_ps(x_0, blp_mask_tmp));
                      s_buffer[j] = q_0;
                      q_0 = _mm_sub_ps(s_0, q_0);
                      x_0 = _mm_add_ps(x_0, q_0);
                    }
                    s_buffer[j] = _mm_add_ps(s_buffer[j], _mm_or_ps(x_0, blp_mask_tmp));
                  }
                  if(i < N_block){
                    x_0 = _mm_div_ps(_mm_set_ps(0, 0, ((float*)x)[1], ((float*)x)[0]), scale_mask);
                    x_0 = _mm_mul_ps(x_0, x_0);

                    s_0 = s_buffer[0];
                    q_0 = _mm_add_ps(s_0, _mm_or_ps(_mm_mul_ps(x_0, compression_0), blp_mask_tmp));
                    s_buffer[0] = q_0;
                    q_0 = _mm_sub_ps(s_0, q_0);
                    x_0 = _mm_add_ps(_mm_add_ps(x_0, _mm_mul_ps(q_0, expansion_0)), _mm_mul_ps(q_0, expansion_mask_0));
                    for(j = 1; j < fold - 1; j++){
                      s_0 = s_buffer[j];
                      q_0 = _mm_add_ps(s_0, _mm_or_ps(x_0, blp_mask_tmp));
                      s_buffer[j] = q_0;
                      q_0 = _mm_sub_ps(s_0, q_0);
                      x_0 = _mm_add_ps(x_0, q_0);
                    }
                    s_buffer[j] = _mm_add_ps(s_buffer[j], _mm_or_ps(x_0, blp_mask_tmp));
                    x += ((N_block - i) * 2);
                  }
                }else{
                  for(i = 0; i + 2 <= N_block; i += 2, x += 4){
                    x_0 = _mm_div_ps(_mm_loadu_ps(((float*)x)), scale_mask);
                    x_0 = _mm_mul_ps(x_0, x_0);

                    for(j = 0; j < fold - 1; j++){
                      s_0 = s_buffer[j];
                      q_0 = _mm_add_ps(s_0, _mm_or_ps(x_0, blp_mask_tmp));
                      s_buffer[j] = q_0;
                      q_0 = _mm_sub_ps(s_0, q_0);
                      x_0 = _mm_add_ps(x_0, q_0);
                    }
                    s_buffer[j] = _mm_add_ps(s_buffer[j], _mm_or_ps(x_0, blp_mask_tmp));
                  }
                  if(i < N_block){
                    x_0 = _mm_div_ps(_mm_set_ps(0, 0, ((float*)x)[1], ((float*)x)[0]), scale_mask);
                    x_0 = _mm_mul_ps(x_0, x_0);

                    for(j = 0; j < fold - 1; j++){
                      s_0 = s_buffer[j];
                      q_0 = _mm_add_ps(s_0, _mm_or_ps(x_0, blp_mask_tmp));
                      s_buffer[j] = q_0;
                      q_0 = _mm_sub_ps(s_0, q_0);
                      x_0 = _mm_add_ps(x_0, q_0);
                    }
                    s_buffer[j] = _mm_add_ps(s_buffer[j], _mm_or_ps(x_0, blp_mask_tmp));
                    x += ((N_block - i) * 2);
                  }
                }
              }else{
                if(idxd_smindex0(ssq) || idxd_smindex0(ssq + 1)){
                  if(idxd_smindex0(ssq)){
                    if(idxd_smindex0(ssq + 1)){
                      compression_0 = _mm_set1_ps(idxd_SMCOMPRESSION);
                      expansion_0 = _mm_set1_ps(idxd_SMEXPANSION * 0.5);
                      expansion_mask_0 = _mm_set1_ps(idxd_SMEXPANSION * 0.5);
                    }else{
                      compression_0 = _mm_set_ps(1.0, idxd_SMCOMPRESSION, 1.0, idxd_SMCOMPRESSION);
                      expansion_0 = _mm_set_ps(1.0, idxd_SMEXPANSION * 0.5, 1.0, idxd_SMEXPANSION * 0.5);
                      expansion_mask_0 = _mm_set_ps(0.0, idxd_SMEXPANSION * 0.5, 0.0, idxd_SMEXPANSION * 0.5);
                    }
                  }else{
                    compression_0 = _mm_set_ps(idxd_SMCOMPRESSION, 1.0, idxd_SMCOMPRESSION, 1.0);
                    expansion_0 = _mm_set_ps(idxd_SMEXPANSION * 0.5, 1.0, idxd_SMEXPANSION * 0.5, 1.0);
                    expansion_mask_0 = _mm_set_ps(idxd_SMEXPANSION * 0.5, 0.0, idxd_SMEXPANSION * 0.5, 0.0);
                  }
                  for(i = 0; i + 2 <= N_block; i += 2, x += (incX * 4)){
                    x_0 = _mm_div_ps(_mm_set_ps(((float*)x)[((incX * 2) + 1)], ((float*)x)[(incX * 2)], ((float*)x)[1], ((float*)x)[0]), scale_mask);
                    x_0 = _mm_mul_ps(x_0, x_0);

                    s_0 = s_buffer[0];
                    q_0 = _mm_add_ps(s_0, _mm_or_ps(_mm_mul_ps(x_0, compression_0), blp_mask_tmp));
                    s_buffer[0] = q_0;
                    q_0 = _mm_sub_ps(s_0, q_0);
                    x_0 = _mm_add_ps(_mm_add_ps(x_0, _mm_mul_ps(q_0, expansion_0)), _mm_mul_ps(q_0, expansion_mask_0));
                    for(j = 1; j < fold - 1; j++){
                      s_0 = s_buffer[j];
                      q_0 = _mm_add_ps(s_0, _mm_or_ps(x_0, blp_mask_tmp));
                      s_buffer[j] = q_0;
                      q_0 = _mm_sub_ps(s_0, q_0);
                      x_0 = _mm_add_ps(x_0, q_0);
                    }
                    s_buffer[j] = _mm_add_ps(s_buffer[j], _mm_or_ps(x_0, blp_mask_tmp));
                  }
                  if(i < N_block){
                    x_0 = _mm_div_ps(_mm_set_ps(0, 0, ((float*)x)[1], ((float*)x)[0]), scale_mask);
                    x_0 = _mm_mul_ps(x_0, x_0);

                    s_0 = s_buffer[0];
                    q_0 = _mm_add_ps(s_0, _mm_or_ps(_mm_mul_ps(x_0, compression_0), blp_mask_tmp));
                    s_buffer[0] = q_0;
                    q_0 = _mm_sub_ps(s_0, q_0);
                    x_0 = _mm_add_ps(_mm_add_ps(x_0, _mm_mul_ps(q_0, expansion_0)), _mm_mul_ps(q_0, expansion_mask_0));
                    for(j = 1; j < fold - 1; j++){
                      s_0 = s_buffer[j];
                      q_0 = _mm_add_ps(s_0, _mm_or_ps(x_0, blp_mask_tmp));
                      s_buffer[j] = q_0;
                      q_0 = _mm_sub_ps(s_0, q_0);
                      x_0 = _mm_add_ps(x_0, q_0);
                    }
                    s_buffer[j] = _mm_add_ps(s_buffer[j], _mm_or_ps(x_0, blp_mask_tmp));
                    x += (incX * (N_block - i) * 2);
                  }
                }else{
                  for(i = 0; i + 2 <= N_block; i += 2, x += (incX * 4)){
                    x_0 = _mm_div_ps(_mm_set_ps(((float*)x)[((incX * 2) + 1)], ((float*)x)[(incX * 2)], ((float*)x)[1], ((float*)x)[0]), scale_mask);
                    x_0 = _mm_mul_ps(x_0, x_0);

                    for(j = 0; j < fold - 1; j++){
                      s_0 = s_buffer[j];
                      q_0 = _mm_add_ps(s_0, _mm_or_ps(x_0, blp_mask_tmp));
                      s_buffer[j] = q_0;
                      q_0 = _mm_sub_ps(s_0, q_0);
                      x_0 = _mm_add_ps(x_0, q_0);
                    }
                    s_buffer[j] = _mm_add_ps(s_buffer[j], _mm_or_ps(x_0, blp_mask_tmp));
                  }
                  if(i < N_block){
                    x_0 = _mm_div_ps(_mm_set_ps(0, 0, ((float*)x)[1], ((float*)x)[0]), scale_mask);
                    x_0 = _mm_mul_ps(x_0, x_0);

                    for(j = 0; j < fold - 1; j++){
                      s_0 = s_buffer[j];
                      q_0 = _mm_add_ps(s_0, _mm_or_ps(x_0, blp_mask_tmp));
                      s_buffer[j] = q_0;
                      q_0 = _mm_sub_ps(s_0, q_0);
                      x_0 = _mm_add_ps(x_0, q_0);
                    }
                    s_buffer[j] = _mm_add_ps(s_buffer[j], _mm_or_ps(x_0, blp_mask_tmp));
                    x += (incX * (N_block - i) * 2);
                  }
                }
              }

              for(j = 0; j < fold; j += 1){
                s_buffer[j] = _mm_sub_ps(s_buffer[j], _mm_set_ps(((float*)ssq)[((j * 2) + 1)], ((float*)ssq)[(j * 2)], 0, 0));
                _mm_store_ps(cons_buffer_tmp, s_buffer[j]);
                ((float*)ssq)[(j * 2)] = cons_buffer_tmp[0] + cons_buffer_tmp[2];
                ((float*)ssq)[((j * 2) + 1)] = cons_buffer_tmp[1] + cons_buffer_tmp[3];
              }

              if(SIMD_daz_ftz_new_tmp != SIMD_daz_ftz_old_tmp){
                _mm_setcsr(SIMD_daz_ftz_old_tmp);
              }
            }
            break;
        }

      #else
        float scale_mask = scl;
        int_float blp_tmp; (void)blp_tmp;
        float cons_tmp; (void)cons_tmp;


        switch(fold){
          case 3:
            {
              int i;
              float x_0, x_1;
              float compression_0, compression_1;
              float expansion_0, expansion_1;
              float expansion_mask_0, expansion_mask_1;
              float q_0, q_1;
              float s_0_0, s_0_1;
              float s_1_0, s_1_1;
              float s_2_0, s_2_1;

              s_0_0 = ((float*)ssq)[0];
              s_0_1 = ((float*)ssq)[1];
              s_1_0 = ((float*)ssq)[2];
              s_1_1 = ((float*)ssq)[3];
              s_2_0 = ((float*)ssq)[4];
              s_2_1 = ((float*)ssq)[5];

              if(incX == 1){
                if(idxd_smindex0(ssq) || idxd_smindex0(ssq + 1)){
                  if(idxd_smindex0(ssq)){
                    if(idxd_smindex0(ssq + 1)){
                      compression_0 = idxd_SMCOMPRESSION;
                      expansion_0 = idxd_SMEXPANSION * 0.5;
                      expansion_mask_0 = idxd_SMEXPANSION * 0.5;
                    }else{
                      compression_0 = idxd_SMCOMPRESSION;
                      compression_1 = 1.0;
                      expansion_0 = idxd_SMEXPANSION * 0.5;
                      expansion_1 = 1.0;
                      expansion_mask_0 = idxd_SMEXPANSION * 0.5;
                      expansion_mask_1 = 0.0;
                    }
                  }else{
                    compression_0 = 1.0;
                    compression_1 = idxd_SMCOMPRESSION;
                    expansion_0 = 1.0;
                    expansion_1 = idxd_SMEXPANSION * 0.5;
                    expansion_mask_0 = 0.0;
                    expansion_mask_1 = idxd_SMEXPANSION * 0.5;
                  }
                  for(i = 0; i + 1 <= N_block; i += 1, x += 2){
                    x_0 = ((float*)x)[0] / scale_mask;
                    x_1 = ((float*)x)[1] / scale_mask;
                    x_0 = x_0 * x_0;
                    x_1 = x_1 * x_1;

                    q_0 = s_0_0;
                    q_1 = s_0_1;
                    blp_tmp.f = x_0 * compression_0;
                    blp_tmp.i |= 1;
                    s_0_0 = s_0_0 + blp_tmp.f;
                    blp_tmp.f = x_1 * compression_1;
                    blp_tmp.i |= 1;
                    s_0_1 = s_0_1 + blp_tmp.f;
                    q_0 = q_0 - s_0_0;
                    q_1 = q_1 - s_0_1;
                    x_0 = x_0 + q_0 * expansion_0 + q_0 * expansion_mask_0;
                    x_1 = x_1 + q_1 * expansion_1 + q_1 * expansion_mask_1;
                    q_0 = s_1_0;
                    q_1 = s_1_1;
                    blp_tmp.f = x_0;
                    blp_tmp.i |= 1;
                    s_1_0 = s_1_0 + blp_tmp.f;
                    blp_tmp.f = x_1;
                    blp_tmp.i |= 1;
                    s_1_1 = s_1_1 + blp_tmp.f;
                    q_0 = q_0 - s_1_0;
                    q_1 = q_1 - s_1_1;
                    x_0 = x_0 + q_0;
                    x_1 = x_1 + q_1;
                    blp_tmp.f = x_0;
                    blp_tmp.i |= 1;
                    s_2_0 = s_2_0 + blp_tmp.f;
                    blp_tmp.f = x_1;
                    blp_tmp.i |= 1;
                    s_2_1 = s_2_1 + blp_tmp.f;
                  }
                }else{
                  for(i = 0; i + 1 <= N_block; i += 1, x += 2){
                    x_0 = ((float*)x)[0] / scale_mask;
                    x_1 = ((float*)x)[1] / scale_mask;
                    x_0 = x_0 * x_0;
                    x_1 = x_1 * x_1;

                    q_0 = s_0_0;
                    q_1 = s_0_1;
                    blp_tmp.f = x_0;
                    blp_tmp.i |= 1;
                    s_0_0 = s_0_0 + blp_tmp.f;
                    blp_tmp.f = x_1;
                    blp_tmp.i |= 1;
                    s_0_1 = s_0_1 + blp_tmp.f;
                    q_0 = q_0 - s_0_0;
                    q_1 = q_1 - s_0_1;
                    x_0 = x_0 + q_0;
                    x_1 = x_1 + q_1;
                    q_0 = s_1_0;
                    q_1 = s_1_1;
                    blp_tmp.f = x_0;
                    blp_tmp.i |= 1;
                    s_1_0 = s_1_0 + blp_tmp.f;
                    blp_tmp.f = x_1;
                    blp_tmp.i |= 1;
                    s_1_1 = s_1_1 + blp_tmp.f;
                    q_0 = q_0 - s_1_0;
                    q_1 = q_1 - s_1_1;
                    x_0 = x_0 + q_0;
                    x_1 = x_1 + q_1;
                    blp_tmp.f = x_0;
                    blp_tmp.i |= 1;
                    s_2_0 = s_2_0 + blp_tmp.f;
                    blp_tmp.f = x_1;
                    blp_tmp.i |= 1;
                    s_2_1 = s_2_1 + blp_tmp.f;
                  }
                }
              }else{
                if(idxd_smindex0(ssq) || idxd_smindex0(ssq + 1)){
                  if(idxd_smindex0(ssq)){
                    if(idxd_smindex0(ssq + 1)){
                      compression_0 = idxd_SMCOMPRESSION;
                      expansion_0 = idxd_SMEXPANSION * 0.5;
                      expansion_mask_0 = idxd_SMEXPANSION * 0.5;
                    }else{
                      compression_0 = idxd_SMCOMPRESSION;
                      compression_1 = 1.0;
                      expansion_0 = idxd_SMEXPANSION * 0.5;
                      expansion_1 = 1.0;
                      expansion_mask_0 = idxd_SMEXPANSION * 0.5;
                      expansion_mask_1 = 0.0;
                    }
                  }else{
                    compression_0 = 1.0;
                    compression_1 = idxd_SMCOMPRESSION;
                    expansion_0 = 1.0;
                    expansion_1 = idxd_SMEXPANSION * 0.5;
                    expansion_mask_0 = 0.0;
                    expansion_mask_1 = idxd_SMEXPANSION * 0.5;
                  }
                  for(i = 0; i + 1 <= N_block; i += 1, x += (incX * 2)){
                    x_0 = ((float*)x)[0] / scale_mask;
                    x_1 = ((float*)x)[1] / scale_mask;
                    x_0 = x_0 * x_0;
                    x_1 = x_1 * x_1;

                    q_0 = s_0_0;
                    q_1 = s_0_1;
                    blp_tmp.f = x_0 * compression_0;
                    blp_tmp.i |= 1;
                    s_0_0 = s_0_0 + blp_tmp.f;
                    blp_tmp.f = x_1 * compression_1;
                    blp_tmp.i |= 1;
                    s_0_1 = s_0_1 + blp_tmp.f;
                    q_0 = q_0 - s_0_0;
                    q_1 = q_1 - s_0_1;
                    x_0 = x_0 + q_0 * expansion_0 + q_0 * expansion_mask_0;
                    x_1 = x_1 + q_1 * expansion_1 + q_1 * expansion_mask_1;
                    q_0 = s_1_0;
                    q_1 = s_1_1;
                    blp_tmp.f = x_0;
                    blp_tmp.i |= 1;
                    s_1_0 = s_1_0 + blp_tmp.f;
                    blp_tmp.f = x_1;
                    blp_tmp.i |= 1;
                    s_1_1 = s_1_1 + blp_tmp.f;
                    q_0 = q_0 - s_1_0;
                    q_1 = q_1 - s_1_1;
                    x_0 = x_0 + q_0;
                    x_1 = x_1 + q_1;
                    blp_tmp.f = x_0;
                    blp_tmp.i |= 1;
                    s_2_0 = s_2_0 + blp_tmp.f;
                    blp_tmp.f = x_1;
                    blp_tmp.i |= 1;
                    s_2_1 = s_2_1 + blp_tmp.f;
                  }
                }else{
                  for(i = 0; i + 1 <= N_block; i += 1, x += (incX * 2)){
                    x_0 = ((float*)x)[0] / scale_mask;
                    x_1 = ((float*)x)[1] / scale_mask;
                    x_0 = x_0 * x_0;
                    x_1 = x_1 * x_1;

                    q_0 = s_0_0;
                    q_1 = s_0_1;
                    blp_tmp.f = x_0;
                    blp_tmp.i |= 1;
                    s_0_0 = s_0_0 + blp_tmp.f;
                    blp_tmp.f = x_1;
                    blp_tmp.i |= 1;
                    s_0_1 = s_0_1 + blp_tmp.f;
                    q_0 = q_0 - s_0_0;
                    q_1 = q_1 - s_0_1;
                    x_0 = x_0 + q_0;
                    x_1 = x_1 + q_1;
                    q_0 = s_1_0;
                    q_1 = s_1_1;
                    blp_tmp.f = x_0;
                    blp_tmp.i |= 1;
                    s_1_0 = s_1_0 + blp_tmp.f;
                    blp_tmp.f = x_1;
                    blp_tmp.i |= 1;
                    s_1_1 = s_1_1 + blp_tmp.f;
                    q_0 = q_0 - s_1_0;
                    q_1 = q_1 - s_1_1;
                    x_0 = x_0 + q_0;
                    x_1 = x_1 + q_1;
                    blp_tmp.f = x_0;
                    blp_tmp.i |= 1;
                    s_2_0 = s_2_0 + blp_tmp.f;
                    blp_tmp.f = x_1;
                    blp_tmp.i |= 1;
                    s_2_1 = s_2_1 + blp_tmp.f;
                  }
                }
              }

              ((float*)ssq)[0] = s_0_0;
              ((float*)ssq)[1] = s_0_1;
              ((float*)ssq)[2] = s_1_0;
              ((float*)ssq)[3] = s_1_1;
              ((float*)ssq)[4] = s_2_0;
              ((float*)ssq)[5] = s_2_1;

            }
            break;
          default:
            {
              int i, j;
              float x_0, x_1;
              float compression_0, compression_1;
              float expansion_0, expansion_1;
              float expansion_mask_0, expansion_mask_1;
              float q_0, q_1;
              float s_0, s_1;
              float s_buffer[(SIMAXFOLD * 2)];

              for(j = 0; j < fold; j += 1){
                s_buffer[(j * 2)] = ((float*)ssq)[(j * 2)];
                s_buffer[((j * 2) + 1)] = ((float*)ssq)[((j * 2) + 1)];
              }

              if(incX == 1){
                if(idxd_smindex0(ssq) || idxd_smindex0(ssq + 1)){
                  if(idxd_smindex0(ssq)){
                    if(idxd_smindex0(ssq + 1)){
                      compression_0 = idxd_SMCOMPRESSION;
                      expansion_0 = idxd_SMEXPANSION * 0.5;
                      expansion_mask_0 = idxd_SMEXPANSION * 0.5;
                    }else{
                      compression_0 = idxd_SMCOMPRESSION;
                      compression_1 = 1.0;
                      expansion_0 = idxd_SMEXPANSION * 0.5;
                      expansion_1 = 1.0;
                      expansion_mask_0 = idxd_SMEXPANSION * 0.5;
                      expansion_mask_1 = 0.0;
                    }
                  }else{
                    compression_0 = 1.0;
                    compression_1 = idxd_SMCOMPRESSION;
                    expansion_0 = 1.0;
                    expansion_1 = idxd_SMEXPANSION * 0.5;
                    expansion_mask_0 = 0.0;
                    expansion_mask_1 = idxd_SMEXPANSION * 0.5;
                  }
                  for(i = 0; i + 1 <= N_block; i += 1, x += 2){
                    x_0 = ((float*)x)[0] / scale_mask;
                    x_1 = ((float*)x)[1] / scale_mask;
                    x_0 = x_0 * x_0;
                    x_1 = x_1 * x_1;

                    s_0 = s_buffer[0];
                    s_1 = s_buffer[1];
                    blp_tmp.f = x_0 * compression_0;
                    blp_tmp.i |= 1;
                    q_0 = s_0 + blp_tmp.f;
                    blp_tmp.f = x_1 * compression_1;
                    blp_tmp.i |= 1;
                    q_1 = s_1 + blp_tmp.f;
                    s_buffer[0] = q_0;
                    s_buffer[1] = q_1;
                    q_0 = s_0 - q_0;
                    q_1 = s_1 - q_1;
                    x_0 = x_0 + q_0 * expansion_0 + q_0 * expansion_mask_0;
                    x_1 = x_1 + q_1 * expansion_1 + q_1 * expansion_mask_1;
                    for(j = 1; j < fold - 1; j++){
                      s_0 = s_buffer[(j * 2)];
                      s_1 = s_buffer[((j * 2) + 1)];
                      blp_tmp.f = x_0;
                      blp_tmp.i |= 1;
                      q_0 = s_0 + blp_tmp.f;
                      blp_tmp.f = x_1;
                      blp_tmp.i |= 1;
                      q_1 = s_1 + blp_tmp.f;
                      s_buffer[(j * 2)] = q_0;
                      s_buffer[((j * 2) + 1)] = q_1;
                      q_0 = s_0 - q_0;
                      q_1 = s_1 - q_1;
                      x_0 = x_0 + q_0;
                      x_1 = x_1 + q_1;
                    }
                    blp_tmp.f = x_0;
                    blp_tmp.i |= 1;
                    s_buffer[(j * 2)] = s_buffer[(j * 2)] + blp_tmp.f;
                    blp_tmp.f = x_1;
                    blp_tmp.i |= 1;
                    s_buffer[((j * 2) + 1)] = s_buffer[((j * 2) + 1)] + blp_tmp.f;
                  }
                }else{
                  for(i = 0; i + 1 <= N_block; i += 1, x += 2){
                    x_0 = ((float*)x)[0] / scale_mask;
                    x_1 = ((float*)x)[1] / scale_mask;
                    x_0 = x_0 * x_0;
                    x_1 = x_1 * x_1;

                    for(j = 0; j < fold - 1; j++){
                      s_0 = s_buffer[(j * 2)];
                      s_1 = s_buffer[((j * 2) + 1)];
                      blp_tmp.f = x_0;
                      blp_tmp.i |= 1;
                      q_0 = s_0 + blp_tmp.f;
                      blp_tmp.f = x_1;
                      blp_tmp.i |= 1;
                      q_1 = s_1 + blp_tmp.f;
                      s_buffer[(j * 2)] = q_0;
                      s_buffer[((j * 2) + 1)] = q_1;
                      q_0 = s_0 - q_0;
                      q_1 = s_1 - q_1;
                      x_0 = x_0 + q_0;
                      x_1 = x_1 + q_1;
                    }
                    blp_tmp.f = x_0;
                    blp_tmp.i |= 1;
                    s_buffer[(j * 2)] = s_buffer[(j * 2)] + blp_tmp.f;
                    blp_tmp.f = x_1;
                    blp_tmp.i |= 1;
                    s_buffer[((j * 2) + 1)] = s_buffer[((j * 2) + 1)] + blp_tmp.f;
                  }
                }
              }else{
                if(idxd_smindex0(ssq) || idxd_smindex0(ssq + 1)){
                  if(idxd_smindex0(ssq)){
                    if(idxd_smindex0(ssq + 1)){
                      compression_0 = idxd_SMCOMPRESSION;
                      expansion_0 = idxd_SMEXPANSION * 0.5;
                      expansion_mask_0 = idxd_SMEXPANSION * 0.5;
                    }else{
                      compression_0 = idxd_SMCOMPRESSION;
                      compression_1 = 1.0;
                      expansion_0 = idxd_SMEXPANSION * 0.5;
                      expansion_1 = 1.0;
                      expansion_mask_0 = idxd_SMEXPANSION * 0.5;
                      expansion_mask_1 = 0.0;
                    }
                  }else{
                    compression_0 = 1.0;
                    compression_1 = idxd_SMCOMPRESSION;
                    expansion_0 = 1.0;
                    expansion_1 = idxd_SMEXPANSION * 0.5;
                    expansion_mask_0 = 0.0;
                    expansion_mask_1 = idxd_SMEXPANSION * 0.5;
                  }
                  for(i = 0; i + 1 <= N_block; i += 1, x += (incX * 2)){
                    x_0 = ((float*)x)[0] / scale_mask;
                    x_1 = ((float*)x)[1] / scale_mask;
                    x_0 = x_0 * x_0;
                    x_1 = x_1 * x_1;

                    s_0 = s_buffer[0];
                    s_1 = s_buffer[1];
                    blp_tmp.f = x_0 * compression_0;
                    blp_tmp.i |= 1;
                    q_0 = s_0 + blp_tmp.f;
                    blp_tmp.f = x_1 * compression_1;
                    blp_tmp.i |= 1;
                    q_1 = s_1 + blp_tmp.f;
                    s_buffer[0] = q_0;
                    s_buffer[1] = q_1;
                    q_0 = s_0 - q_0;
                    q_1 = s_1 - q_1;
                    x_0 = x_0 + q_0 * expansion_0 + q_0 * expansion_mask_0;
                    x_1 = x_1 + q_1 * expansion_1 + q_1 * expansion_mask_1;
                    for(j = 1; j < fold - 1; j++){
                      s_0 = s_buffer[(j * 2)];
                      s_1 = s_buffer[((j * 2) + 1)];
                      blp_tmp.f = x_0;
                      blp_tmp.i |= 1;
                      q_0 = s_0 + blp_tmp.f;
                      blp_tmp.f = x_1;
                      blp_tmp.i |= 1;
                      q_1 = s_1 + blp_tmp.f;
                      s_buffer[(j * 2)] = q_0;
                      s_buffer[((j * 2) + 1)] = q_1;
                      q_0 = s_0 - q_0;
                      q_1 = s_1 - q_1;
                      x_0 = x_0 + q_0;
                      x_1 = x_1 + q_1;
                    }
                    blp_tmp.f = x_0;
                    blp_tmp.i |= 1;
                    s_buffer[(j * 2)] = s_buffer[(j * 2)] + blp_tmp.f;
                    blp_tmp.f = x_1;
                    blp_tmp.i |= 1;
                    s_buffer[((j * 2) + 1)] = s_buffer[((j * 2) + 1)] + blp_tmp.f;
                  }
                }else{
                  for(i = 0; i + 1 <= N_block; i += 1, x += (incX * 2)){
                    x_0 = ((float*)x)[0] / scale_mask;
                    x_1 = ((float*)x)[1] / scale_mask;
                    x_0 = x_0 * x_0;
                    x_1 = x_1 * x_1;

                    for(j = 0; j < fold - 1; j++){
                      s_0 = s_buffer[(j * 2)];
                      s_1 = s_buffer[((j * 2) + 1)];
                      blp_tmp.f = x_0;
                      blp_tmp.i |= 1;
                      q_0 = s_0 + blp_tmp.f;
                      blp_tmp.f = x_1;
                      blp_tmp.i |= 1;
                      q_1 = s_1 + blp_tmp.f;
                      s_buffer[(j * 2)] = q_0;
                      s_buffer[((j * 2) + 1)] = q_1;
                      q_0 = s_0 - q_0;
                      q_1 = s_1 - q_1;
                      x_0 = x_0 + q_0;
                      x_1 = x_1 + q_1;
                    }
                    blp_tmp.f = x_0;
                    blp_tmp.i |= 1;
                    s_buffer[(j * 2)] = s_buffer[(j * 2)] + blp_tmp.f;
                    blp_tmp.f = x_1;
                    blp_tmp.i |= 1;
                    s_buffer[((j * 2) + 1)] = s_buffer[((j * 2) + 1)] + blp_tmp.f;
                  }
                }
              }

              for(j = 0; j < fold; j += 1){
                ((float*)ssq)[(j * 2)] = s_buffer[(j * 2)];
                ((float*)ssq)[((j * 2) + 1)] = s_buffer[((j * 2) + 1)];
              }

            }
            break;
        }

      #endif

        }
    //[[[end]]]

    deposits += N_block;
  }

  idxd_cirenorm(fold, ssq);
  new_scl = idxd_smsmaddsq(fold, scl, ssq, 2, ssq + 2 * fold, 2, scaleY, manY, incmanY, carY, inccarY);
  scl = idxd_smsmaddsq(fold, scl, ssq + 1, 2, ssq + 2 * fold + 1, 2, new_scl, manY, incmanY, carY, inccarY);

  free(ssq);

  if (ISNANINFF(manY[0])){
    return idxd_sscale(1.0);
  } else {
    return scl;
  }
}
