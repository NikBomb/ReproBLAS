#include <complex.h>
#include <stdio.h>
#include <stdlib.h>
#include <float.h>
#include <math.h>
#include "../config.h"
#include "../Common/Common.h"
#include <immintrin.h>
#include <emmintrin.h>

/*[[[cog
import cog
import generate
import dataTypes
import nrm2I2
]]]*/
//[[[end]]]

void scnrm2I2(int n, float complex* v, int incv, float scale, int fold, float complex* sum){
  /*[[[cog
  cog.out(generate.generate(nrm2I2.Nrm2I2(dataTypes.FloatComplex), cog.inFile, args, params, mode))
  ]]]*/
  #ifdef __AVX__
    __m256 scale_mask = _mm256_set1_ps(scale);
    __m256 mask_BLP; AVX_BLP_MASKS(mask_BLP);
    float complex tmp_cons[4] __attribute__((aligned(32)));
    SET_DAZ_FLAG;
    switch(fold){
      case 3:{
        int i;

        float* sum_base = (float*) sum;
        float* v_base = (float*) v;
        __m256 v_0, v_1, v_2, v_3, v_4, v_5, v_6, v_7, v_8, v_9, v_10, v_11, v_12, v_13, v_14, v_15;
        __m256 q_0, q_1, q_2, q_3;
        __m256 s_0_0, s_0_1, s_0_2, s_0_3;
        __m256 s_1_0, s_1_1, s_1_2, s_1_3;
        __m256 s_2_0, s_2_1, s_2_2, s_2_3;

        s_0_0 = s_0_1 = s_0_2 = s_0_3 = (__m256)_mm256_broadcast_sd((double *)(sum_base));
        s_1_0 = s_1_1 = s_1_2 = s_1_3 = (__m256)_mm256_broadcast_sd((double *)(sum_base + 2));
        s_2_0 = s_2_1 = s_2_2 = s_2_3 = (__m256)_mm256_broadcast_sd((double *)(sum_base + 4));
        if(incv == 1){

          for(i = 0; i + 64 <= n; i += 64, v_base += 128){
            v_0 = _mm256_mul_ps(_mm256_loadu_ps(v_base), scale_mask);
            v_1 = _mm256_mul_ps(_mm256_loadu_ps(v_base + 8), scale_mask);
            v_2 = _mm256_mul_ps(_mm256_loadu_ps(v_base + 16), scale_mask);
            v_3 = _mm256_mul_ps(_mm256_loadu_ps(v_base + 24), scale_mask);
            v_4 = _mm256_mul_ps(_mm256_loadu_ps(v_base + 32), scale_mask);
            v_5 = _mm256_mul_ps(_mm256_loadu_ps(v_base + 40), scale_mask);
            v_6 = _mm256_mul_ps(_mm256_loadu_ps(v_base + 48), scale_mask);
            v_7 = _mm256_mul_ps(_mm256_loadu_ps(v_base + 56), scale_mask);
            v_8 = _mm256_mul_ps(_mm256_loadu_ps(v_base + 64), scale_mask);
            v_9 = _mm256_mul_ps(_mm256_loadu_ps(v_base + 72), scale_mask);
            v_10 = _mm256_mul_ps(_mm256_loadu_ps(v_base + 80), scale_mask);
            v_11 = _mm256_mul_ps(_mm256_loadu_ps(v_base + 88), scale_mask);
            v_12 = _mm256_mul_ps(_mm256_loadu_ps(v_base + 96), scale_mask);
            v_13 = _mm256_mul_ps(_mm256_loadu_ps(v_base + 104), scale_mask);
            v_14 = _mm256_mul_ps(_mm256_loadu_ps(v_base + 112), scale_mask);
            v_15 = _mm256_mul_ps(_mm256_loadu_ps(v_base + 120), scale_mask);
            v_0 = _mm256_mul_ps(v_0, v_0);
            v_1 = _mm256_mul_ps(v_1, v_1);
            v_2 = _mm256_mul_ps(v_2, v_2);
            v_3 = _mm256_mul_ps(v_3, v_3);
            v_4 = _mm256_mul_ps(v_4, v_4);
            v_5 = _mm256_mul_ps(v_5, v_5);
            v_6 = _mm256_mul_ps(v_6, v_6);
            v_7 = _mm256_mul_ps(v_7, v_7);
            v_8 = _mm256_mul_ps(v_8, v_8);
            v_9 = _mm256_mul_ps(v_9, v_9);
            v_10 = _mm256_mul_ps(v_10, v_10);
            v_11 = _mm256_mul_ps(v_11, v_11);
            v_12 = _mm256_mul_ps(v_12, v_12);
            v_13 = _mm256_mul_ps(v_13, v_13);
            v_14 = _mm256_mul_ps(v_14, v_14);
            v_15 = _mm256_mul_ps(v_15, v_15);
            q_0 = s_0_0;
            q_1 = s_0_1;
            q_2 = s_0_2;
            q_3 = s_0_3;
            s_0_0 = _mm256_add_ps(s_0_0, _mm256_or_ps(v_0, mask_BLP));
            s_0_1 = _mm256_add_ps(s_0_1, _mm256_or_ps(v_1, mask_BLP));
            s_0_2 = _mm256_add_ps(s_0_2, _mm256_or_ps(v_2, mask_BLP));
            s_0_3 = _mm256_add_ps(s_0_3, _mm256_or_ps(v_3, mask_BLP));
            q_0 = _mm256_sub_ps(q_0, s_0_0);
            q_1 = _mm256_sub_ps(q_1, s_0_1);
            q_2 = _mm256_sub_ps(q_2, s_0_2);
            q_3 = _mm256_sub_ps(q_3, s_0_3);
            v_0 = _mm256_add_ps(v_0, q_0);
            v_1 = _mm256_add_ps(v_1, q_1);
            v_2 = _mm256_add_ps(v_2, q_2);
            v_3 = _mm256_add_ps(v_3, q_3);
            q_0 = s_1_0;
            q_1 = s_1_1;
            q_2 = s_1_2;
            q_3 = s_1_3;
            s_1_0 = _mm256_add_ps(s_1_0, _mm256_or_ps(v_0, mask_BLP));
            s_1_1 = _mm256_add_ps(s_1_1, _mm256_or_ps(v_1, mask_BLP));
            s_1_2 = _mm256_add_ps(s_1_2, _mm256_or_ps(v_2, mask_BLP));
            s_1_3 = _mm256_add_ps(s_1_3, _mm256_or_ps(v_3, mask_BLP));
            q_0 = _mm256_sub_ps(q_0, s_1_0);
            q_1 = _mm256_sub_ps(q_1, s_1_1);
            q_2 = _mm256_sub_ps(q_2, s_1_2);
            q_3 = _mm256_sub_ps(q_3, s_1_3);
            v_0 = _mm256_add_ps(v_0, q_0);
            v_1 = _mm256_add_ps(v_1, q_1);
            v_2 = _mm256_add_ps(v_2, q_2);
            v_3 = _mm256_add_ps(v_3, q_3);
            s_2_0 = _mm256_add_ps(s_2_0, _mm256_or_ps(v_0, mask_BLP));
            s_2_1 = _mm256_add_ps(s_2_1, _mm256_or_ps(v_1, mask_BLP));
            s_2_2 = _mm256_add_ps(s_2_2, _mm256_or_ps(v_2, mask_BLP));
            s_2_3 = _mm256_add_ps(s_2_3, _mm256_or_ps(v_3, mask_BLP));
            q_0 = s_0_0;
            q_1 = s_0_1;
            q_2 = s_0_2;
            q_3 = s_0_3;
            s_0_0 = _mm256_add_ps(s_0_0, _mm256_or_ps(v_4, mask_BLP));
            s_0_1 = _mm256_add_ps(s_0_1, _mm256_or_ps(v_5, mask_BLP));
            s_0_2 = _mm256_add_ps(s_0_2, _mm256_or_ps(v_6, mask_BLP));
            s_0_3 = _mm256_add_ps(s_0_3, _mm256_or_ps(v_7, mask_BLP));
            q_0 = _mm256_sub_ps(q_0, s_0_0);
            q_1 = _mm256_sub_ps(q_1, s_0_1);
            q_2 = _mm256_sub_ps(q_2, s_0_2);
            q_3 = _mm256_sub_ps(q_3, s_0_3);
            v_4 = _mm256_add_ps(v_4, q_0);
            v_5 = _mm256_add_ps(v_5, q_1);
            v_6 = _mm256_add_ps(v_6, q_2);
            v_7 = _mm256_add_ps(v_7, q_3);
            q_0 = s_1_0;
            q_1 = s_1_1;
            q_2 = s_1_2;
            q_3 = s_1_3;
            s_1_0 = _mm256_add_ps(s_1_0, _mm256_or_ps(v_4, mask_BLP));
            s_1_1 = _mm256_add_ps(s_1_1, _mm256_or_ps(v_5, mask_BLP));
            s_1_2 = _mm256_add_ps(s_1_2, _mm256_or_ps(v_6, mask_BLP));
            s_1_3 = _mm256_add_ps(s_1_3, _mm256_or_ps(v_7, mask_BLP));
            q_0 = _mm256_sub_ps(q_0, s_1_0);
            q_1 = _mm256_sub_ps(q_1, s_1_1);
            q_2 = _mm256_sub_ps(q_2, s_1_2);
            q_3 = _mm256_sub_ps(q_3, s_1_3);
            v_4 = _mm256_add_ps(v_4, q_0);
            v_5 = _mm256_add_ps(v_5, q_1);
            v_6 = _mm256_add_ps(v_6, q_2);
            v_7 = _mm256_add_ps(v_7, q_3);
            s_2_0 = _mm256_add_ps(s_2_0, _mm256_or_ps(v_4, mask_BLP));
            s_2_1 = _mm256_add_ps(s_2_1, _mm256_or_ps(v_5, mask_BLP));
            s_2_2 = _mm256_add_ps(s_2_2, _mm256_or_ps(v_6, mask_BLP));
            s_2_3 = _mm256_add_ps(s_2_3, _mm256_or_ps(v_7, mask_BLP));
            q_0 = s_0_0;
            q_1 = s_0_1;
            q_2 = s_0_2;
            q_3 = s_0_3;
            s_0_0 = _mm256_add_ps(s_0_0, _mm256_or_ps(v_8, mask_BLP));
            s_0_1 = _mm256_add_ps(s_0_1, _mm256_or_ps(v_9, mask_BLP));
            s_0_2 = _mm256_add_ps(s_0_2, _mm256_or_ps(v_10, mask_BLP));
            s_0_3 = _mm256_add_ps(s_0_3, _mm256_or_ps(v_11, mask_BLP));
            q_0 = _mm256_sub_ps(q_0, s_0_0);
            q_1 = _mm256_sub_ps(q_1, s_0_1);
            q_2 = _mm256_sub_ps(q_2, s_0_2);
            q_3 = _mm256_sub_ps(q_3, s_0_3);
            v_8 = _mm256_add_ps(v_8, q_0);
            v_9 = _mm256_add_ps(v_9, q_1);
            v_10 = _mm256_add_ps(v_10, q_2);
            v_11 = _mm256_add_ps(v_11, q_3);
            q_0 = s_1_0;
            q_1 = s_1_1;
            q_2 = s_1_2;
            q_3 = s_1_3;
            s_1_0 = _mm256_add_ps(s_1_0, _mm256_or_ps(v_8, mask_BLP));
            s_1_1 = _mm256_add_ps(s_1_1, _mm256_or_ps(v_9, mask_BLP));
            s_1_2 = _mm256_add_ps(s_1_2, _mm256_or_ps(v_10, mask_BLP));
            s_1_3 = _mm256_add_ps(s_1_3, _mm256_or_ps(v_11, mask_BLP));
            q_0 = _mm256_sub_ps(q_0, s_1_0);
            q_1 = _mm256_sub_ps(q_1, s_1_1);
            q_2 = _mm256_sub_ps(q_2, s_1_2);
            q_3 = _mm256_sub_ps(q_3, s_1_3);
            v_8 = _mm256_add_ps(v_8, q_0);
            v_9 = _mm256_add_ps(v_9, q_1);
            v_10 = _mm256_add_ps(v_10, q_2);
            v_11 = _mm256_add_ps(v_11, q_3);
            s_2_0 = _mm256_add_ps(s_2_0, _mm256_or_ps(v_8, mask_BLP));
            s_2_1 = _mm256_add_ps(s_2_1, _mm256_or_ps(v_9, mask_BLP));
            s_2_2 = _mm256_add_ps(s_2_2, _mm256_or_ps(v_10, mask_BLP));
            s_2_3 = _mm256_add_ps(s_2_3, _mm256_or_ps(v_11, mask_BLP));
            q_0 = s_0_0;
            q_1 = s_0_1;
            q_2 = s_0_2;
            q_3 = s_0_3;
            s_0_0 = _mm256_add_ps(s_0_0, _mm256_or_ps(v_12, mask_BLP));
            s_0_1 = _mm256_add_ps(s_0_1, _mm256_or_ps(v_13, mask_BLP));
            s_0_2 = _mm256_add_ps(s_0_2, _mm256_or_ps(v_14, mask_BLP));
            s_0_3 = _mm256_add_ps(s_0_3, _mm256_or_ps(v_15, mask_BLP));
            q_0 = _mm256_sub_ps(q_0, s_0_0);
            q_1 = _mm256_sub_ps(q_1, s_0_1);
            q_2 = _mm256_sub_ps(q_2, s_0_2);
            q_3 = _mm256_sub_ps(q_3, s_0_3);
            v_12 = _mm256_add_ps(v_12, q_0);
            v_13 = _mm256_add_ps(v_13, q_1);
            v_14 = _mm256_add_ps(v_14, q_2);
            v_15 = _mm256_add_ps(v_15, q_3);
            q_0 = s_1_0;
            q_1 = s_1_1;
            q_2 = s_1_2;
            q_3 = s_1_3;
            s_1_0 = _mm256_add_ps(s_1_0, _mm256_or_ps(v_12, mask_BLP));
            s_1_1 = _mm256_add_ps(s_1_1, _mm256_or_ps(v_13, mask_BLP));
            s_1_2 = _mm256_add_ps(s_1_2, _mm256_or_ps(v_14, mask_BLP));
            s_1_3 = _mm256_add_ps(s_1_3, _mm256_or_ps(v_15, mask_BLP));
            q_0 = _mm256_sub_ps(q_0, s_1_0);
            q_1 = _mm256_sub_ps(q_1, s_1_1);
            q_2 = _mm256_sub_ps(q_2, s_1_2);
            q_3 = _mm256_sub_ps(q_3, s_1_3);
            v_12 = _mm256_add_ps(v_12, q_0);
            v_13 = _mm256_add_ps(v_13, q_1);
            v_14 = _mm256_add_ps(v_14, q_2);
            v_15 = _mm256_add_ps(v_15, q_3);
            s_2_0 = _mm256_add_ps(s_2_0, _mm256_or_ps(v_12, mask_BLP));
            s_2_1 = _mm256_add_ps(s_2_1, _mm256_or_ps(v_13, mask_BLP));
            s_2_2 = _mm256_add_ps(s_2_2, _mm256_or_ps(v_14, mask_BLP));
            s_2_3 = _mm256_add_ps(s_2_3, _mm256_or_ps(v_15, mask_BLP));
          }
          if(i + 32 <= n){
            v_0 = _mm256_mul_ps(_mm256_loadu_ps(v_base), scale_mask);
            v_1 = _mm256_mul_ps(_mm256_loadu_ps(v_base + 8), scale_mask);
            v_2 = _mm256_mul_ps(_mm256_loadu_ps(v_base + 16), scale_mask);
            v_3 = _mm256_mul_ps(_mm256_loadu_ps(v_base + 24), scale_mask);
            v_4 = _mm256_mul_ps(_mm256_loadu_ps(v_base + 32), scale_mask);
            v_5 = _mm256_mul_ps(_mm256_loadu_ps(v_base + 40), scale_mask);
            v_6 = _mm256_mul_ps(_mm256_loadu_ps(v_base + 48), scale_mask);
            v_7 = _mm256_mul_ps(_mm256_loadu_ps(v_base + 56), scale_mask);
            v_0 = _mm256_mul_ps(v_0, v_0);
            v_1 = _mm256_mul_ps(v_1, v_1);
            v_2 = _mm256_mul_ps(v_2, v_2);
            v_3 = _mm256_mul_ps(v_3, v_3);
            v_4 = _mm256_mul_ps(v_4, v_4);
            v_5 = _mm256_mul_ps(v_5, v_5);
            v_6 = _mm256_mul_ps(v_6, v_6);
            v_7 = _mm256_mul_ps(v_7, v_7);
            q_0 = s_0_0;
            q_1 = s_0_1;
            q_2 = s_0_2;
            q_3 = s_0_3;
            s_0_0 = _mm256_add_ps(s_0_0, _mm256_or_ps(v_0, mask_BLP));
            s_0_1 = _mm256_add_ps(s_0_1, _mm256_or_ps(v_1, mask_BLP));
            s_0_2 = _mm256_add_ps(s_0_2, _mm256_or_ps(v_2, mask_BLP));
            s_0_3 = _mm256_add_ps(s_0_3, _mm256_or_ps(v_3, mask_BLP));
            q_0 = _mm256_sub_ps(q_0, s_0_0);
            q_1 = _mm256_sub_ps(q_1, s_0_1);
            q_2 = _mm256_sub_ps(q_2, s_0_2);
            q_3 = _mm256_sub_ps(q_3, s_0_3);
            v_0 = _mm256_add_ps(v_0, q_0);
            v_1 = _mm256_add_ps(v_1, q_1);
            v_2 = _mm256_add_ps(v_2, q_2);
            v_3 = _mm256_add_ps(v_3, q_3);
            q_0 = s_1_0;
            q_1 = s_1_1;
            q_2 = s_1_2;
            q_3 = s_1_3;
            s_1_0 = _mm256_add_ps(s_1_0, _mm256_or_ps(v_0, mask_BLP));
            s_1_1 = _mm256_add_ps(s_1_1, _mm256_or_ps(v_1, mask_BLP));
            s_1_2 = _mm256_add_ps(s_1_2, _mm256_or_ps(v_2, mask_BLP));
            s_1_3 = _mm256_add_ps(s_1_3, _mm256_or_ps(v_3, mask_BLP));
            q_0 = _mm256_sub_ps(q_0, s_1_0);
            q_1 = _mm256_sub_ps(q_1, s_1_1);
            q_2 = _mm256_sub_ps(q_2, s_1_2);
            q_3 = _mm256_sub_ps(q_3, s_1_3);
            v_0 = _mm256_add_ps(v_0, q_0);
            v_1 = _mm256_add_ps(v_1, q_1);
            v_2 = _mm256_add_ps(v_2, q_2);
            v_3 = _mm256_add_ps(v_3, q_3);
            s_2_0 = _mm256_add_ps(s_2_0, _mm256_or_ps(v_0, mask_BLP));
            s_2_1 = _mm256_add_ps(s_2_1, _mm256_or_ps(v_1, mask_BLP));
            s_2_2 = _mm256_add_ps(s_2_2, _mm256_or_ps(v_2, mask_BLP));
            s_2_3 = _mm256_add_ps(s_2_3, _mm256_or_ps(v_3, mask_BLP));
            q_0 = s_0_0;
            q_1 = s_0_1;
            q_2 = s_0_2;
            q_3 = s_0_3;
            s_0_0 = _mm256_add_ps(s_0_0, _mm256_or_ps(v_4, mask_BLP));
            s_0_1 = _mm256_add_ps(s_0_1, _mm256_or_ps(v_5, mask_BLP));
            s_0_2 = _mm256_add_ps(s_0_2, _mm256_or_ps(v_6, mask_BLP));
            s_0_3 = _mm256_add_ps(s_0_3, _mm256_or_ps(v_7, mask_BLP));
            q_0 = _mm256_sub_ps(q_0, s_0_0);
            q_1 = _mm256_sub_ps(q_1, s_0_1);
            q_2 = _mm256_sub_ps(q_2, s_0_2);
            q_3 = _mm256_sub_ps(q_3, s_0_3);
            v_4 = _mm256_add_ps(v_4, q_0);
            v_5 = _mm256_add_ps(v_5, q_1);
            v_6 = _mm256_add_ps(v_6, q_2);
            v_7 = _mm256_add_ps(v_7, q_3);
            q_0 = s_1_0;
            q_1 = s_1_1;
            q_2 = s_1_2;
            q_3 = s_1_3;
            s_1_0 = _mm256_add_ps(s_1_0, _mm256_or_ps(v_4, mask_BLP));
            s_1_1 = _mm256_add_ps(s_1_1, _mm256_or_ps(v_5, mask_BLP));
            s_1_2 = _mm256_add_ps(s_1_2, _mm256_or_ps(v_6, mask_BLP));
            s_1_3 = _mm256_add_ps(s_1_3, _mm256_or_ps(v_7, mask_BLP));
            q_0 = _mm256_sub_ps(q_0, s_1_0);
            q_1 = _mm256_sub_ps(q_1, s_1_1);
            q_2 = _mm256_sub_ps(q_2, s_1_2);
            q_3 = _mm256_sub_ps(q_3, s_1_3);
            v_4 = _mm256_add_ps(v_4, q_0);
            v_5 = _mm256_add_ps(v_5, q_1);
            v_6 = _mm256_add_ps(v_6, q_2);
            v_7 = _mm256_add_ps(v_7, q_3);
            s_2_0 = _mm256_add_ps(s_2_0, _mm256_or_ps(v_4, mask_BLP));
            s_2_1 = _mm256_add_ps(s_2_1, _mm256_or_ps(v_5, mask_BLP));
            s_2_2 = _mm256_add_ps(s_2_2, _mm256_or_ps(v_6, mask_BLP));
            s_2_3 = _mm256_add_ps(s_2_3, _mm256_or_ps(v_7, mask_BLP));
            i += 32, v_base += 64;
          }
          if(i + 16 <= n){
            v_0 = _mm256_mul_ps(_mm256_loadu_ps(v_base), scale_mask);
            v_1 = _mm256_mul_ps(_mm256_loadu_ps(v_base + 8), scale_mask);
            v_2 = _mm256_mul_ps(_mm256_loadu_ps(v_base + 16), scale_mask);
            v_3 = _mm256_mul_ps(_mm256_loadu_ps(v_base + 24), scale_mask);
            v_0 = _mm256_mul_ps(v_0, v_0);
            v_1 = _mm256_mul_ps(v_1, v_1);
            v_2 = _mm256_mul_ps(v_2, v_2);
            v_3 = _mm256_mul_ps(v_3, v_3);
            q_0 = s_0_0;
            q_1 = s_0_1;
            q_2 = s_0_2;
            q_3 = s_0_3;
            s_0_0 = _mm256_add_ps(s_0_0, _mm256_or_ps(v_0, mask_BLP));
            s_0_1 = _mm256_add_ps(s_0_1, _mm256_or_ps(v_1, mask_BLP));
            s_0_2 = _mm256_add_ps(s_0_2, _mm256_or_ps(v_2, mask_BLP));
            s_0_3 = _mm256_add_ps(s_0_3, _mm256_or_ps(v_3, mask_BLP));
            q_0 = _mm256_sub_ps(q_0, s_0_0);
            q_1 = _mm256_sub_ps(q_1, s_0_1);
            q_2 = _mm256_sub_ps(q_2, s_0_2);
            q_3 = _mm256_sub_ps(q_3, s_0_3);
            v_0 = _mm256_add_ps(v_0, q_0);
            v_1 = _mm256_add_ps(v_1, q_1);
            v_2 = _mm256_add_ps(v_2, q_2);
            v_3 = _mm256_add_ps(v_3, q_3);
            q_0 = s_1_0;
            q_1 = s_1_1;
            q_2 = s_1_2;
            q_3 = s_1_3;
            s_1_0 = _mm256_add_ps(s_1_0, _mm256_or_ps(v_0, mask_BLP));
            s_1_1 = _mm256_add_ps(s_1_1, _mm256_or_ps(v_1, mask_BLP));
            s_1_2 = _mm256_add_ps(s_1_2, _mm256_or_ps(v_2, mask_BLP));
            s_1_3 = _mm256_add_ps(s_1_3, _mm256_or_ps(v_3, mask_BLP));
            q_0 = _mm256_sub_ps(q_0, s_1_0);
            q_1 = _mm256_sub_ps(q_1, s_1_1);
            q_2 = _mm256_sub_ps(q_2, s_1_2);
            q_3 = _mm256_sub_ps(q_3, s_1_3);
            v_0 = _mm256_add_ps(v_0, q_0);
            v_1 = _mm256_add_ps(v_1, q_1);
            v_2 = _mm256_add_ps(v_2, q_2);
            v_3 = _mm256_add_ps(v_3, q_3);
            s_2_0 = _mm256_add_ps(s_2_0, _mm256_or_ps(v_0, mask_BLP));
            s_2_1 = _mm256_add_ps(s_2_1, _mm256_or_ps(v_1, mask_BLP));
            s_2_2 = _mm256_add_ps(s_2_2, _mm256_or_ps(v_2, mask_BLP));
            s_2_3 = _mm256_add_ps(s_2_3, _mm256_or_ps(v_3, mask_BLP));
            i += 16, v_base += 32;
          }
          if(i + 8 <= n){
            v_0 = _mm256_mul_ps(_mm256_loadu_ps(v_base), scale_mask);
            v_1 = _mm256_mul_ps(_mm256_loadu_ps(v_base + 8), scale_mask);
            v_0 = _mm256_mul_ps(v_0, v_0);
            v_1 = _mm256_mul_ps(v_1, v_1);
            q_0 = s_0_0;
            q_1 = s_0_1;
            s_0_0 = _mm256_add_ps(s_0_0, _mm256_or_ps(v_0, mask_BLP));
            s_0_1 = _mm256_add_ps(s_0_1, _mm256_or_ps(v_1, mask_BLP));
            q_0 = _mm256_sub_ps(q_0, s_0_0);
            q_1 = _mm256_sub_ps(q_1, s_0_1);
            v_0 = _mm256_add_ps(v_0, q_0);
            v_1 = _mm256_add_ps(v_1, q_1);
            q_0 = s_1_0;
            q_1 = s_1_1;
            s_1_0 = _mm256_add_ps(s_1_0, _mm256_or_ps(v_0, mask_BLP));
            s_1_1 = _mm256_add_ps(s_1_1, _mm256_or_ps(v_1, mask_BLP));
            q_0 = _mm256_sub_ps(q_0, s_1_0);
            q_1 = _mm256_sub_ps(q_1, s_1_1);
            v_0 = _mm256_add_ps(v_0, q_0);
            v_1 = _mm256_add_ps(v_1, q_1);
            s_2_0 = _mm256_add_ps(s_2_0, _mm256_or_ps(v_0, mask_BLP));
            s_2_1 = _mm256_add_ps(s_2_1, _mm256_or_ps(v_1, mask_BLP));
            i += 8, v_base += 16;
          }
          if(i + 4 <= n){
            v_0 = _mm256_mul_ps(_mm256_loadu_ps(v_base), scale_mask);
            v_0 = _mm256_mul_ps(v_0, v_0);
            q_0 = s_0_0;
            s_0_0 = _mm256_add_ps(s_0_0, _mm256_or_ps(v_0, mask_BLP));
            q_0 = _mm256_sub_ps(q_0, s_0_0);
            v_0 = _mm256_add_ps(v_0, q_0);
            q_0 = s_1_0;
            s_1_0 = _mm256_add_ps(s_1_0, _mm256_or_ps(v_0, mask_BLP));
            q_0 = _mm256_sub_ps(q_0, s_1_0);
            v_0 = _mm256_add_ps(v_0, q_0);
            s_2_0 = _mm256_add_ps(s_2_0, _mm256_or_ps(v_0, mask_BLP));
            i += 4, v_base += 8;
          }
          if(i < n){
            v_0 = _mm256_mul_ps((__m256)_mm256_set_pd(0, (n - i)>2?((double*)v_base)[2]:0, (n - i)>1?((double*)v_base)[1]:0, ((double*)v_base)[0]), scale_mask);
            v_0 = _mm256_mul_ps(v_0, v_0);
            q_0 = s_0_0;
            s_0_0 = _mm256_add_ps(s_0_0, _mm256_or_ps(v_0, mask_BLP));
            q_0 = _mm256_sub_ps(q_0, s_0_0);
            v_0 = _mm256_add_ps(v_0, q_0);
            q_0 = s_1_0;
            s_1_0 = _mm256_add_ps(s_1_0, _mm256_or_ps(v_0, mask_BLP));
            q_0 = _mm256_sub_ps(q_0, s_1_0);
            v_0 = _mm256_add_ps(v_0, q_0);
            s_2_0 = _mm256_add_ps(s_2_0, _mm256_or_ps(v_0, mask_BLP));
          }
        }else{

          for(i = 0; i + 64 <= n; i += 64, v_base += (incv * 128)){
            v_0 = _mm256_mul_ps(_mm256_set_ps(v_base[((incv * 6) + 1)], v_base[(incv * 6)], v_base[((incv * 4) + 1)], v_base[(incv * 4)], v_base[((incv * 2) + 1)], v_base[(incv * 2)], v_base[1], v_base[0]), scale_mask);
            v_1 = _mm256_mul_ps(_mm256_set_ps(v_base[((incv * 14) + 1)], v_base[(incv * 14)], v_base[((incv * 12) + 1)], v_base[(incv * 12)], v_base[((incv * 10) + 1)], v_base[(incv * 10)], v_base[((incv * 8) + 1)], v_base[(incv * 8)]), scale_mask);
            v_2 = _mm256_mul_ps(_mm256_set_ps(v_base[((incv * 22) + 1)], v_base[(incv * 22)], v_base[((incv * 20) + 1)], v_base[(incv * 20)], v_base[((incv * 18) + 1)], v_base[(incv * 18)], v_base[((incv * 16) + 1)], v_base[(incv * 16)]), scale_mask);
            v_3 = _mm256_mul_ps(_mm256_set_ps(v_base[((incv * 30) + 1)], v_base[(incv * 30)], v_base[((incv * 28) + 1)], v_base[(incv * 28)], v_base[((incv * 26) + 1)], v_base[(incv * 26)], v_base[((incv * 24) + 1)], v_base[(incv * 24)]), scale_mask);
            v_4 = _mm256_mul_ps(_mm256_set_ps(v_base[((incv * 38) + 1)], v_base[(incv * 38)], v_base[((incv * 36) + 1)], v_base[(incv * 36)], v_base[((incv * 34) + 1)], v_base[(incv * 34)], v_base[((incv * 32) + 1)], v_base[(incv * 32)]), scale_mask);
            v_5 = _mm256_mul_ps(_mm256_set_ps(v_base[((incv * 46) + 1)], v_base[(incv * 46)], v_base[((incv * 44) + 1)], v_base[(incv * 44)], v_base[((incv * 42) + 1)], v_base[(incv * 42)], v_base[((incv * 40) + 1)], v_base[(incv * 40)]), scale_mask);
            v_6 = _mm256_mul_ps(_mm256_set_ps(v_base[((incv * 54) + 1)], v_base[(incv * 54)], v_base[((incv * 52) + 1)], v_base[(incv * 52)], v_base[((incv * 50) + 1)], v_base[(incv * 50)], v_base[((incv * 48) + 1)], v_base[(incv * 48)]), scale_mask);
            v_7 = _mm256_mul_ps(_mm256_set_ps(v_base[((incv * 62) + 1)], v_base[(incv * 62)], v_base[((incv * 60) + 1)], v_base[(incv * 60)], v_base[((incv * 58) + 1)], v_base[(incv * 58)], v_base[((incv * 56) + 1)], v_base[(incv * 56)]), scale_mask);
            v_8 = _mm256_mul_ps(_mm256_set_ps(v_base[((incv * 70) + 1)], v_base[(incv * 70)], v_base[((incv * 68) + 1)], v_base[(incv * 68)], v_base[((incv * 66) + 1)], v_base[(incv * 66)], v_base[((incv * 64) + 1)], v_base[(incv * 64)]), scale_mask);
            v_9 = _mm256_mul_ps(_mm256_set_ps(v_base[((incv * 78) + 1)], v_base[(incv * 78)], v_base[((incv * 76) + 1)], v_base[(incv * 76)], v_base[((incv * 74) + 1)], v_base[(incv * 74)], v_base[((incv * 72) + 1)], v_base[(incv * 72)]), scale_mask);
            v_10 = _mm256_mul_ps(_mm256_set_ps(v_base[((incv * 86) + 1)], v_base[(incv * 86)], v_base[((incv * 84) + 1)], v_base[(incv * 84)], v_base[((incv * 82) + 1)], v_base[(incv * 82)], v_base[((incv * 80) + 1)], v_base[(incv * 80)]), scale_mask);
            v_11 = _mm256_mul_ps(_mm256_set_ps(v_base[((incv * 94) + 1)], v_base[(incv * 94)], v_base[((incv * 92) + 1)], v_base[(incv * 92)], v_base[((incv * 90) + 1)], v_base[(incv * 90)], v_base[((incv * 88) + 1)], v_base[(incv * 88)]), scale_mask);
            v_12 = _mm256_mul_ps(_mm256_set_ps(v_base[((incv * 102) + 1)], v_base[(incv * 102)], v_base[((incv * 100) + 1)], v_base[(incv * 100)], v_base[((incv * 98) + 1)], v_base[(incv * 98)], v_base[((incv * 96) + 1)], v_base[(incv * 96)]), scale_mask);
            v_13 = _mm256_mul_ps(_mm256_set_ps(v_base[((incv * 110) + 1)], v_base[(incv * 110)], v_base[((incv * 108) + 1)], v_base[(incv * 108)], v_base[((incv * 106) + 1)], v_base[(incv * 106)], v_base[((incv * 104) + 1)], v_base[(incv * 104)]), scale_mask);
            v_14 = _mm256_mul_ps(_mm256_set_ps(v_base[((incv * 118) + 1)], v_base[(incv * 118)], v_base[((incv * 116) + 1)], v_base[(incv * 116)], v_base[((incv * 114) + 1)], v_base[(incv * 114)], v_base[((incv * 112) + 1)], v_base[(incv * 112)]), scale_mask);
            v_15 = _mm256_mul_ps(_mm256_set_ps(v_base[((incv * 126) + 1)], v_base[(incv * 126)], v_base[((incv * 124) + 1)], v_base[(incv * 124)], v_base[((incv * 122) + 1)], v_base[(incv * 122)], v_base[((incv * 120) + 1)], v_base[(incv * 120)]), scale_mask);
            v_0 = _mm256_mul_ps(v_0, v_0);
            v_1 = _mm256_mul_ps(v_1, v_1);
            v_2 = _mm256_mul_ps(v_2, v_2);
            v_3 = _mm256_mul_ps(v_3, v_3);
            v_4 = _mm256_mul_ps(v_4, v_4);
            v_5 = _mm256_mul_ps(v_5, v_5);
            v_6 = _mm256_mul_ps(v_6, v_6);
            v_7 = _mm256_mul_ps(v_7, v_7);
            v_8 = _mm256_mul_ps(v_8, v_8);
            v_9 = _mm256_mul_ps(v_9, v_9);
            v_10 = _mm256_mul_ps(v_10, v_10);
            v_11 = _mm256_mul_ps(v_11, v_11);
            v_12 = _mm256_mul_ps(v_12, v_12);
            v_13 = _mm256_mul_ps(v_13, v_13);
            v_14 = _mm256_mul_ps(v_14, v_14);
            v_15 = _mm256_mul_ps(v_15, v_15);
            q_0 = s_0_0;
            q_1 = s_0_1;
            q_2 = s_0_2;
            q_3 = s_0_3;
            s_0_0 = _mm256_add_ps(s_0_0, _mm256_or_ps(v_0, mask_BLP));
            s_0_1 = _mm256_add_ps(s_0_1, _mm256_or_ps(v_1, mask_BLP));
            s_0_2 = _mm256_add_ps(s_0_2, _mm256_or_ps(v_2, mask_BLP));
            s_0_3 = _mm256_add_ps(s_0_3, _mm256_or_ps(v_3, mask_BLP));
            q_0 = _mm256_sub_ps(q_0, s_0_0);
            q_1 = _mm256_sub_ps(q_1, s_0_1);
            q_2 = _mm256_sub_ps(q_2, s_0_2);
            q_3 = _mm256_sub_ps(q_3, s_0_3);
            v_0 = _mm256_add_ps(v_0, q_0);
            v_1 = _mm256_add_ps(v_1, q_1);
            v_2 = _mm256_add_ps(v_2, q_2);
            v_3 = _mm256_add_ps(v_3, q_3);
            q_0 = s_1_0;
            q_1 = s_1_1;
            q_2 = s_1_2;
            q_3 = s_1_3;
            s_1_0 = _mm256_add_ps(s_1_0, _mm256_or_ps(v_0, mask_BLP));
            s_1_1 = _mm256_add_ps(s_1_1, _mm256_or_ps(v_1, mask_BLP));
            s_1_2 = _mm256_add_ps(s_1_2, _mm256_or_ps(v_2, mask_BLP));
            s_1_3 = _mm256_add_ps(s_1_3, _mm256_or_ps(v_3, mask_BLP));
            q_0 = _mm256_sub_ps(q_0, s_1_0);
            q_1 = _mm256_sub_ps(q_1, s_1_1);
            q_2 = _mm256_sub_ps(q_2, s_1_2);
            q_3 = _mm256_sub_ps(q_3, s_1_3);
            v_0 = _mm256_add_ps(v_0, q_0);
            v_1 = _mm256_add_ps(v_1, q_1);
            v_2 = _mm256_add_ps(v_2, q_2);
            v_3 = _mm256_add_ps(v_3, q_3);
            s_2_0 = _mm256_add_ps(s_2_0, _mm256_or_ps(v_0, mask_BLP));
            s_2_1 = _mm256_add_ps(s_2_1, _mm256_or_ps(v_1, mask_BLP));
            s_2_2 = _mm256_add_ps(s_2_2, _mm256_or_ps(v_2, mask_BLP));
            s_2_3 = _mm256_add_ps(s_2_3, _mm256_or_ps(v_3, mask_BLP));
            q_0 = s_0_0;
            q_1 = s_0_1;
            q_2 = s_0_2;
            q_3 = s_0_3;
            s_0_0 = _mm256_add_ps(s_0_0, _mm256_or_ps(v_4, mask_BLP));
            s_0_1 = _mm256_add_ps(s_0_1, _mm256_or_ps(v_5, mask_BLP));
            s_0_2 = _mm256_add_ps(s_0_2, _mm256_or_ps(v_6, mask_BLP));
            s_0_3 = _mm256_add_ps(s_0_3, _mm256_or_ps(v_7, mask_BLP));
            q_0 = _mm256_sub_ps(q_0, s_0_0);
            q_1 = _mm256_sub_ps(q_1, s_0_1);
            q_2 = _mm256_sub_ps(q_2, s_0_2);
            q_3 = _mm256_sub_ps(q_3, s_0_3);
            v_4 = _mm256_add_ps(v_4, q_0);
            v_5 = _mm256_add_ps(v_5, q_1);
            v_6 = _mm256_add_ps(v_6, q_2);
            v_7 = _mm256_add_ps(v_7, q_3);
            q_0 = s_1_0;
            q_1 = s_1_1;
            q_2 = s_1_2;
            q_3 = s_1_3;
            s_1_0 = _mm256_add_ps(s_1_0, _mm256_or_ps(v_4, mask_BLP));
            s_1_1 = _mm256_add_ps(s_1_1, _mm256_or_ps(v_5, mask_BLP));
            s_1_2 = _mm256_add_ps(s_1_2, _mm256_or_ps(v_6, mask_BLP));
            s_1_3 = _mm256_add_ps(s_1_3, _mm256_or_ps(v_7, mask_BLP));
            q_0 = _mm256_sub_ps(q_0, s_1_0);
            q_1 = _mm256_sub_ps(q_1, s_1_1);
            q_2 = _mm256_sub_ps(q_2, s_1_2);
            q_3 = _mm256_sub_ps(q_3, s_1_3);
            v_4 = _mm256_add_ps(v_4, q_0);
            v_5 = _mm256_add_ps(v_5, q_1);
            v_6 = _mm256_add_ps(v_6, q_2);
            v_7 = _mm256_add_ps(v_7, q_3);
            s_2_0 = _mm256_add_ps(s_2_0, _mm256_or_ps(v_4, mask_BLP));
            s_2_1 = _mm256_add_ps(s_2_1, _mm256_or_ps(v_5, mask_BLP));
            s_2_2 = _mm256_add_ps(s_2_2, _mm256_or_ps(v_6, mask_BLP));
            s_2_3 = _mm256_add_ps(s_2_3, _mm256_or_ps(v_7, mask_BLP));
            q_0 = s_0_0;
            q_1 = s_0_1;
            q_2 = s_0_2;
            q_3 = s_0_3;
            s_0_0 = _mm256_add_ps(s_0_0, _mm256_or_ps(v_8, mask_BLP));
            s_0_1 = _mm256_add_ps(s_0_1, _mm256_or_ps(v_9, mask_BLP));
            s_0_2 = _mm256_add_ps(s_0_2, _mm256_or_ps(v_10, mask_BLP));
            s_0_3 = _mm256_add_ps(s_0_3, _mm256_or_ps(v_11, mask_BLP));
            q_0 = _mm256_sub_ps(q_0, s_0_0);
            q_1 = _mm256_sub_ps(q_1, s_0_1);
            q_2 = _mm256_sub_ps(q_2, s_0_2);
            q_3 = _mm256_sub_ps(q_3, s_0_3);
            v_8 = _mm256_add_ps(v_8, q_0);
            v_9 = _mm256_add_ps(v_9, q_1);
            v_10 = _mm256_add_ps(v_10, q_2);
            v_11 = _mm256_add_ps(v_11, q_3);
            q_0 = s_1_0;
            q_1 = s_1_1;
            q_2 = s_1_2;
            q_3 = s_1_3;
            s_1_0 = _mm256_add_ps(s_1_0, _mm256_or_ps(v_8, mask_BLP));
            s_1_1 = _mm256_add_ps(s_1_1, _mm256_or_ps(v_9, mask_BLP));
            s_1_2 = _mm256_add_ps(s_1_2, _mm256_or_ps(v_10, mask_BLP));
            s_1_3 = _mm256_add_ps(s_1_3, _mm256_or_ps(v_11, mask_BLP));
            q_0 = _mm256_sub_ps(q_0, s_1_0);
            q_1 = _mm256_sub_ps(q_1, s_1_1);
            q_2 = _mm256_sub_ps(q_2, s_1_2);
            q_3 = _mm256_sub_ps(q_3, s_1_3);
            v_8 = _mm256_add_ps(v_8, q_0);
            v_9 = _mm256_add_ps(v_9, q_1);
            v_10 = _mm256_add_ps(v_10, q_2);
            v_11 = _mm256_add_ps(v_11, q_3);
            s_2_0 = _mm256_add_ps(s_2_0, _mm256_or_ps(v_8, mask_BLP));
            s_2_1 = _mm256_add_ps(s_2_1, _mm256_or_ps(v_9, mask_BLP));
            s_2_2 = _mm256_add_ps(s_2_2, _mm256_or_ps(v_10, mask_BLP));
            s_2_3 = _mm256_add_ps(s_2_3, _mm256_or_ps(v_11, mask_BLP));
            q_0 = s_0_0;
            q_1 = s_0_1;
            q_2 = s_0_2;
            q_3 = s_0_3;
            s_0_0 = _mm256_add_ps(s_0_0, _mm256_or_ps(v_12, mask_BLP));
            s_0_1 = _mm256_add_ps(s_0_1, _mm256_or_ps(v_13, mask_BLP));
            s_0_2 = _mm256_add_ps(s_0_2, _mm256_or_ps(v_14, mask_BLP));
            s_0_3 = _mm256_add_ps(s_0_3, _mm256_or_ps(v_15, mask_BLP));
            q_0 = _mm256_sub_ps(q_0, s_0_0);
            q_1 = _mm256_sub_ps(q_1, s_0_1);
            q_2 = _mm256_sub_ps(q_2, s_0_2);
            q_3 = _mm256_sub_ps(q_3, s_0_3);
            v_12 = _mm256_add_ps(v_12, q_0);
            v_13 = _mm256_add_ps(v_13, q_1);
            v_14 = _mm256_add_ps(v_14, q_2);
            v_15 = _mm256_add_ps(v_15, q_3);
            q_0 = s_1_0;
            q_1 = s_1_1;
            q_2 = s_1_2;
            q_3 = s_1_3;
            s_1_0 = _mm256_add_ps(s_1_0, _mm256_or_ps(v_12, mask_BLP));
            s_1_1 = _mm256_add_ps(s_1_1, _mm256_or_ps(v_13, mask_BLP));
            s_1_2 = _mm256_add_ps(s_1_2, _mm256_or_ps(v_14, mask_BLP));
            s_1_3 = _mm256_add_ps(s_1_3, _mm256_or_ps(v_15, mask_BLP));
            q_0 = _mm256_sub_ps(q_0, s_1_0);
            q_1 = _mm256_sub_ps(q_1, s_1_1);
            q_2 = _mm256_sub_ps(q_2, s_1_2);
            q_3 = _mm256_sub_ps(q_3, s_1_3);
            v_12 = _mm256_add_ps(v_12, q_0);
            v_13 = _mm256_add_ps(v_13, q_1);
            v_14 = _mm256_add_ps(v_14, q_2);
            v_15 = _mm256_add_ps(v_15, q_3);
            s_2_0 = _mm256_add_ps(s_2_0, _mm256_or_ps(v_12, mask_BLP));
            s_2_1 = _mm256_add_ps(s_2_1, _mm256_or_ps(v_13, mask_BLP));
            s_2_2 = _mm256_add_ps(s_2_2, _mm256_or_ps(v_14, mask_BLP));
            s_2_3 = _mm256_add_ps(s_2_3, _mm256_or_ps(v_15, mask_BLP));
          }
          if(i + 32 <= n){
            v_0 = _mm256_mul_ps(_mm256_set_ps(v_base[((incv * 6) + 1)], v_base[(incv * 6)], v_base[((incv * 4) + 1)], v_base[(incv * 4)], v_base[((incv * 2) + 1)], v_base[(incv * 2)], v_base[1], v_base[0]), scale_mask);
            v_1 = _mm256_mul_ps(_mm256_set_ps(v_base[((incv * 14) + 1)], v_base[(incv * 14)], v_base[((incv * 12) + 1)], v_base[(incv * 12)], v_base[((incv * 10) + 1)], v_base[(incv * 10)], v_base[((incv * 8) + 1)], v_base[(incv * 8)]), scale_mask);
            v_2 = _mm256_mul_ps(_mm256_set_ps(v_base[((incv * 22) + 1)], v_base[(incv * 22)], v_base[((incv * 20) + 1)], v_base[(incv * 20)], v_base[((incv * 18) + 1)], v_base[(incv * 18)], v_base[((incv * 16) + 1)], v_base[(incv * 16)]), scale_mask);
            v_3 = _mm256_mul_ps(_mm256_set_ps(v_base[((incv * 30) + 1)], v_base[(incv * 30)], v_base[((incv * 28) + 1)], v_base[(incv * 28)], v_base[((incv * 26) + 1)], v_base[(incv * 26)], v_base[((incv * 24) + 1)], v_base[(incv * 24)]), scale_mask);
            v_4 = _mm256_mul_ps(_mm256_set_ps(v_base[((incv * 38) + 1)], v_base[(incv * 38)], v_base[((incv * 36) + 1)], v_base[(incv * 36)], v_base[((incv * 34) + 1)], v_base[(incv * 34)], v_base[((incv * 32) + 1)], v_base[(incv * 32)]), scale_mask);
            v_5 = _mm256_mul_ps(_mm256_set_ps(v_base[((incv * 46) + 1)], v_base[(incv * 46)], v_base[((incv * 44) + 1)], v_base[(incv * 44)], v_base[((incv * 42) + 1)], v_base[(incv * 42)], v_base[((incv * 40) + 1)], v_base[(incv * 40)]), scale_mask);
            v_6 = _mm256_mul_ps(_mm256_set_ps(v_base[((incv * 54) + 1)], v_base[(incv * 54)], v_base[((incv * 52) + 1)], v_base[(incv * 52)], v_base[((incv * 50) + 1)], v_base[(incv * 50)], v_base[((incv * 48) + 1)], v_base[(incv * 48)]), scale_mask);
            v_7 = _mm256_mul_ps(_mm256_set_ps(v_base[((incv * 62) + 1)], v_base[(incv * 62)], v_base[((incv * 60) + 1)], v_base[(incv * 60)], v_base[((incv * 58) + 1)], v_base[(incv * 58)], v_base[((incv * 56) + 1)], v_base[(incv * 56)]), scale_mask);
            v_0 = _mm256_mul_ps(v_0, v_0);
            v_1 = _mm256_mul_ps(v_1, v_1);
            v_2 = _mm256_mul_ps(v_2, v_2);
            v_3 = _mm256_mul_ps(v_3, v_3);
            v_4 = _mm256_mul_ps(v_4, v_4);
            v_5 = _mm256_mul_ps(v_5, v_5);
            v_6 = _mm256_mul_ps(v_6, v_6);
            v_7 = _mm256_mul_ps(v_7, v_7);
            q_0 = s_0_0;
            q_1 = s_0_1;
            q_2 = s_0_2;
            q_3 = s_0_3;
            s_0_0 = _mm256_add_ps(s_0_0, _mm256_or_ps(v_0, mask_BLP));
            s_0_1 = _mm256_add_ps(s_0_1, _mm256_or_ps(v_1, mask_BLP));
            s_0_2 = _mm256_add_ps(s_0_2, _mm256_or_ps(v_2, mask_BLP));
            s_0_3 = _mm256_add_ps(s_0_3, _mm256_or_ps(v_3, mask_BLP));
            q_0 = _mm256_sub_ps(q_0, s_0_0);
            q_1 = _mm256_sub_ps(q_1, s_0_1);
            q_2 = _mm256_sub_ps(q_2, s_0_2);
            q_3 = _mm256_sub_ps(q_3, s_0_3);
            v_0 = _mm256_add_ps(v_0, q_0);
            v_1 = _mm256_add_ps(v_1, q_1);
            v_2 = _mm256_add_ps(v_2, q_2);
            v_3 = _mm256_add_ps(v_3, q_3);
            q_0 = s_1_0;
            q_1 = s_1_1;
            q_2 = s_1_2;
            q_3 = s_1_3;
            s_1_0 = _mm256_add_ps(s_1_0, _mm256_or_ps(v_0, mask_BLP));
            s_1_1 = _mm256_add_ps(s_1_1, _mm256_or_ps(v_1, mask_BLP));
            s_1_2 = _mm256_add_ps(s_1_2, _mm256_or_ps(v_2, mask_BLP));
            s_1_3 = _mm256_add_ps(s_1_3, _mm256_or_ps(v_3, mask_BLP));
            q_0 = _mm256_sub_ps(q_0, s_1_0);
            q_1 = _mm256_sub_ps(q_1, s_1_1);
            q_2 = _mm256_sub_ps(q_2, s_1_2);
            q_3 = _mm256_sub_ps(q_3, s_1_3);
            v_0 = _mm256_add_ps(v_0, q_0);
            v_1 = _mm256_add_ps(v_1, q_1);
            v_2 = _mm256_add_ps(v_2, q_2);
            v_3 = _mm256_add_ps(v_3, q_3);
            s_2_0 = _mm256_add_ps(s_2_0, _mm256_or_ps(v_0, mask_BLP));
            s_2_1 = _mm256_add_ps(s_2_1, _mm256_or_ps(v_1, mask_BLP));
            s_2_2 = _mm256_add_ps(s_2_2, _mm256_or_ps(v_2, mask_BLP));
            s_2_3 = _mm256_add_ps(s_2_3, _mm256_or_ps(v_3, mask_BLP));
            q_0 = s_0_0;
            q_1 = s_0_1;
            q_2 = s_0_2;
            q_3 = s_0_3;
            s_0_0 = _mm256_add_ps(s_0_0, _mm256_or_ps(v_4, mask_BLP));
            s_0_1 = _mm256_add_ps(s_0_1, _mm256_or_ps(v_5, mask_BLP));
            s_0_2 = _mm256_add_ps(s_0_2, _mm256_or_ps(v_6, mask_BLP));
            s_0_3 = _mm256_add_ps(s_0_3, _mm256_or_ps(v_7, mask_BLP));
            q_0 = _mm256_sub_ps(q_0, s_0_0);
            q_1 = _mm256_sub_ps(q_1, s_0_1);
            q_2 = _mm256_sub_ps(q_2, s_0_2);
            q_3 = _mm256_sub_ps(q_3, s_0_3);
            v_4 = _mm256_add_ps(v_4, q_0);
            v_5 = _mm256_add_ps(v_5, q_1);
            v_6 = _mm256_add_ps(v_6, q_2);
            v_7 = _mm256_add_ps(v_7, q_3);
            q_0 = s_1_0;
            q_1 = s_1_1;
            q_2 = s_1_2;
            q_3 = s_1_3;
            s_1_0 = _mm256_add_ps(s_1_0, _mm256_or_ps(v_4, mask_BLP));
            s_1_1 = _mm256_add_ps(s_1_1, _mm256_or_ps(v_5, mask_BLP));
            s_1_2 = _mm256_add_ps(s_1_2, _mm256_or_ps(v_6, mask_BLP));
            s_1_3 = _mm256_add_ps(s_1_3, _mm256_or_ps(v_7, mask_BLP));
            q_0 = _mm256_sub_ps(q_0, s_1_0);
            q_1 = _mm256_sub_ps(q_1, s_1_1);
            q_2 = _mm256_sub_ps(q_2, s_1_2);
            q_3 = _mm256_sub_ps(q_3, s_1_3);
            v_4 = _mm256_add_ps(v_4, q_0);
            v_5 = _mm256_add_ps(v_5, q_1);
            v_6 = _mm256_add_ps(v_6, q_2);
            v_7 = _mm256_add_ps(v_7, q_3);
            s_2_0 = _mm256_add_ps(s_2_0, _mm256_or_ps(v_4, mask_BLP));
            s_2_1 = _mm256_add_ps(s_2_1, _mm256_or_ps(v_5, mask_BLP));
            s_2_2 = _mm256_add_ps(s_2_2, _mm256_or_ps(v_6, mask_BLP));
            s_2_3 = _mm256_add_ps(s_2_3, _mm256_or_ps(v_7, mask_BLP));
            i += 32, v_base += (incv * 64);
          }
          if(i + 16 <= n){
            v_0 = _mm256_mul_ps(_mm256_set_ps(v_base[((incv * 6) + 1)], v_base[(incv * 6)], v_base[((incv * 4) + 1)], v_base[(incv * 4)], v_base[((incv * 2) + 1)], v_base[(incv * 2)], v_base[1], v_base[0]), scale_mask);
            v_1 = _mm256_mul_ps(_mm256_set_ps(v_base[((incv * 14) + 1)], v_base[(incv * 14)], v_base[((incv * 12) + 1)], v_base[(incv * 12)], v_base[((incv * 10) + 1)], v_base[(incv * 10)], v_base[((incv * 8) + 1)], v_base[(incv * 8)]), scale_mask);
            v_2 = _mm256_mul_ps(_mm256_set_ps(v_base[((incv * 22) + 1)], v_base[(incv * 22)], v_base[((incv * 20) + 1)], v_base[(incv * 20)], v_base[((incv * 18) + 1)], v_base[(incv * 18)], v_base[((incv * 16) + 1)], v_base[(incv * 16)]), scale_mask);
            v_3 = _mm256_mul_ps(_mm256_set_ps(v_base[((incv * 30) + 1)], v_base[(incv * 30)], v_base[((incv * 28) + 1)], v_base[(incv * 28)], v_base[((incv * 26) + 1)], v_base[(incv * 26)], v_base[((incv * 24) + 1)], v_base[(incv * 24)]), scale_mask);
            v_0 = _mm256_mul_ps(v_0, v_0);
            v_1 = _mm256_mul_ps(v_1, v_1);
            v_2 = _mm256_mul_ps(v_2, v_2);
            v_3 = _mm256_mul_ps(v_3, v_3);
            q_0 = s_0_0;
            q_1 = s_0_1;
            q_2 = s_0_2;
            q_3 = s_0_3;
            s_0_0 = _mm256_add_ps(s_0_0, _mm256_or_ps(v_0, mask_BLP));
            s_0_1 = _mm256_add_ps(s_0_1, _mm256_or_ps(v_1, mask_BLP));
            s_0_2 = _mm256_add_ps(s_0_2, _mm256_or_ps(v_2, mask_BLP));
            s_0_3 = _mm256_add_ps(s_0_3, _mm256_or_ps(v_3, mask_BLP));
            q_0 = _mm256_sub_ps(q_0, s_0_0);
            q_1 = _mm256_sub_ps(q_1, s_0_1);
            q_2 = _mm256_sub_ps(q_2, s_0_2);
            q_3 = _mm256_sub_ps(q_3, s_0_3);
            v_0 = _mm256_add_ps(v_0, q_0);
            v_1 = _mm256_add_ps(v_1, q_1);
            v_2 = _mm256_add_ps(v_2, q_2);
            v_3 = _mm256_add_ps(v_3, q_3);
            q_0 = s_1_0;
            q_1 = s_1_1;
            q_2 = s_1_2;
            q_3 = s_1_3;
            s_1_0 = _mm256_add_ps(s_1_0, _mm256_or_ps(v_0, mask_BLP));
            s_1_1 = _mm256_add_ps(s_1_1, _mm256_or_ps(v_1, mask_BLP));
            s_1_2 = _mm256_add_ps(s_1_2, _mm256_or_ps(v_2, mask_BLP));
            s_1_3 = _mm256_add_ps(s_1_3, _mm256_or_ps(v_3, mask_BLP));
            q_0 = _mm256_sub_ps(q_0, s_1_0);
            q_1 = _mm256_sub_ps(q_1, s_1_1);
            q_2 = _mm256_sub_ps(q_2, s_1_2);
            q_3 = _mm256_sub_ps(q_3, s_1_3);
            v_0 = _mm256_add_ps(v_0, q_0);
            v_1 = _mm256_add_ps(v_1, q_1);
            v_2 = _mm256_add_ps(v_2, q_2);
            v_3 = _mm256_add_ps(v_3, q_3);
            s_2_0 = _mm256_add_ps(s_2_0, _mm256_or_ps(v_0, mask_BLP));
            s_2_1 = _mm256_add_ps(s_2_1, _mm256_or_ps(v_1, mask_BLP));
            s_2_2 = _mm256_add_ps(s_2_2, _mm256_or_ps(v_2, mask_BLP));
            s_2_3 = _mm256_add_ps(s_2_3, _mm256_or_ps(v_3, mask_BLP));
            i += 16, v_base += (incv * 32);
          }
          if(i + 8 <= n){
            v_0 = _mm256_mul_ps(_mm256_set_ps(v_base[((incv * 6) + 1)], v_base[(incv * 6)], v_base[((incv * 4) + 1)], v_base[(incv * 4)], v_base[((incv * 2) + 1)], v_base[(incv * 2)], v_base[1], v_base[0]), scale_mask);
            v_1 = _mm256_mul_ps(_mm256_set_ps(v_base[((incv * 14) + 1)], v_base[(incv * 14)], v_base[((incv * 12) + 1)], v_base[(incv * 12)], v_base[((incv * 10) + 1)], v_base[(incv * 10)], v_base[((incv * 8) + 1)], v_base[(incv * 8)]), scale_mask);
            v_0 = _mm256_mul_ps(v_0, v_0);
            v_1 = _mm256_mul_ps(v_1, v_1);
            q_0 = s_0_0;
            q_1 = s_0_1;
            s_0_0 = _mm256_add_ps(s_0_0, _mm256_or_ps(v_0, mask_BLP));
            s_0_1 = _mm256_add_ps(s_0_1, _mm256_or_ps(v_1, mask_BLP));
            q_0 = _mm256_sub_ps(q_0, s_0_0);
            q_1 = _mm256_sub_ps(q_1, s_0_1);
            v_0 = _mm256_add_ps(v_0, q_0);
            v_1 = _mm256_add_ps(v_1, q_1);
            q_0 = s_1_0;
            q_1 = s_1_1;
            s_1_0 = _mm256_add_ps(s_1_0, _mm256_or_ps(v_0, mask_BLP));
            s_1_1 = _mm256_add_ps(s_1_1, _mm256_or_ps(v_1, mask_BLP));
            q_0 = _mm256_sub_ps(q_0, s_1_0);
            q_1 = _mm256_sub_ps(q_1, s_1_1);
            v_0 = _mm256_add_ps(v_0, q_0);
            v_1 = _mm256_add_ps(v_1, q_1);
            s_2_0 = _mm256_add_ps(s_2_0, _mm256_or_ps(v_0, mask_BLP));
            s_2_1 = _mm256_add_ps(s_2_1, _mm256_or_ps(v_1, mask_BLP));
            i += 8, v_base += (incv * 16);
          }
          if(i + 4 <= n){
            v_0 = _mm256_mul_ps(_mm256_set_ps(v_base[((incv * 6) + 1)], v_base[(incv * 6)], v_base[((incv * 4) + 1)], v_base[(incv * 4)], v_base[((incv * 2) + 1)], v_base[(incv * 2)], v_base[1], v_base[0]), scale_mask);
            v_0 = _mm256_mul_ps(v_0, v_0);
            q_0 = s_0_0;
            s_0_0 = _mm256_add_ps(s_0_0, _mm256_or_ps(v_0, mask_BLP));
            q_0 = _mm256_sub_ps(q_0, s_0_0);
            v_0 = _mm256_add_ps(v_0, q_0);
            q_0 = s_1_0;
            s_1_0 = _mm256_add_ps(s_1_0, _mm256_or_ps(v_0, mask_BLP));
            q_0 = _mm256_sub_ps(q_0, s_1_0);
            v_0 = _mm256_add_ps(v_0, q_0);
            s_2_0 = _mm256_add_ps(s_2_0, _mm256_or_ps(v_0, mask_BLP));
            i += 4, v_base += (incv * 8);
          }
          if(i < n){
            v_0 = _mm256_mul_ps((__m256)_mm256_set_pd(0, (n - i)>2?((double*)v_base)[(incv * 2)]:0, (n - i)>1?((double*)v_base)[incv]:0, ((double*)v_base)[0]), scale_mask);
            v_0 = _mm256_mul_ps(v_0, v_0);
            q_0 = s_0_0;
            s_0_0 = _mm256_add_ps(s_0_0, _mm256_or_ps(v_0, mask_BLP));
            q_0 = _mm256_sub_ps(q_0, s_0_0);
            v_0 = _mm256_add_ps(v_0, q_0);
            q_0 = s_1_0;
            s_1_0 = _mm256_add_ps(s_1_0, _mm256_or_ps(v_0, mask_BLP));
            q_0 = _mm256_sub_ps(q_0, s_1_0);
            v_0 = _mm256_add_ps(v_0, q_0);
            s_2_0 = _mm256_add_ps(s_2_0, _mm256_or_ps(v_0, mask_BLP));
          }
        }
        s_0_0 = _mm256_sub_ps(s_0_0, _mm256_set_ps(sum_base[1], sum_base[0], sum_base[1], sum_base[0], sum_base[1], sum_base[0], 0, 0));
        q_0 = (__m256)_mm256_broadcast_sd((double *)(sum_base));
        s_0_0 = _mm256_add_ps(s_0_0, _mm256_sub_ps(s_0_1, q_0));
        s_0_0 = _mm256_add_ps(s_0_0, _mm256_sub_ps(s_0_2, q_0));
        s_0_0 = _mm256_add_ps(s_0_0, _mm256_sub_ps(s_0_3, q_0));
        _mm256_store_ps((float*)tmp_cons, s_0_0);
        sum[0] = tmp_cons[0] + tmp_cons[1] + tmp_cons[2] + tmp_cons[3];
        s_1_0 = _mm256_sub_ps(s_1_0, _mm256_set_ps(sum_base[3], sum_base[2], sum_base[3], sum_base[2], sum_base[3], sum_base[2], 0, 0));
        q_0 = (__m256)_mm256_broadcast_sd((double *)(sum_base + 2));
        s_1_0 = _mm256_add_ps(s_1_0, _mm256_sub_ps(s_1_1, q_0));
        s_1_0 = _mm256_add_ps(s_1_0, _mm256_sub_ps(s_1_2, q_0));
        s_1_0 = _mm256_add_ps(s_1_0, _mm256_sub_ps(s_1_3, q_0));
        _mm256_store_ps((float*)tmp_cons, s_1_0);
        sum[1] = tmp_cons[0] + tmp_cons[1] + tmp_cons[2] + tmp_cons[3];
        s_2_0 = _mm256_sub_ps(s_2_0, _mm256_set_ps(sum_base[5], sum_base[4], sum_base[5], sum_base[4], sum_base[5], sum_base[4], 0, 0));
        q_0 = (__m256)_mm256_broadcast_sd((double *)(sum_base + 4));
        s_2_0 = _mm256_add_ps(s_2_0, _mm256_sub_ps(s_2_1, q_0));
        s_2_0 = _mm256_add_ps(s_2_0, _mm256_sub_ps(s_2_2, q_0));
        s_2_0 = _mm256_add_ps(s_2_0, _mm256_sub_ps(s_2_3, q_0));
        _mm256_store_ps((float*)tmp_cons, s_2_0);
        sum[2] = tmp_cons[0] + tmp_cons[1] + tmp_cons[2] + tmp_cons[3];
        RESET_DAZ_FLAG
        return;
      }
      default:{
        int i, j;

        float* sum_base = (float*) sum;
        float* v_base = (float*) v;
        __m256 v_0;
        __m256 q_0;
        __m256 s_0;
        __m256 s_buffer[MAX_FOLD];

        for(j = 0; j < fold; j += 1){
          s_buffer[j] = (__m256)_mm256_broadcast_sd((double *)(sum_base + (j * 2)));
        }
        if(incv == 1){

          for(i = 0; i + 4 <= n; i += 4, v_base += 8){
            v_0 = _mm256_mul_ps(_mm256_loadu_ps(v_base), scale_mask);
            v_0 = _mm256_mul_ps(v_0, v_0);
            for(j = 0; j < fold - 1; j++){
              s_0 = s_buffer[j];
              q_0 = _mm256_add_ps(s_0, _mm256_or_ps(v_0, mask_BLP));
              s_buffer[j] = q_0;
              q_0 = _mm256_sub_ps(s_0, q_0);
              v_0 = _mm256_add_ps(v_0, q_0);
            }
            s_buffer[j] = _mm256_add_ps(s_buffer[j], _mm256_or_ps(v_0, mask_BLP));
          }
          if(i < n){
            v_0 = _mm256_mul_ps((__m256)_mm256_set_pd(0, (n - i)>2?((double*)v_base)[2]:0, (n - i)>1?((double*)v_base)[1]:0, ((double*)v_base)[0]), scale_mask);
            v_0 = _mm256_mul_ps(v_0, v_0);
            for(j = 0; j < fold - 1; j++){
              s_0 = s_buffer[j];
              q_0 = _mm256_add_ps(s_0, _mm256_or_ps(v_0, mask_BLP));
              s_buffer[j] = q_0;
              q_0 = _mm256_sub_ps(s_0, q_0);
              v_0 = _mm256_add_ps(v_0, q_0);
            }
            s_buffer[j] = _mm256_add_ps(s_buffer[j], _mm256_or_ps(v_0, mask_BLP));
          }
        }else{

          for(i = 0; i + 4 <= n; i += 4, v_base += (incv * 8)){
            v_0 = _mm256_mul_ps(_mm256_set_ps(v_base[((incv * 6) + 1)], v_base[(incv * 6)], v_base[((incv * 4) + 1)], v_base[(incv * 4)], v_base[((incv * 2) + 1)], v_base[(incv * 2)], v_base[1], v_base[0]), scale_mask);
            v_0 = _mm256_mul_ps(v_0, v_0);
            for(j = 0; j < fold - 1; j++){
              s_0 = s_buffer[j];
              q_0 = _mm256_add_ps(s_0, _mm256_or_ps(v_0, mask_BLP));
              s_buffer[j] = q_0;
              q_0 = _mm256_sub_ps(s_0, q_0);
              v_0 = _mm256_add_ps(v_0, q_0);
            }
            s_buffer[j] = _mm256_add_ps(s_buffer[j], _mm256_or_ps(v_0, mask_BLP));
          }
          if(i < n){
            v_0 = _mm256_mul_ps((__m256)_mm256_set_pd(0, (n - i)>2?((double*)v_base)[(incv * 2)]:0, (n - i)>1?((double*)v_base)[incv]:0, ((double*)v_base)[0]), scale_mask);
            v_0 = _mm256_mul_ps(v_0, v_0);
            for(j = 0; j < fold - 1; j++){
              s_0 = s_buffer[j];
              q_0 = _mm256_add_ps(s_0, _mm256_or_ps(v_0, mask_BLP));
              s_buffer[j] = q_0;
              q_0 = _mm256_sub_ps(s_0, q_0);
              v_0 = _mm256_add_ps(v_0, q_0);
            }
            s_buffer[j] = _mm256_add_ps(s_buffer[j], _mm256_or_ps(v_0, mask_BLP));
          }
        }
        for(j = 0; j < fold; j += 1){
          s_buffer[j] = _mm256_sub_ps(s_buffer[j], _mm256_set_ps(sum_base[((j * 2) + 1)], sum_base[(j * 2)], sum_base[((j * 2) + 1)], sum_base[(j * 2)], sum_base[((j * 2) + 1)], sum_base[(j * 2)], 0, 0));
          _mm256_store_ps((float*)tmp_cons, s_buffer[j]);
          sum[j] = tmp_cons[0] + tmp_cons[1] + tmp_cons[2] + tmp_cons[3];
        }
        RESET_DAZ_FLAG
        return;
      }
    }
  #elif defined(__SSE2__)
    __m128 scale_mask = _mm_set1_ps(scale);
    __m128 mask_BLP; SSE_BLP_MASKS(mask_BLP);
    float complex tmp_cons[2] __attribute__((aligned(16)));
    SET_DAZ_FLAG;
    switch(fold){
      case 3:{
        int i;

        float* sum_base = (float*) sum;
        float* v_base = (float*) v;
        __m128 v_0, v_1, v_2, v_3, v_4, v_5, v_6, v_7, v_8, v_9, v_10, v_11;
        __m128 q_0, q_1, q_2, q_3;
        __m128 s_0_0, s_0_1, s_0_2, s_0_3;
        __m128 s_1_0, s_1_1, s_1_2, s_1_3;
        __m128 s_2_0, s_2_1, s_2_2, s_2_3;

        s_0_0 = s_0_1 = s_0_2 = s_0_3 = (__m128)_mm_load1_pd((double *)(sum_base));
        s_1_0 = s_1_1 = s_1_2 = s_1_3 = (__m128)_mm_load1_pd((double *)(sum_base + 2));
        s_2_0 = s_2_1 = s_2_2 = s_2_3 = (__m128)_mm_load1_pd((double *)(sum_base + 4));
        if(incv == 1){

          for(i = 0; i + 24 <= n; i += 24, v_base += 48){
            v_0 = _mm_mul_ps(_mm_loadu_ps(v_base), scale_mask);
            v_1 = _mm_mul_ps(_mm_loadu_ps(v_base + 4), scale_mask);
            v_2 = _mm_mul_ps(_mm_loadu_ps(v_base + 8), scale_mask);
            v_3 = _mm_mul_ps(_mm_loadu_ps(v_base + 12), scale_mask);
            v_4 = _mm_mul_ps(_mm_loadu_ps(v_base + 16), scale_mask);
            v_5 = _mm_mul_ps(_mm_loadu_ps(v_base + 20), scale_mask);
            v_6 = _mm_mul_ps(_mm_loadu_ps(v_base + 24), scale_mask);
            v_7 = _mm_mul_ps(_mm_loadu_ps(v_base + 28), scale_mask);
            v_8 = _mm_mul_ps(_mm_loadu_ps(v_base + 32), scale_mask);
            v_9 = _mm_mul_ps(_mm_loadu_ps(v_base + 36), scale_mask);
            v_10 = _mm_mul_ps(_mm_loadu_ps(v_base + 40), scale_mask);
            v_11 = _mm_mul_ps(_mm_loadu_ps(v_base + 44), scale_mask);
            v_0 = _mm_mul_ps(v_0, v_0);
            v_1 = _mm_mul_ps(v_1, v_1);
            v_2 = _mm_mul_ps(v_2, v_2);
            v_3 = _mm_mul_ps(v_3, v_3);
            v_4 = _mm_mul_ps(v_4, v_4);
            v_5 = _mm_mul_ps(v_5, v_5);
            v_6 = _mm_mul_ps(v_6, v_6);
            v_7 = _mm_mul_ps(v_7, v_7);
            v_8 = _mm_mul_ps(v_8, v_8);
            v_9 = _mm_mul_ps(v_9, v_9);
            v_10 = _mm_mul_ps(v_10, v_10);
            v_11 = _mm_mul_ps(v_11, v_11);
            q_0 = s_0_0;
            q_1 = s_0_1;
            q_2 = s_0_2;
            q_3 = s_0_3;
            s_0_0 = _mm_add_ps(s_0_0, _mm_or_ps(v_0, mask_BLP));
            s_0_1 = _mm_add_ps(s_0_1, _mm_or_ps(v_1, mask_BLP));
            s_0_2 = _mm_add_ps(s_0_2, _mm_or_ps(v_2, mask_BLP));
            s_0_3 = _mm_add_ps(s_0_3, _mm_or_ps(v_3, mask_BLP));
            q_0 = _mm_sub_ps(q_0, s_0_0);
            q_1 = _mm_sub_ps(q_1, s_0_1);
            q_2 = _mm_sub_ps(q_2, s_0_2);
            q_3 = _mm_sub_ps(q_3, s_0_3);
            v_0 = _mm_add_ps(v_0, q_0);
            v_1 = _mm_add_ps(v_1, q_1);
            v_2 = _mm_add_ps(v_2, q_2);
            v_3 = _mm_add_ps(v_3, q_3);
            q_0 = s_1_0;
            q_1 = s_1_1;
            q_2 = s_1_2;
            q_3 = s_1_3;
            s_1_0 = _mm_add_ps(s_1_0, _mm_or_ps(v_0, mask_BLP));
            s_1_1 = _mm_add_ps(s_1_1, _mm_or_ps(v_1, mask_BLP));
            s_1_2 = _mm_add_ps(s_1_2, _mm_or_ps(v_2, mask_BLP));
            s_1_3 = _mm_add_ps(s_1_3, _mm_or_ps(v_3, mask_BLP));
            q_0 = _mm_sub_ps(q_0, s_1_0);
            q_1 = _mm_sub_ps(q_1, s_1_1);
            q_2 = _mm_sub_ps(q_2, s_1_2);
            q_3 = _mm_sub_ps(q_3, s_1_3);
            v_0 = _mm_add_ps(v_0, q_0);
            v_1 = _mm_add_ps(v_1, q_1);
            v_2 = _mm_add_ps(v_2, q_2);
            v_3 = _mm_add_ps(v_3, q_3);
            s_2_0 = _mm_add_ps(s_2_0, _mm_or_ps(v_0, mask_BLP));
            s_2_1 = _mm_add_ps(s_2_1, _mm_or_ps(v_1, mask_BLP));
            s_2_2 = _mm_add_ps(s_2_2, _mm_or_ps(v_2, mask_BLP));
            s_2_3 = _mm_add_ps(s_2_3, _mm_or_ps(v_3, mask_BLP));
            q_0 = s_0_0;
            q_1 = s_0_1;
            q_2 = s_0_2;
            q_3 = s_0_3;
            s_0_0 = _mm_add_ps(s_0_0, _mm_or_ps(v_4, mask_BLP));
            s_0_1 = _mm_add_ps(s_0_1, _mm_or_ps(v_5, mask_BLP));
            s_0_2 = _mm_add_ps(s_0_2, _mm_or_ps(v_6, mask_BLP));
            s_0_3 = _mm_add_ps(s_0_3, _mm_or_ps(v_7, mask_BLP));
            q_0 = _mm_sub_ps(q_0, s_0_0);
            q_1 = _mm_sub_ps(q_1, s_0_1);
            q_2 = _mm_sub_ps(q_2, s_0_2);
            q_3 = _mm_sub_ps(q_3, s_0_3);
            v_4 = _mm_add_ps(v_4, q_0);
            v_5 = _mm_add_ps(v_5, q_1);
            v_6 = _mm_add_ps(v_6, q_2);
            v_7 = _mm_add_ps(v_7, q_3);
            q_0 = s_1_0;
            q_1 = s_1_1;
            q_2 = s_1_2;
            q_3 = s_1_3;
            s_1_0 = _mm_add_ps(s_1_0, _mm_or_ps(v_4, mask_BLP));
            s_1_1 = _mm_add_ps(s_1_1, _mm_or_ps(v_5, mask_BLP));
            s_1_2 = _mm_add_ps(s_1_2, _mm_or_ps(v_6, mask_BLP));
            s_1_3 = _mm_add_ps(s_1_3, _mm_or_ps(v_7, mask_BLP));
            q_0 = _mm_sub_ps(q_0, s_1_0);
            q_1 = _mm_sub_ps(q_1, s_1_1);
            q_2 = _mm_sub_ps(q_2, s_1_2);
            q_3 = _mm_sub_ps(q_3, s_1_3);
            v_4 = _mm_add_ps(v_4, q_0);
            v_5 = _mm_add_ps(v_5, q_1);
            v_6 = _mm_add_ps(v_6, q_2);
            v_7 = _mm_add_ps(v_7, q_3);
            s_2_0 = _mm_add_ps(s_2_0, _mm_or_ps(v_4, mask_BLP));
            s_2_1 = _mm_add_ps(s_2_1, _mm_or_ps(v_5, mask_BLP));
            s_2_2 = _mm_add_ps(s_2_2, _mm_or_ps(v_6, mask_BLP));
            s_2_3 = _mm_add_ps(s_2_3, _mm_or_ps(v_7, mask_BLP));
            q_0 = s_0_0;
            q_1 = s_0_1;
            q_2 = s_0_2;
            q_3 = s_0_3;
            s_0_0 = _mm_add_ps(s_0_0, _mm_or_ps(v_8, mask_BLP));
            s_0_1 = _mm_add_ps(s_0_1, _mm_or_ps(v_9, mask_BLP));
            s_0_2 = _mm_add_ps(s_0_2, _mm_or_ps(v_10, mask_BLP));
            s_0_3 = _mm_add_ps(s_0_3, _mm_or_ps(v_11, mask_BLP));
            q_0 = _mm_sub_ps(q_0, s_0_0);
            q_1 = _mm_sub_ps(q_1, s_0_1);
            q_2 = _mm_sub_ps(q_2, s_0_2);
            q_3 = _mm_sub_ps(q_3, s_0_3);
            v_8 = _mm_add_ps(v_8, q_0);
            v_9 = _mm_add_ps(v_9, q_1);
            v_10 = _mm_add_ps(v_10, q_2);
            v_11 = _mm_add_ps(v_11, q_3);
            q_0 = s_1_0;
            q_1 = s_1_1;
            q_2 = s_1_2;
            q_3 = s_1_3;
            s_1_0 = _mm_add_ps(s_1_0, _mm_or_ps(v_8, mask_BLP));
            s_1_1 = _mm_add_ps(s_1_1, _mm_or_ps(v_9, mask_BLP));
            s_1_2 = _mm_add_ps(s_1_2, _mm_or_ps(v_10, mask_BLP));
            s_1_3 = _mm_add_ps(s_1_3, _mm_or_ps(v_11, mask_BLP));
            q_0 = _mm_sub_ps(q_0, s_1_0);
            q_1 = _mm_sub_ps(q_1, s_1_1);
            q_2 = _mm_sub_ps(q_2, s_1_2);
            q_3 = _mm_sub_ps(q_3, s_1_3);
            v_8 = _mm_add_ps(v_8, q_0);
            v_9 = _mm_add_ps(v_9, q_1);
            v_10 = _mm_add_ps(v_10, q_2);
            v_11 = _mm_add_ps(v_11, q_3);
            s_2_0 = _mm_add_ps(s_2_0, _mm_or_ps(v_8, mask_BLP));
            s_2_1 = _mm_add_ps(s_2_1, _mm_or_ps(v_9, mask_BLP));
            s_2_2 = _mm_add_ps(s_2_2, _mm_or_ps(v_10, mask_BLP));
            s_2_3 = _mm_add_ps(s_2_3, _mm_or_ps(v_11, mask_BLP));
          }
          if(i + 16 <= n){
            v_0 = _mm_mul_ps(_mm_loadu_ps(v_base), scale_mask);
            v_1 = _mm_mul_ps(_mm_loadu_ps(v_base + 4), scale_mask);
            v_2 = _mm_mul_ps(_mm_loadu_ps(v_base + 8), scale_mask);
            v_3 = _mm_mul_ps(_mm_loadu_ps(v_base + 12), scale_mask);
            v_4 = _mm_mul_ps(_mm_loadu_ps(v_base + 16), scale_mask);
            v_5 = _mm_mul_ps(_mm_loadu_ps(v_base + 20), scale_mask);
            v_6 = _mm_mul_ps(_mm_loadu_ps(v_base + 24), scale_mask);
            v_7 = _mm_mul_ps(_mm_loadu_ps(v_base + 28), scale_mask);
            v_0 = _mm_mul_ps(v_0, v_0);
            v_1 = _mm_mul_ps(v_1, v_1);
            v_2 = _mm_mul_ps(v_2, v_2);
            v_3 = _mm_mul_ps(v_3, v_3);
            v_4 = _mm_mul_ps(v_4, v_4);
            v_5 = _mm_mul_ps(v_5, v_5);
            v_6 = _mm_mul_ps(v_6, v_6);
            v_7 = _mm_mul_ps(v_7, v_7);
            q_0 = s_0_0;
            q_1 = s_0_1;
            q_2 = s_0_2;
            q_3 = s_0_3;
            s_0_0 = _mm_add_ps(s_0_0, _mm_or_ps(v_0, mask_BLP));
            s_0_1 = _mm_add_ps(s_0_1, _mm_or_ps(v_1, mask_BLP));
            s_0_2 = _mm_add_ps(s_0_2, _mm_or_ps(v_2, mask_BLP));
            s_0_3 = _mm_add_ps(s_0_3, _mm_or_ps(v_3, mask_BLP));
            q_0 = _mm_sub_ps(q_0, s_0_0);
            q_1 = _mm_sub_ps(q_1, s_0_1);
            q_2 = _mm_sub_ps(q_2, s_0_2);
            q_3 = _mm_sub_ps(q_3, s_0_3);
            v_0 = _mm_add_ps(v_0, q_0);
            v_1 = _mm_add_ps(v_1, q_1);
            v_2 = _mm_add_ps(v_2, q_2);
            v_3 = _mm_add_ps(v_3, q_3);
            q_0 = s_1_0;
            q_1 = s_1_1;
            q_2 = s_1_2;
            q_3 = s_1_3;
            s_1_0 = _mm_add_ps(s_1_0, _mm_or_ps(v_0, mask_BLP));
            s_1_1 = _mm_add_ps(s_1_1, _mm_or_ps(v_1, mask_BLP));
            s_1_2 = _mm_add_ps(s_1_2, _mm_or_ps(v_2, mask_BLP));
            s_1_3 = _mm_add_ps(s_1_3, _mm_or_ps(v_3, mask_BLP));
            q_0 = _mm_sub_ps(q_0, s_1_0);
            q_1 = _mm_sub_ps(q_1, s_1_1);
            q_2 = _mm_sub_ps(q_2, s_1_2);
            q_3 = _mm_sub_ps(q_3, s_1_3);
            v_0 = _mm_add_ps(v_0, q_0);
            v_1 = _mm_add_ps(v_1, q_1);
            v_2 = _mm_add_ps(v_2, q_2);
            v_3 = _mm_add_ps(v_3, q_3);
            s_2_0 = _mm_add_ps(s_2_0, _mm_or_ps(v_0, mask_BLP));
            s_2_1 = _mm_add_ps(s_2_1, _mm_or_ps(v_1, mask_BLP));
            s_2_2 = _mm_add_ps(s_2_2, _mm_or_ps(v_2, mask_BLP));
            s_2_3 = _mm_add_ps(s_2_3, _mm_or_ps(v_3, mask_BLP));
            q_0 = s_0_0;
            q_1 = s_0_1;
            q_2 = s_0_2;
            q_3 = s_0_3;
            s_0_0 = _mm_add_ps(s_0_0, _mm_or_ps(v_4, mask_BLP));
            s_0_1 = _mm_add_ps(s_0_1, _mm_or_ps(v_5, mask_BLP));
            s_0_2 = _mm_add_ps(s_0_2, _mm_or_ps(v_6, mask_BLP));
            s_0_3 = _mm_add_ps(s_0_3, _mm_or_ps(v_7, mask_BLP));
            q_0 = _mm_sub_ps(q_0, s_0_0);
            q_1 = _mm_sub_ps(q_1, s_0_1);
            q_2 = _mm_sub_ps(q_2, s_0_2);
            q_3 = _mm_sub_ps(q_3, s_0_3);
            v_4 = _mm_add_ps(v_4, q_0);
            v_5 = _mm_add_ps(v_5, q_1);
            v_6 = _mm_add_ps(v_6, q_2);
            v_7 = _mm_add_ps(v_7, q_3);
            q_0 = s_1_0;
            q_1 = s_1_1;
            q_2 = s_1_2;
            q_3 = s_1_3;
            s_1_0 = _mm_add_ps(s_1_0, _mm_or_ps(v_4, mask_BLP));
            s_1_1 = _mm_add_ps(s_1_1, _mm_or_ps(v_5, mask_BLP));
            s_1_2 = _mm_add_ps(s_1_2, _mm_or_ps(v_6, mask_BLP));
            s_1_3 = _mm_add_ps(s_1_3, _mm_or_ps(v_7, mask_BLP));
            q_0 = _mm_sub_ps(q_0, s_1_0);
            q_1 = _mm_sub_ps(q_1, s_1_1);
            q_2 = _mm_sub_ps(q_2, s_1_2);
            q_3 = _mm_sub_ps(q_3, s_1_3);
            v_4 = _mm_add_ps(v_4, q_0);
            v_5 = _mm_add_ps(v_5, q_1);
            v_6 = _mm_add_ps(v_6, q_2);
            v_7 = _mm_add_ps(v_7, q_3);
            s_2_0 = _mm_add_ps(s_2_0, _mm_or_ps(v_4, mask_BLP));
            s_2_1 = _mm_add_ps(s_2_1, _mm_or_ps(v_5, mask_BLP));
            s_2_2 = _mm_add_ps(s_2_2, _mm_or_ps(v_6, mask_BLP));
            s_2_3 = _mm_add_ps(s_2_3, _mm_or_ps(v_7, mask_BLP));
            i += 16, v_base += 32;
          }
          if(i + 8 <= n){
            v_0 = _mm_mul_ps(_mm_loadu_ps(v_base), scale_mask);
            v_1 = _mm_mul_ps(_mm_loadu_ps(v_base + 4), scale_mask);
            v_2 = _mm_mul_ps(_mm_loadu_ps(v_base + 8), scale_mask);
            v_3 = _mm_mul_ps(_mm_loadu_ps(v_base + 12), scale_mask);
            v_0 = _mm_mul_ps(v_0, v_0);
            v_1 = _mm_mul_ps(v_1, v_1);
            v_2 = _mm_mul_ps(v_2, v_2);
            v_3 = _mm_mul_ps(v_3, v_3);
            q_0 = s_0_0;
            q_1 = s_0_1;
            q_2 = s_0_2;
            q_3 = s_0_3;
            s_0_0 = _mm_add_ps(s_0_0, _mm_or_ps(v_0, mask_BLP));
            s_0_1 = _mm_add_ps(s_0_1, _mm_or_ps(v_1, mask_BLP));
            s_0_2 = _mm_add_ps(s_0_2, _mm_or_ps(v_2, mask_BLP));
            s_0_3 = _mm_add_ps(s_0_3, _mm_or_ps(v_3, mask_BLP));
            q_0 = _mm_sub_ps(q_0, s_0_0);
            q_1 = _mm_sub_ps(q_1, s_0_1);
            q_2 = _mm_sub_ps(q_2, s_0_2);
            q_3 = _mm_sub_ps(q_3, s_0_3);
            v_0 = _mm_add_ps(v_0, q_0);
            v_1 = _mm_add_ps(v_1, q_1);
            v_2 = _mm_add_ps(v_2, q_2);
            v_3 = _mm_add_ps(v_3, q_3);
            q_0 = s_1_0;
            q_1 = s_1_1;
            q_2 = s_1_2;
            q_3 = s_1_3;
            s_1_0 = _mm_add_ps(s_1_0, _mm_or_ps(v_0, mask_BLP));
            s_1_1 = _mm_add_ps(s_1_1, _mm_or_ps(v_1, mask_BLP));
            s_1_2 = _mm_add_ps(s_1_2, _mm_or_ps(v_2, mask_BLP));
            s_1_3 = _mm_add_ps(s_1_3, _mm_or_ps(v_3, mask_BLP));
            q_0 = _mm_sub_ps(q_0, s_1_0);
            q_1 = _mm_sub_ps(q_1, s_1_1);
            q_2 = _mm_sub_ps(q_2, s_1_2);
            q_3 = _mm_sub_ps(q_3, s_1_3);
            v_0 = _mm_add_ps(v_0, q_0);
            v_1 = _mm_add_ps(v_1, q_1);
            v_2 = _mm_add_ps(v_2, q_2);
            v_3 = _mm_add_ps(v_3, q_3);
            s_2_0 = _mm_add_ps(s_2_0, _mm_or_ps(v_0, mask_BLP));
            s_2_1 = _mm_add_ps(s_2_1, _mm_or_ps(v_1, mask_BLP));
            s_2_2 = _mm_add_ps(s_2_2, _mm_or_ps(v_2, mask_BLP));
            s_2_3 = _mm_add_ps(s_2_3, _mm_or_ps(v_3, mask_BLP));
            i += 8, v_base += 16;
          }
          if(i + 4 <= n){
            v_0 = _mm_mul_ps(_mm_loadu_ps(v_base), scale_mask);
            v_1 = _mm_mul_ps(_mm_loadu_ps(v_base + 4), scale_mask);
            v_0 = _mm_mul_ps(v_0, v_0);
            v_1 = _mm_mul_ps(v_1, v_1);
            q_0 = s_0_0;
            q_1 = s_0_1;
            s_0_0 = _mm_add_ps(s_0_0, _mm_or_ps(v_0, mask_BLP));
            s_0_1 = _mm_add_ps(s_0_1, _mm_or_ps(v_1, mask_BLP));
            q_0 = _mm_sub_ps(q_0, s_0_0);
            q_1 = _mm_sub_ps(q_1, s_0_1);
            v_0 = _mm_add_ps(v_0, q_0);
            v_1 = _mm_add_ps(v_1, q_1);
            q_0 = s_1_0;
            q_1 = s_1_1;
            s_1_0 = _mm_add_ps(s_1_0, _mm_or_ps(v_0, mask_BLP));
            s_1_1 = _mm_add_ps(s_1_1, _mm_or_ps(v_1, mask_BLP));
            q_0 = _mm_sub_ps(q_0, s_1_0);
            q_1 = _mm_sub_ps(q_1, s_1_1);
            v_0 = _mm_add_ps(v_0, q_0);
            v_1 = _mm_add_ps(v_1, q_1);
            s_2_0 = _mm_add_ps(s_2_0, _mm_or_ps(v_0, mask_BLP));
            s_2_1 = _mm_add_ps(s_2_1, _mm_or_ps(v_1, mask_BLP));
            i += 4, v_base += 8;
          }
          if(i + 2 <= n){
            v_0 = _mm_mul_ps(_mm_loadu_ps(v_base), scale_mask);
            v_0 = _mm_mul_ps(v_0, v_0);
            q_0 = s_0_0;
            s_0_0 = _mm_add_ps(s_0_0, _mm_or_ps(v_0, mask_BLP));
            q_0 = _mm_sub_ps(q_0, s_0_0);
            v_0 = _mm_add_ps(v_0, q_0);
            q_0 = s_1_0;
            s_1_0 = _mm_add_ps(s_1_0, _mm_or_ps(v_0, mask_BLP));
            q_0 = _mm_sub_ps(q_0, s_1_0);
            v_0 = _mm_add_ps(v_0, q_0);
            s_2_0 = _mm_add_ps(s_2_0, _mm_or_ps(v_0, mask_BLP));
            i += 2, v_base += 4;
          }
          if(i < n){
            v_0 = _mm_mul_ps(_mm_set_ps(0, 0, v_base[1], v_base[0]), scale_mask);
            v_0 = _mm_mul_ps(v_0, v_0);
            q_0 = s_0_0;
            s_0_0 = _mm_add_ps(s_0_0, _mm_or_ps(v_0, mask_BLP));
            q_0 = _mm_sub_ps(q_0, s_0_0);
            v_0 = _mm_add_ps(v_0, q_0);
            q_0 = s_1_0;
            s_1_0 = _mm_add_ps(s_1_0, _mm_or_ps(v_0, mask_BLP));
            q_0 = _mm_sub_ps(q_0, s_1_0);
            v_0 = _mm_add_ps(v_0, q_0);
            s_2_0 = _mm_add_ps(s_2_0, _mm_or_ps(v_0, mask_BLP));
          }
        }else{

          for(i = 0; i + 24 <= n; i += 24, v_base += (incv * 48)){
            v_0 = _mm_mul_ps(_mm_set_ps(v_base[((incv * 2) + 1)], v_base[(incv * 2)], v_base[1], v_base[0]), scale_mask);
            v_1 = _mm_mul_ps(_mm_set_ps(v_base[((incv * 6) + 1)], v_base[(incv * 6)], v_base[((incv * 4) + 1)], v_base[(incv * 4)]), scale_mask);
            v_2 = _mm_mul_ps(_mm_set_ps(v_base[((incv * 10) + 1)], v_base[(incv * 10)], v_base[((incv * 8) + 1)], v_base[(incv * 8)]), scale_mask);
            v_3 = _mm_mul_ps(_mm_set_ps(v_base[((incv * 14) + 1)], v_base[(incv * 14)], v_base[((incv * 12) + 1)], v_base[(incv * 12)]), scale_mask);
            v_4 = _mm_mul_ps(_mm_set_ps(v_base[((incv * 18) + 1)], v_base[(incv * 18)], v_base[((incv * 16) + 1)], v_base[(incv * 16)]), scale_mask);
            v_5 = _mm_mul_ps(_mm_set_ps(v_base[((incv * 22) + 1)], v_base[(incv * 22)], v_base[((incv * 20) + 1)], v_base[(incv * 20)]), scale_mask);
            v_6 = _mm_mul_ps(_mm_set_ps(v_base[((incv * 26) + 1)], v_base[(incv * 26)], v_base[((incv * 24) + 1)], v_base[(incv * 24)]), scale_mask);
            v_7 = _mm_mul_ps(_mm_set_ps(v_base[((incv * 30) + 1)], v_base[(incv * 30)], v_base[((incv * 28) + 1)], v_base[(incv * 28)]), scale_mask);
            v_8 = _mm_mul_ps(_mm_set_ps(v_base[((incv * 34) + 1)], v_base[(incv * 34)], v_base[((incv * 32) + 1)], v_base[(incv * 32)]), scale_mask);
            v_9 = _mm_mul_ps(_mm_set_ps(v_base[((incv * 38) + 1)], v_base[(incv * 38)], v_base[((incv * 36) + 1)], v_base[(incv * 36)]), scale_mask);
            v_10 = _mm_mul_ps(_mm_set_ps(v_base[((incv * 42) + 1)], v_base[(incv * 42)], v_base[((incv * 40) + 1)], v_base[(incv * 40)]), scale_mask);
            v_11 = _mm_mul_ps(_mm_set_ps(v_base[((incv * 46) + 1)], v_base[(incv * 46)], v_base[((incv * 44) + 1)], v_base[(incv * 44)]), scale_mask);
            v_0 = _mm_mul_ps(v_0, v_0);
            v_1 = _mm_mul_ps(v_1, v_1);
            v_2 = _mm_mul_ps(v_2, v_2);
            v_3 = _mm_mul_ps(v_3, v_3);
            v_4 = _mm_mul_ps(v_4, v_4);
            v_5 = _mm_mul_ps(v_5, v_5);
            v_6 = _mm_mul_ps(v_6, v_6);
            v_7 = _mm_mul_ps(v_7, v_7);
            v_8 = _mm_mul_ps(v_8, v_8);
            v_9 = _mm_mul_ps(v_9, v_9);
            v_10 = _mm_mul_ps(v_10, v_10);
            v_11 = _mm_mul_ps(v_11, v_11);
            q_0 = s_0_0;
            q_1 = s_0_1;
            q_2 = s_0_2;
            q_3 = s_0_3;
            s_0_0 = _mm_add_ps(s_0_0, _mm_or_ps(v_0, mask_BLP));
            s_0_1 = _mm_add_ps(s_0_1, _mm_or_ps(v_1, mask_BLP));
            s_0_2 = _mm_add_ps(s_0_2, _mm_or_ps(v_2, mask_BLP));
            s_0_3 = _mm_add_ps(s_0_3, _mm_or_ps(v_3, mask_BLP));
            q_0 = _mm_sub_ps(q_0, s_0_0);
            q_1 = _mm_sub_ps(q_1, s_0_1);
            q_2 = _mm_sub_ps(q_2, s_0_2);
            q_3 = _mm_sub_ps(q_3, s_0_3);
            v_0 = _mm_add_ps(v_0, q_0);
            v_1 = _mm_add_ps(v_1, q_1);
            v_2 = _mm_add_ps(v_2, q_2);
            v_3 = _mm_add_ps(v_3, q_3);
            q_0 = s_1_0;
            q_1 = s_1_1;
            q_2 = s_1_2;
            q_3 = s_1_3;
            s_1_0 = _mm_add_ps(s_1_0, _mm_or_ps(v_0, mask_BLP));
            s_1_1 = _mm_add_ps(s_1_1, _mm_or_ps(v_1, mask_BLP));
            s_1_2 = _mm_add_ps(s_1_2, _mm_or_ps(v_2, mask_BLP));
            s_1_3 = _mm_add_ps(s_1_3, _mm_or_ps(v_3, mask_BLP));
            q_0 = _mm_sub_ps(q_0, s_1_0);
            q_1 = _mm_sub_ps(q_1, s_1_1);
            q_2 = _mm_sub_ps(q_2, s_1_2);
            q_3 = _mm_sub_ps(q_3, s_1_3);
            v_0 = _mm_add_ps(v_0, q_0);
            v_1 = _mm_add_ps(v_1, q_1);
            v_2 = _mm_add_ps(v_2, q_2);
            v_3 = _mm_add_ps(v_3, q_3);
            s_2_0 = _mm_add_ps(s_2_0, _mm_or_ps(v_0, mask_BLP));
            s_2_1 = _mm_add_ps(s_2_1, _mm_or_ps(v_1, mask_BLP));
            s_2_2 = _mm_add_ps(s_2_2, _mm_or_ps(v_2, mask_BLP));
            s_2_3 = _mm_add_ps(s_2_3, _mm_or_ps(v_3, mask_BLP));
            q_0 = s_0_0;
            q_1 = s_0_1;
            q_2 = s_0_2;
            q_3 = s_0_3;
            s_0_0 = _mm_add_ps(s_0_0, _mm_or_ps(v_4, mask_BLP));
            s_0_1 = _mm_add_ps(s_0_1, _mm_or_ps(v_5, mask_BLP));
            s_0_2 = _mm_add_ps(s_0_2, _mm_or_ps(v_6, mask_BLP));
            s_0_3 = _mm_add_ps(s_0_3, _mm_or_ps(v_7, mask_BLP));
            q_0 = _mm_sub_ps(q_0, s_0_0);
            q_1 = _mm_sub_ps(q_1, s_0_1);
            q_2 = _mm_sub_ps(q_2, s_0_2);
            q_3 = _mm_sub_ps(q_3, s_0_3);
            v_4 = _mm_add_ps(v_4, q_0);
            v_5 = _mm_add_ps(v_5, q_1);
            v_6 = _mm_add_ps(v_6, q_2);
            v_7 = _mm_add_ps(v_7, q_3);
            q_0 = s_1_0;
            q_1 = s_1_1;
            q_2 = s_1_2;
            q_3 = s_1_3;
            s_1_0 = _mm_add_ps(s_1_0, _mm_or_ps(v_4, mask_BLP));
            s_1_1 = _mm_add_ps(s_1_1, _mm_or_ps(v_5, mask_BLP));
            s_1_2 = _mm_add_ps(s_1_2, _mm_or_ps(v_6, mask_BLP));
            s_1_3 = _mm_add_ps(s_1_3, _mm_or_ps(v_7, mask_BLP));
            q_0 = _mm_sub_ps(q_0, s_1_0);
            q_1 = _mm_sub_ps(q_1, s_1_1);
            q_2 = _mm_sub_ps(q_2, s_1_2);
            q_3 = _mm_sub_ps(q_3, s_1_3);
            v_4 = _mm_add_ps(v_4, q_0);
            v_5 = _mm_add_ps(v_5, q_1);
            v_6 = _mm_add_ps(v_6, q_2);
            v_7 = _mm_add_ps(v_7, q_3);
            s_2_0 = _mm_add_ps(s_2_0, _mm_or_ps(v_4, mask_BLP));
            s_2_1 = _mm_add_ps(s_2_1, _mm_or_ps(v_5, mask_BLP));
            s_2_2 = _mm_add_ps(s_2_2, _mm_or_ps(v_6, mask_BLP));
            s_2_3 = _mm_add_ps(s_2_3, _mm_or_ps(v_7, mask_BLP));
            q_0 = s_0_0;
            q_1 = s_0_1;
            q_2 = s_0_2;
            q_3 = s_0_3;
            s_0_0 = _mm_add_ps(s_0_0, _mm_or_ps(v_8, mask_BLP));
            s_0_1 = _mm_add_ps(s_0_1, _mm_or_ps(v_9, mask_BLP));
            s_0_2 = _mm_add_ps(s_0_2, _mm_or_ps(v_10, mask_BLP));
            s_0_3 = _mm_add_ps(s_0_3, _mm_or_ps(v_11, mask_BLP));
            q_0 = _mm_sub_ps(q_0, s_0_0);
            q_1 = _mm_sub_ps(q_1, s_0_1);
            q_2 = _mm_sub_ps(q_2, s_0_2);
            q_3 = _mm_sub_ps(q_3, s_0_3);
            v_8 = _mm_add_ps(v_8, q_0);
            v_9 = _mm_add_ps(v_9, q_1);
            v_10 = _mm_add_ps(v_10, q_2);
            v_11 = _mm_add_ps(v_11, q_3);
            q_0 = s_1_0;
            q_1 = s_1_1;
            q_2 = s_1_2;
            q_3 = s_1_3;
            s_1_0 = _mm_add_ps(s_1_0, _mm_or_ps(v_8, mask_BLP));
            s_1_1 = _mm_add_ps(s_1_1, _mm_or_ps(v_9, mask_BLP));
            s_1_2 = _mm_add_ps(s_1_2, _mm_or_ps(v_10, mask_BLP));
            s_1_3 = _mm_add_ps(s_1_3, _mm_or_ps(v_11, mask_BLP));
            q_0 = _mm_sub_ps(q_0, s_1_0);
            q_1 = _mm_sub_ps(q_1, s_1_1);
            q_2 = _mm_sub_ps(q_2, s_1_2);
            q_3 = _mm_sub_ps(q_3, s_1_3);
            v_8 = _mm_add_ps(v_8, q_0);
            v_9 = _mm_add_ps(v_9, q_1);
            v_10 = _mm_add_ps(v_10, q_2);
            v_11 = _mm_add_ps(v_11, q_3);
            s_2_0 = _mm_add_ps(s_2_0, _mm_or_ps(v_8, mask_BLP));
            s_2_1 = _mm_add_ps(s_2_1, _mm_or_ps(v_9, mask_BLP));
            s_2_2 = _mm_add_ps(s_2_2, _mm_or_ps(v_10, mask_BLP));
            s_2_3 = _mm_add_ps(s_2_3, _mm_or_ps(v_11, mask_BLP));
          }
          if(i + 16 <= n){
            v_0 = _mm_mul_ps(_mm_set_ps(v_base[((incv * 2) + 1)], v_base[(incv * 2)], v_base[1], v_base[0]), scale_mask);
            v_1 = _mm_mul_ps(_mm_set_ps(v_base[((incv * 6) + 1)], v_base[(incv * 6)], v_base[((incv * 4) + 1)], v_base[(incv * 4)]), scale_mask);
            v_2 = _mm_mul_ps(_mm_set_ps(v_base[((incv * 10) + 1)], v_base[(incv * 10)], v_base[((incv * 8) + 1)], v_base[(incv * 8)]), scale_mask);
            v_3 = _mm_mul_ps(_mm_set_ps(v_base[((incv * 14) + 1)], v_base[(incv * 14)], v_base[((incv * 12) + 1)], v_base[(incv * 12)]), scale_mask);
            v_4 = _mm_mul_ps(_mm_set_ps(v_base[((incv * 18) + 1)], v_base[(incv * 18)], v_base[((incv * 16) + 1)], v_base[(incv * 16)]), scale_mask);
            v_5 = _mm_mul_ps(_mm_set_ps(v_base[((incv * 22) + 1)], v_base[(incv * 22)], v_base[((incv * 20) + 1)], v_base[(incv * 20)]), scale_mask);
            v_6 = _mm_mul_ps(_mm_set_ps(v_base[((incv * 26) + 1)], v_base[(incv * 26)], v_base[((incv * 24) + 1)], v_base[(incv * 24)]), scale_mask);
            v_7 = _mm_mul_ps(_mm_set_ps(v_base[((incv * 30) + 1)], v_base[(incv * 30)], v_base[((incv * 28) + 1)], v_base[(incv * 28)]), scale_mask);
            v_0 = _mm_mul_ps(v_0, v_0);
            v_1 = _mm_mul_ps(v_1, v_1);
            v_2 = _mm_mul_ps(v_2, v_2);
            v_3 = _mm_mul_ps(v_3, v_3);
            v_4 = _mm_mul_ps(v_4, v_4);
            v_5 = _mm_mul_ps(v_5, v_5);
            v_6 = _mm_mul_ps(v_6, v_6);
            v_7 = _mm_mul_ps(v_7, v_7);
            q_0 = s_0_0;
            q_1 = s_0_1;
            q_2 = s_0_2;
            q_3 = s_0_3;
            s_0_0 = _mm_add_ps(s_0_0, _mm_or_ps(v_0, mask_BLP));
            s_0_1 = _mm_add_ps(s_0_1, _mm_or_ps(v_1, mask_BLP));
            s_0_2 = _mm_add_ps(s_0_2, _mm_or_ps(v_2, mask_BLP));
            s_0_3 = _mm_add_ps(s_0_3, _mm_or_ps(v_3, mask_BLP));
            q_0 = _mm_sub_ps(q_0, s_0_0);
            q_1 = _mm_sub_ps(q_1, s_0_1);
            q_2 = _mm_sub_ps(q_2, s_0_2);
            q_3 = _mm_sub_ps(q_3, s_0_3);
            v_0 = _mm_add_ps(v_0, q_0);
            v_1 = _mm_add_ps(v_1, q_1);
            v_2 = _mm_add_ps(v_2, q_2);
            v_3 = _mm_add_ps(v_3, q_3);
            q_0 = s_1_0;
            q_1 = s_1_1;
            q_2 = s_1_2;
            q_3 = s_1_3;
            s_1_0 = _mm_add_ps(s_1_0, _mm_or_ps(v_0, mask_BLP));
            s_1_1 = _mm_add_ps(s_1_1, _mm_or_ps(v_1, mask_BLP));
            s_1_2 = _mm_add_ps(s_1_2, _mm_or_ps(v_2, mask_BLP));
            s_1_3 = _mm_add_ps(s_1_3, _mm_or_ps(v_3, mask_BLP));
            q_0 = _mm_sub_ps(q_0, s_1_0);
            q_1 = _mm_sub_ps(q_1, s_1_1);
            q_2 = _mm_sub_ps(q_2, s_1_2);
            q_3 = _mm_sub_ps(q_3, s_1_3);
            v_0 = _mm_add_ps(v_0, q_0);
            v_1 = _mm_add_ps(v_1, q_1);
            v_2 = _mm_add_ps(v_2, q_2);
            v_3 = _mm_add_ps(v_3, q_3);
            s_2_0 = _mm_add_ps(s_2_0, _mm_or_ps(v_0, mask_BLP));
            s_2_1 = _mm_add_ps(s_2_1, _mm_or_ps(v_1, mask_BLP));
            s_2_2 = _mm_add_ps(s_2_2, _mm_or_ps(v_2, mask_BLP));
            s_2_3 = _mm_add_ps(s_2_3, _mm_or_ps(v_3, mask_BLP));
            q_0 = s_0_0;
            q_1 = s_0_1;
            q_2 = s_0_2;
            q_3 = s_0_3;
            s_0_0 = _mm_add_ps(s_0_0, _mm_or_ps(v_4, mask_BLP));
            s_0_1 = _mm_add_ps(s_0_1, _mm_or_ps(v_5, mask_BLP));
            s_0_2 = _mm_add_ps(s_0_2, _mm_or_ps(v_6, mask_BLP));
            s_0_3 = _mm_add_ps(s_0_3, _mm_or_ps(v_7, mask_BLP));
            q_0 = _mm_sub_ps(q_0, s_0_0);
            q_1 = _mm_sub_ps(q_1, s_0_1);
            q_2 = _mm_sub_ps(q_2, s_0_2);
            q_3 = _mm_sub_ps(q_3, s_0_3);
            v_4 = _mm_add_ps(v_4, q_0);
            v_5 = _mm_add_ps(v_5, q_1);
            v_6 = _mm_add_ps(v_6, q_2);
            v_7 = _mm_add_ps(v_7, q_3);
            q_0 = s_1_0;
            q_1 = s_1_1;
            q_2 = s_1_2;
            q_3 = s_1_3;
            s_1_0 = _mm_add_ps(s_1_0, _mm_or_ps(v_4, mask_BLP));
            s_1_1 = _mm_add_ps(s_1_1, _mm_or_ps(v_5, mask_BLP));
            s_1_2 = _mm_add_ps(s_1_2, _mm_or_ps(v_6, mask_BLP));
            s_1_3 = _mm_add_ps(s_1_3, _mm_or_ps(v_7, mask_BLP));
            q_0 = _mm_sub_ps(q_0, s_1_0);
            q_1 = _mm_sub_ps(q_1, s_1_1);
            q_2 = _mm_sub_ps(q_2, s_1_2);
            q_3 = _mm_sub_ps(q_3, s_1_3);
            v_4 = _mm_add_ps(v_4, q_0);
            v_5 = _mm_add_ps(v_5, q_1);
            v_6 = _mm_add_ps(v_6, q_2);
            v_7 = _mm_add_ps(v_7, q_3);
            s_2_0 = _mm_add_ps(s_2_0, _mm_or_ps(v_4, mask_BLP));
            s_2_1 = _mm_add_ps(s_2_1, _mm_or_ps(v_5, mask_BLP));
            s_2_2 = _mm_add_ps(s_2_2, _mm_or_ps(v_6, mask_BLP));
            s_2_3 = _mm_add_ps(s_2_3, _mm_or_ps(v_7, mask_BLP));
            i += 16, v_base += (incv * 32);
          }
          if(i + 8 <= n){
            v_0 = _mm_mul_ps(_mm_set_ps(v_base[((incv * 2) + 1)], v_base[(incv * 2)], v_base[1], v_base[0]), scale_mask);
            v_1 = _mm_mul_ps(_mm_set_ps(v_base[((incv * 6) + 1)], v_base[(incv * 6)], v_base[((incv * 4) + 1)], v_base[(incv * 4)]), scale_mask);
            v_2 = _mm_mul_ps(_mm_set_ps(v_base[((incv * 10) + 1)], v_base[(incv * 10)], v_base[((incv * 8) + 1)], v_base[(incv * 8)]), scale_mask);
            v_3 = _mm_mul_ps(_mm_set_ps(v_base[((incv * 14) + 1)], v_base[(incv * 14)], v_base[((incv * 12) + 1)], v_base[(incv * 12)]), scale_mask);
            v_0 = _mm_mul_ps(v_0, v_0);
            v_1 = _mm_mul_ps(v_1, v_1);
            v_2 = _mm_mul_ps(v_2, v_2);
            v_3 = _mm_mul_ps(v_3, v_3);
            q_0 = s_0_0;
            q_1 = s_0_1;
            q_2 = s_0_2;
            q_3 = s_0_3;
            s_0_0 = _mm_add_ps(s_0_0, _mm_or_ps(v_0, mask_BLP));
            s_0_1 = _mm_add_ps(s_0_1, _mm_or_ps(v_1, mask_BLP));
            s_0_2 = _mm_add_ps(s_0_2, _mm_or_ps(v_2, mask_BLP));
            s_0_3 = _mm_add_ps(s_0_3, _mm_or_ps(v_3, mask_BLP));
            q_0 = _mm_sub_ps(q_0, s_0_0);
            q_1 = _mm_sub_ps(q_1, s_0_1);
            q_2 = _mm_sub_ps(q_2, s_0_2);
            q_3 = _mm_sub_ps(q_3, s_0_3);
            v_0 = _mm_add_ps(v_0, q_0);
            v_1 = _mm_add_ps(v_1, q_1);
            v_2 = _mm_add_ps(v_2, q_2);
            v_3 = _mm_add_ps(v_3, q_3);
            q_0 = s_1_0;
            q_1 = s_1_1;
            q_2 = s_1_2;
            q_3 = s_1_3;
            s_1_0 = _mm_add_ps(s_1_0, _mm_or_ps(v_0, mask_BLP));
            s_1_1 = _mm_add_ps(s_1_1, _mm_or_ps(v_1, mask_BLP));
            s_1_2 = _mm_add_ps(s_1_2, _mm_or_ps(v_2, mask_BLP));
            s_1_3 = _mm_add_ps(s_1_3, _mm_or_ps(v_3, mask_BLP));
            q_0 = _mm_sub_ps(q_0, s_1_0);
            q_1 = _mm_sub_ps(q_1, s_1_1);
            q_2 = _mm_sub_ps(q_2, s_1_2);
            q_3 = _mm_sub_ps(q_3, s_1_3);
            v_0 = _mm_add_ps(v_0, q_0);
            v_1 = _mm_add_ps(v_1, q_1);
            v_2 = _mm_add_ps(v_2, q_2);
            v_3 = _mm_add_ps(v_3, q_3);
            s_2_0 = _mm_add_ps(s_2_0, _mm_or_ps(v_0, mask_BLP));
            s_2_1 = _mm_add_ps(s_2_1, _mm_or_ps(v_1, mask_BLP));
            s_2_2 = _mm_add_ps(s_2_2, _mm_or_ps(v_2, mask_BLP));
            s_2_3 = _mm_add_ps(s_2_3, _mm_or_ps(v_3, mask_BLP));
            i += 8, v_base += (incv * 16);
          }
          if(i + 4 <= n){
            v_0 = _mm_mul_ps(_mm_set_ps(v_base[((incv * 2) + 1)], v_base[(incv * 2)], v_base[1], v_base[0]), scale_mask);
            v_1 = _mm_mul_ps(_mm_set_ps(v_base[((incv * 6) + 1)], v_base[(incv * 6)], v_base[((incv * 4) + 1)], v_base[(incv * 4)]), scale_mask);
            v_0 = _mm_mul_ps(v_0, v_0);
            v_1 = _mm_mul_ps(v_1, v_1);
            q_0 = s_0_0;
            q_1 = s_0_1;
            s_0_0 = _mm_add_ps(s_0_0, _mm_or_ps(v_0, mask_BLP));
            s_0_1 = _mm_add_ps(s_0_1, _mm_or_ps(v_1, mask_BLP));
            q_0 = _mm_sub_ps(q_0, s_0_0);
            q_1 = _mm_sub_ps(q_1, s_0_1);
            v_0 = _mm_add_ps(v_0, q_0);
            v_1 = _mm_add_ps(v_1, q_1);
            q_0 = s_1_0;
            q_1 = s_1_1;
            s_1_0 = _mm_add_ps(s_1_0, _mm_or_ps(v_0, mask_BLP));
            s_1_1 = _mm_add_ps(s_1_1, _mm_or_ps(v_1, mask_BLP));
            q_0 = _mm_sub_ps(q_0, s_1_0);
            q_1 = _mm_sub_ps(q_1, s_1_1);
            v_0 = _mm_add_ps(v_0, q_0);
            v_1 = _mm_add_ps(v_1, q_1);
            s_2_0 = _mm_add_ps(s_2_0, _mm_or_ps(v_0, mask_BLP));
            s_2_1 = _mm_add_ps(s_2_1, _mm_or_ps(v_1, mask_BLP));
            i += 4, v_base += (incv * 8);
          }
          if(i + 2 <= n){
            v_0 = _mm_mul_ps(_mm_set_ps(v_base[((incv * 2) + 1)], v_base[(incv * 2)], v_base[1], v_base[0]), scale_mask);
            v_0 = _mm_mul_ps(v_0, v_0);
            q_0 = s_0_0;
            s_0_0 = _mm_add_ps(s_0_0, _mm_or_ps(v_0, mask_BLP));
            q_0 = _mm_sub_ps(q_0, s_0_0);
            v_0 = _mm_add_ps(v_0, q_0);
            q_0 = s_1_0;
            s_1_0 = _mm_add_ps(s_1_0, _mm_or_ps(v_0, mask_BLP));
            q_0 = _mm_sub_ps(q_0, s_1_0);
            v_0 = _mm_add_ps(v_0, q_0);
            s_2_0 = _mm_add_ps(s_2_0, _mm_or_ps(v_0, mask_BLP));
            i += 2, v_base += (incv * 4);
          }
          if(i < n){
            v_0 = _mm_mul_ps(_mm_set_ps(0, 0, v_base[1], v_base[0]), scale_mask);
            v_0 = _mm_mul_ps(v_0, v_0);
            q_0 = s_0_0;
            s_0_0 = _mm_add_ps(s_0_0, _mm_or_ps(v_0, mask_BLP));
            q_0 = _mm_sub_ps(q_0, s_0_0);
            v_0 = _mm_add_ps(v_0, q_0);
            q_0 = s_1_0;
            s_1_0 = _mm_add_ps(s_1_0, _mm_or_ps(v_0, mask_BLP));
            q_0 = _mm_sub_ps(q_0, s_1_0);
            v_0 = _mm_add_ps(v_0, q_0);
            s_2_0 = _mm_add_ps(s_2_0, _mm_or_ps(v_0, mask_BLP));
          }
        }
        s_0_0 = _mm_sub_ps(s_0_0, _mm_set_ps(sum_base[1], sum_base[0], 0, 0));
        q_0 = (__m128)_mm_load1_pd((double *)(sum_base));
        s_0_0 = _mm_add_ps(s_0_0, _mm_sub_ps(s_0_1, q_0));
        s_0_0 = _mm_add_ps(s_0_0, _mm_sub_ps(s_0_2, q_0));
        s_0_0 = _mm_add_ps(s_0_0, _mm_sub_ps(s_0_3, q_0));
        _mm_store_ps((float*)tmp_cons, s_0_0);
        sum[0] = tmp_cons[0] + tmp_cons[1];
        s_1_0 = _mm_sub_ps(s_1_0, _mm_set_ps(sum_base[3], sum_base[2], 0, 0));
        q_0 = (__m128)_mm_load1_pd((double *)(sum_base + 2));
        s_1_0 = _mm_add_ps(s_1_0, _mm_sub_ps(s_1_1, q_0));
        s_1_0 = _mm_add_ps(s_1_0, _mm_sub_ps(s_1_2, q_0));
        s_1_0 = _mm_add_ps(s_1_0, _mm_sub_ps(s_1_3, q_0));
        _mm_store_ps((float*)tmp_cons, s_1_0);
        sum[1] = tmp_cons[0] + tmp_cons[1];
        s_2_0 = _mm_sub_ps(s_2_0, _mm_set_ps(sum_base[5], sum_base[4], 0, 0));
        q_0 = (__m128)_mm_load1_pd((double *)(sum_base + 4));
        s_2_0 = _mm_add_ps(s_2_0, _mm_sub_ps(s_2_1, q_0));
        s_2_0 = _mm_add_ps(s_2_0, _mm_sub_ps(s_2_2, q_0));
        s_2_0 = _mm_add_ps(s_2_0, _mm_sub_ps(s_2_3, q_0));
        _mm_store_ps((float*)tmp_cons, s_2_0);
        sum[2] = tmp_cons[0] + tmp_cons[1];
        RESET_DAZ_FLAG
        return;
      }
      default:{
        int i, j;

        float* sum_base = (float*) sum;
        float* v_base = (float*) v;
        __m128 v_0;
        __m128 q_0;
        __m128 s_0;
        __m128 s_buffer[MAX_FOLD];

        for(j = 0; j < fold; j += 1){
          s_buffer[j] = (__m128)_mm_load1_pd((double *)(sum_base + (j * 2)));
        }
        if(incv == 1){

          for(i = 0; i + 2 <= n; i += 2, v_base += 4){
            v_0 = _mm_mul_ps(_mm_loadu_ps(v_base), scale_mask);
            v_0 = _mm_mul_ps(v_0, v_0);
            for(j = 0; j < fold - 1; j++){
              s_0 = s_buffer[j];
              q_0 = _mm_add_ps(s_0, _mm_or_ps(v_0, mask_BLP));
              s_buffer[j] = q_0;
              q_0 = _mm_sub_ps(s_0, q_0);
              v_0 = _mm_add_ps(v_0, q_0);
            }
            s_buffer[j] = _mm_add_ps(s_buffer[j], _mm_or_ps(v_0, mask_BLP));
          }
          if(i < n){
            v_0 = _mm_mul_ps(_mm_set_ps(0, 0, v_base[1], v_base[0]), scale_mask);
            v_0 = _mm_mul_ps(v_0, v_0);
            for(j = 0; j < fold - 1; j++){
              s_0 = s_buffer[j];
              q_0 = _mm_add_ps(s_0, _mm_or_ps(v_0, mask_BLP));
              s_buffer[j] = q_0;
              q_0 = _mm_sub_ps(s_0, q_0);
              v_0 = _mm_add_ps(v_0, q_0);
            }
            s_buffer[j] = _mm_add_ps(s_buffer[j], _mm_or_ps(v_0, mask_BLP));
          }
        }else{

          for(i = 0; i + 2 <= n; i += 2, v_base += (incv * 4)){
            v_0 = _mm_mul_ps(_mm_set_ps(v_base[((incv * 2) + 1)], v_base[(incv * 2)], v_base[1], v_base[0]), scale_mask);
            v_0 = _mm_mul_ps(v_0, v_0);
            for(j = 0; j < fold - 1; j++){
              s_0 = s_buffer[j];
              q_0 = _mm_add_ps(s_0, _mm_or_ps(v_0, mask_BLP));
              s_buffer[j] = q_0;
              q_0 = _mm_sub_ps(s_0, q_0);
              v_0 = _mm_add_ps(v_0, q_0);
            }
            s_buffer[j] = _mm_add_ps(s_buffer[j], _mm_or_ps(v_0, mask_BLP));
          }
          if(i < n){
            v_0 = _mm_mul_ps(_mm_set_ps(0, 0, v_base[1], v_base[0]), scale_mask);
            v_0 = _mm_mul_ps(v_0, v_0);
            for(j = 0; j < fold - 1; j++){
              s_0 = s_buffer[j];
              q_0 = _mm_add_ps(s_0, _mm_or_ps(v_0, mask_BLP));
              s_buffer[j] = q_0;
              q_0 = _mm_sub_ps(s_0, q_0);
              v_0 = _mm_add_ps(v_0, q_0);
            }
            s_buffer[j] = _mm_add_ps(s_buffer[j], _mm_or_ps(v_0, mask_BLP));
          }
        }
        for(j = 0; j < fold; j += 1){
          s_buffer[j] = _mm_sub_ps(s_buffer[j], _mm_set_ps(sum_base[((j * 2) + 1)], sum_base[(j * 2)], 0, 0));
          _mm_store_ps((float*)tmp_cons, s_buffer[j]);
          sum[j] = tmp_cons[0] + tmp_cons[1];
        }
        RESET_DAZ_FLAG
        return;
      }
    }
  #else
    float scale_mask = scale;
    int_float tmp_BLP;
    SET_DAZ_FLAG;
    switch(fold){
      case 3:{
        int i;

        float* sum_base = (float*) sum;
        float* v_base = (float*) v;
        float v_0, v_1;
        float q_0, q_1;
        float s_0_0, s_0_1;
        float s_1_0, s_1_1;
        float s_2_0, s_2_1;

        s_0_0 = sum_base[0];
        s_0_1 = sum_base[1];
        s_1_0 = sum_base[2];
        s_1_1 = sum_base[3];
        s_2_0 = sum_base[4];
        s_2_1 = sum_base[5];
        if(incv == 1){

          for(i = 0; i + 1 <= n; i += 1, v_base += 2){
            v_0 = v_base[0] * scale_mask;
            v_1 = v_base[1] * scale_mask;
            v_0 = v_0 * v_0;
            v_1 = v_1 * v_1;
            q_0 = s_0_0;
            q_1 = s_0_1;
            tmp_BLP.f = v_0;
            tmp_BLP.i |= 1;
            s_0_0 = s_0_0 + tmp_BLP.f;
            tmp_BLP.f = v_1;
            tmp_BLP.i |= 1;
            s_0_1 = s_0_1 + tmp_BLP.f;
            q_0 = q_0 - s_0_0;
            q_1 = q_1 - s_0_1;
            v_0 = v_0 + q_0;
            v_1 = v_1 + q_1;
            q_0 = s_1_0;
            q_1 = s_1_1;
            tmp_BLP.f = v_0;
            tmp_BLP.i |= 1;
            s_1_0 = s_1_0 + tmp_BLP.f;
            tmp_BLP.f = v_1;
            tmp_BLP.i |= 1;
            s_1_1 = s_1_1 + tmp_BLP.f;
            q_0 = q_0 - s_1_0;
            q_1 = q_1 - s_1_1;
            v_0 = v_0 + q_0;
            v_1 = v_1 + q_1;
            tmp_BLP.f = v_0;
            tmp_BLP.i |= 1;
            s_2_0 = s_2_0 + tmp_BLP.f;
            tmp_BLP.f = v_1;
            tmp_BLP.i |= 1;
            s_2_1 = s_2_1 + tmp_BLP.f;
          }
        }else{

          for(i = 0; i + 1 <= n; i += 1, v_base += (incv * 2)){
            v_0 = v_base[0] * scale_mask;
            v_1 = v_base[1] * scale_mask;
            v_0 = v_0 * v_0;
            v_1 = v_1 * v_1;
            q_0 = s_0_0;
            q_1 = s_0_1;
            tmp_BLP.f = v_0;
            tmp_BLP.i |= 1;
            s_0_0 = s_0_0 + tmp_BLP.f;
            tmp_BLP.f = v_1;
            tmp_BLP.i |= 1;
            s_0_1 = s_0_1 + tmp_BLP.f;
            q_0 = q_0 - s_0_0;
            q_1 = q_1 - s_0_1;
            v_0 = v_0 + q_0;
            v_1 = v_1 + q_1;
            q_0 = s_1_0;
            q_1 = s_1_1;
            tmp_BLP.f = v_0;
            tmp_BLP.i |= 1;
            s_1_0 = s_1_0 + tmp_BLP.f;
            tmp_BLP.f = v_1;
            tmp_BLP.i |= 1;
            s_1_1 = s_1_1 + tmp_BLP.f;
            q_0 = q_0 - s_1_0;
            q_1 = q_1 - s_1_1;
            v_0 = v_0 + q_0;
            v_1 = v_1 + q_1;
            tmp_BLP.f = v_0;
            tmp_BLP.i |= 1;
            s_2_0 = s_2_0 + tmp_BLP.f;
            tmp_BLP.f = v_1;
            tmp_BLP.i |= 1;
            s_2_1 = s_2_1 + tmp_BLP.f;
          }
        }
        ((float*)sum)[0] = s_0_0;
        ((float*)sum)[1] = s_0_1;
        ((float*)sum)[2] = s_1_0;
        ((float*)sum)[3] = s_1_1;
        ((float*)sum)[4] = s_2_0;
        ((float*)sum)[5] = s_2_1;
        RESET_DAZ_FLAG
        return;
      }
      default:{
        int i, j;

        float* sum_base = (float*) sum;
        float* v_base = (float*) v;
        float v_0, v_1;
        float q_0, q_1;
        float s_0, s_1;
        float s_buffer[(MAX_FOLD * 2)];

        for(j = 0; j < fold; j += 1){
          s_buffer[(j * 2)] = sum_base[(j * 2)];
          s_buffer[((j * 2) + 1)] = sum_base[((j * 2) + 1)];
        }
        if(incv == 1){

          for(i = 0; i + 1 <= n; i += 1, v_base += 2){
            v_0 = v_base[0] * scale_mask;
            v_1 = v_base[1] * scale_mask;
            v_0 = v_0 * v_0;
            v_1 = v_1 * v_1;
            for(j = 0; j < fold - 1; j++){
              s_0 = s_buffer[(j * 2)];
              s_1 = s_buffer[((j * 2) + 1)];
              tmp_BLP.f = v_0;
              tmp_BLP.i |= 1;
              q_0 = s_0 + tmp_BLP.f;
              tmp_BLP.f = v_1;
              tmp_BLP.i |= 1;
              q_1 = s_1 + tmp_BLP.f;
              s_buffer[(j * 2)] = q_0;
              s_buffer[((j * 2) + 1)] = q_1;
              q_0 = s_0 - q_0;
              q_1 = s_1 - q_1;
              v_0 = v_0 + q_0;
              v_1 = v_1 + q_1;
            }
            tmp_BLP.f = v_0;
            tmp_BLP.i |= 1;
            s_buffer[(j * 2)] = s_buffer[(j * 2)] + tmp_BLP.f;
            tmp_BLP.f = v_1;
            tmp_BLP.i |= 1;
            s_buffer[((j * 2) + 1)] = s_buffer[((j * 2) + 1)] + tmp_BLP.f;
          }
        }else{

          for(i = 0; i + 1 <= n; i += 1, v_base += (incv * 2)){
            v_0 = v_base[0] * scale_mask;
            v_1 = v_base[1] * scale_mask;
            v_0 = v_0 * v_0;
            v_1 = v_1 * v_1;
            for(j = 0; j < fold - 1; j++){
              s_0 = s_buffer[(j * 2)];
              s_1 = s_buffer[((j * 2) + 1)];
              tmp_BLP.f = v_0;
              tmp_BLP.i |= 1;
              q_0 = s_0 + tmp_BLP.f;
              tmp_BLP.f = v_1;
              tmp_BLP.i |= 1;
              q_1 = s_1 + tmp_BLP.f;
              s_buffer[(j * 2)] = q_0;
              s_buffer[((j * 2) + 1)] = q_1;
              q_0 = s_0 - q_0;
              q_1 = s_1 - q_1;
              v_0 = v_0 + q_0;
              v_1 = v_1 + q_1;
            }
            tmp_BLP.f = v_0;
            tmp_BLP.i |= 1;
            s_buffer[(j * 2)] = s_buffer[(j * 2)] + tmp_BLP.f;
            tmp_BLP.f = v_1;
            tmp_BLP.i |= 1;
            s_buffer[((j * 2) + 1)] = s_buffer[((j * 2) + 1)] + tmp_BLP.f;
          }
        }
        for(j = 0; j < fold; j += 1){
          ((float*)sum)[(j * 2)] = s_buffer[(j * 2)];
          ((float*)sum)[((j * 2) + 1)] = s_buffer[((j * 2) + 1)];
        }
        RESET_DAZ_FLAG
        return;
      }
    }
  #endif
  //[[[end]]]
}
